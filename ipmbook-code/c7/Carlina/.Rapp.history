randomForest
?randomForest
nvars <- ncol(x); #
mseDrop1 <- numeric(nvars);#
for(j in 1:nvars) {#
	RFi <- randomForest(x=x[,-j],y=X$lambda.i,keep.forest=FALSE,ntree=500,mtry=4)#
	mseDrop1[j] <- min(RFi$mse)#
    cat(j,"\n")	#
} #
mseFull <- min(lambdaRF$mse);#
deltaMSE <- (mseDrop1 - mseFull)/var(X$lambda.i);
deltaMSE
names(deltaMSE)<-names(x)#
deltaMSE=sort(-deltaMSE)#
dev.new(); dotchart(deltaMSE)
mseFull <- min(lambdaRF$mse);#
deltaMSE <- (mseDrop1 - mseFull)/var(X$lambda.i); #
#
names(deltaMSE)<-names(x)#
deltaMSE=-sort(-deltaMSE)#
dev.new(); dotchart(deltaMSE)
names(deltaMSE)<-names(x)#
deltaMSE=sort(deltaMSE)#
dev.new(); dotchart(deltaMSE)
dev.new(width=8,height=5); set_graph_pars("panel2");#
plot(lambdaRF$rsq,type="l"); add_panel_label("a")#
varImpPlot(lambdaRF,type=1,scale=FALSE,main="")#
add_panel_label("b")#
#
nvars <- ncol(x); #
mseDrop1 <- numeric(nvars);#
for(j in 1:nvars) {#
	RFi <- randomForest(x=x[,-j],y=X$lambda.i,keep.forest=FALSE,ntree=500,mtry=4)#
	mseDrop1[j] <- min(RFi$mse)#
    cat(j,"\n")	#
} #
mseFull <- min(lambdaRF$mse);#
deltaMSE <- (mseDrop1 - mseFull)/var(X$lambda.i); #
#
names(deltaMSE)<-names(x)#
deltaMSE=sort(deltaMSE)#
dev.new(); dotchart(deltaMSE)
graphics.off(); #
dev.new(width=8,height=5); set_graph_pars("panel2");#
plot(lambdaRF$rsq,type="l"); add_panel_label("a")#
varImpPlot(lambdaRF,type=1,scale=FALSE,main="")#
add_panel_label("b")
names(deltaMSE)<-names(x)#
deltaMSE=sort(deltaMSE)#
dev.new(); dotchart(deltaMSE)
names(x)
names(deltaMSE)
mseFull <- min(lambdaRF$mse);#
deltaMSE <- (mseDrop1 - mseFull)/var(X$lambda.i);
e<-order(deltaMSE)
sdm <- deltaMSE[e];
names(sdm) <- names(x)[e]
sdm
dotchart(sdm)
dev.new(width=8,height=5); set_graph_pars("panel2");#
plot(lambdaRF$rsq,type="l"); add_panel_label("a")#
varImpPlot(lambdaRF,type=1,scale=FALSE,main="")#
add_panel_label("b")
?importance
lambdaRF$importance
deltaMSE
plot(deltaMSE,lambdaRF$importance[,2]);
graphics.off(); #
dev.new(width=8,height=5); set_graph_pars("panel2");#
plot(lambdaRF$rsq,type="l"); add_panel_label("a")#
varImpPlot(lambdaRF,type=1,scale=FALSE,main="")#
add_panel_label("b")
graphics.off(); #
dev.new(width=8,height=5); set_graph_pars("panel2");#
plot(lambdaRF$rsq,type="l"); add_panel_label("a")#
varImpPlot(lambdaRF,type=1,scale=FALSE,main="",xlim=c(0,1))#
add_panel_label("b")
graphics.off(); #
dev.new(width=8,height=5); set_graph_pars("panel2");#
plot(lambdaRF$rsq,type="l"); add_panel_label("a")#
varImpPlot(lambdaRF,type=1,scale=FALSE,main="")#
add_panel_label("b")
mseFull <- min(lambdaRF$mse);#
deltaMSE <- (mseDrop1 - mseFull)/var(X$lambda.i); #
#
e <- order(deltaMSE); #
dsort <- deltaMSE[e];#
names(dsort) <- names(x)[e];#
dev.new(); dotchart(dsort,xlab="Relative increase in MSE")
dev.new(); dotchart(dsort,xlab="Relative increase in MSE",xlim=c(0,max(dsort)))
rm(list=ls(all=TRUE))#
require(randomForest)#
#
root=ifelse(.Platform$OS.type=="windows","c:/repos","~/Repos"); #
setwd(paste(root,"/ipm_book/Rcode/c2",sep="")); #
source("../utilities/Standard Graphical Pars.R");#
root=ifelse(.Platform$OS.type=="windows","c:/repos","~/Repos"); #
setwd(paste(root,"/ipm_book/Rcode/c7/Carlina",sep="")); #
#
load("Big params plus lambda.Rdata")#
X <- data.frame(t(params.plus.lambda)); #
names(X) <- row.names(params.plus.lambda)#
vars<- which(apply(X,2,sd)>0);#
X <- X[,vars];  #
out<- tuneRF(x=subset(X,select=-lambda.i), y=X$lambda.i,ntreeTry=500,stepfactor=1,improve=0.02); #
x=subset(X,select=-lambda.i)#
lambdaRF <- randomForest(x=x,y=X$lambda.i,keep.forest=TRUE,ntree=500,importance=TRUE,mtry=4)#
#
graphics.off(); #
dev.new(width=8,height=5); set_graph_pars("panel2");#
plot(lambdaRF$rsq,type="l"); add_panel_label("a")#
varImpPlot(lambdaRF,type=1,scale=FALSE,main="")#
add_panel_label("b")#
nvars <- ncol(x); #
mseDrop1 <- numeric(nvars);#
for(j in 1:nvars) {#
	RFi <- randomForest(x=x[,-j],y=X$lambda.i,keep.forest=FALSE,ntree=500,mtry=4)#
	mseDrop1[j] <- min(RFi$mse)#
    cat(j,"\n")	#
} #
mseFull <- min(lambdaRF$mse);#
deltaMSE <- (mseDrop1 - mseFull)/var(X$lambda.i); #
#
e <- order(deltaMSE); #
dsort <- deltaMSE[e];#
names(dsort) <- names(x)[e];#
dev.new(); dotchart(dsort,xlab="Relative increase in MSE",xlim=c(0,max(dsort)))
out
mtry=which(out$OOBError==min(out$OOBError));
names(out)
j=which(out[,2]==min(out[,2]));
j
out
dim(out)
j=which(out[,2]==min(out[,2]));
j
out[j,1]
plot(out)
lambdaRF <- randomForest(x=x,y=X$lambda.i,ntree=500,importance=TRUE,mtry=4)
graphics.off(); #
dev.new(width=8,height=5); set_graph_pars("panel2");#
plot(lambdaRF$rsq,type="l"); add_panel_label("a")#
varImpPlot(lambdaRF,type=1,scale=FALSE,main="")#
add_panel_label("b")
graphics.off(); #
dev.new(width=8,height=5); set_graph_pars("panel2");#
plot(lambdaRF$rsq,type="l",xlab="Number of trees",ylab="Model r^2"); add_panel_label("a")#
varImpPlot(lambdaRF,type=1,scale=FALSE,main="")#
add_panel_label("b")
lambdaRF$rsq
rm(list=ls(all=TRUE))#
require(randomForest)#
#
root=ifelse(.Platform$OS.type=="windows","c:/repos","~/Repos"); #
setwd(paste(root,"/ipm_book/Rcode/c2",sep="")); #
source("../utilities/Standard Graphical Pars.R");#
root=ifelse(.Platform$OS.type=="windows","c:/repos","~/Repos"); #
setwd(paste(root,"/ipm_book/Rcode/c7/Carlina",sep="")); #
#
load("Big params plus lambda.Rdata")#
#
# Format it as a data frame with only the parameters that vary over time. #
X <- data.frame(t(params.plus.lambda)); #
names(X) <- row.names(params.plus.lambda)#
vars<- which(apply(X,2,sd)>0);#
X <- X[,vars];
# 'Tune' the random forest parameter mtry#
out<- tuneRF(x=subset(X,select=-lambda.i), y=X$lambda.i,ntreeTry=500,stepfactor=1,improve=0.02); #
plot(out); ## tells us to use mtry=4
# Fit the model, computing importance measures#
x=subset(X,select=-lambda.i)#
lambdaRF <- randomForest(x=x,y=X$lambda.i,ntree=500,importance=TRUE,mtry=4)
graphics.off(); #
dev.new(width=8,height=5); set_graph_pars("panel2");#
# verify that the number of trees is enough#
# the plot of R^2 vs. number of trees used should saturate #
plot(lambdaRF$rsq,type="l",xlab="Number of trees",ylab="Model r^2"); add_panel_label("a")#
#
# Plot the importance results #
varImpPlot(lambdaRF,type=1,scale=FALSE,main="")#
add_panel_label("b")
### Alternative importance measure. This takes some patience, to fit all the RF models#
nvars <- ncol(x); #
mseDrop1 <- numeric(nvars);#
for(j in 1:nvars) {#
	RFi <- randomForest(x=x[,-j],y=X$lambda.i,keep.forest=FALSE,ntree=500,mtry=4)#
	mseDrop1[j] <- min(RFi$mse)#
    cat(j,"\n")	#
} #
mseFull <- min(lambdaRF$mse);#
deltaMSE <- (mseDrop1 - mseFull)/var(X$lambda.i); #
#
e <- order(deltaMSE); #
dsort <- deltaMSE[e];#
names(dsort) <- names(x)[e];#
dev.new(); dotchart(dsort,xlab="Relative increase in MSE",xlim=c(0,max(dsort)))
## ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~#
## Fit and run fixed and mixed effects effects Carlina stochastic IPM#
## ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~#
#
rm(list=ls(all=TRUE))#
#
library(doBy)#
library(lme4)#
library(MCMCglmm)#
set.seed(53241986)#
#
## Working directory must be set here, so the source()'s below run#
root=ifelse(.Platform$OS.type=="windows","c:/repos","~/Repos"); #
setwd(paste(root,"/ipm_book/Rcode/c2",sep="")); #
#
source("../utilities/Standard Graphical Pars.R");#
#
root=ifelse(.Platform$OS.type=="windows","c:/repos","~/Repos"); #
setwd(paste(root,"/ipm_book/Rcode/c7/Carlina",sep="")); #
#
#Read in the Carlina demographic data#
load("CarlinaIBMsim.Rdata")#
str(sim.data)#
#
store.sim.data <- sim.data#
#
#Select recruit data for the last 20 years (years 31 to 50)#
recr.data <- subset(sim.data,Recr==1)#
recr.data <- subset(recr.data,Yeart>30)#
recr.data <- transform(recr.data,Yeart = factor(Yeart - 30))#
#
#plant data for years 30 to 49, we do this to as the recruit size in #
#year t was generated by the year t-1 parameters#
sim.data <- subset(sim.data,Yeart>29 & Yeart<50)#
with(sim.data,table(Yeart,Recr))#
#Make Yeart a factor#
sim.data <- transform(sim.data,Yeart = factor(Yeart - 29))#
#fit some survival models#
#
mod.Surv <- glm(Surv ~ Yeart  , family = binomial, data = sim.data)#
mod.Surv.1 <- glm(Surv ~ Yeart + z  , family = binomial, data = sim.data)#
mod.Surv.2 <- glm(Surv ~ Yeart * z  , family = binomial, data = sim.data)#
#
anova(mod.Surv,mod.Surv.1,mod.Surv.2,test="Chisq")#
AIC(mod.Surv,mod.Surv.1,mod.Surv.2)#
#
#there is evidence of an interaction, as we might expect as both the intercept and slope vary from #
#year to year#
#
#Let's refit so we can easily get the parameter estimates#
#
mod.Surv <- glm(Surv ~ Yeart/ z -1 , family = binomial, data = sim.data)#
#
summary(mod.Surv)#
#
#Let's refit using glmer#
#
mod.Surv.glmer   <- glmer(Surv ~ 1 + (1|Yeart)  , family = binomial, data = sim.data)#
mod.Surv.glmer.1 <- glmer(Surv ~ z + (1|Yeart)  , family = binomial, data = sim.data)#
mod.Surv.glmer.2 <- glmer(Surv ~ z + (1|Yeart) + (0 + z|Yeart)  , family = binomial, data = sim.data)#
mod.Surv.glmer.3 <- glmer(Surv ~ z + (z|Yeart)  , family = binomial, data = sim.data)#
anova(mod.Surv.glmer,mod.Surv.glmer.1,mod.Surv.glmer.2,mod.Surv.glmer.3)#
# # d<-1#
# prior=list(R=list(V=1, fix=1), G=list(G1=list(V=diag(d), nu=d, alpha.mu=rep(0,d), alpha.V=diag(d)*1000)))#
#
# mod.Surv.MCMC   <- MCMCglmm(Surv ~ 1, random=~Yeart  , family = "categorical", data = sim.data,#
# slice=TRUE, pr=TRUE,prior=prior)#
#
# mod.Surv.MCMC.1   <- MCMCglmm(Surv ~ z, random=~Yeart  , family = "categorical", data = sim.data,#
# slice=TRUE, pr=TRUE,prior=prior)#
#
# d<-2#
# prior=list(R=list(V=1, fix=1), G=list(G1=list(V=diag(d), nu=d, alpha.mu=rep(0,d), alpha.V=diag(d)*1000)))#
#
# mod.Surv.MCMC.2  <- MCMCglmm(Surv ~ z, random=~idh(1+z):Yeart  , family = "categorical", data = sim.data,#
# slice=TRUE, pr=TRUE,prior=prior)#
#
# mod.Surv.MCMC.3   <- MCMCglmm(Surv ~ z, random=~us(1+z):Yeart  , family = "categorical", data = sim.data,#
# slice=TRUE, pr=TRUE,prior=prior)#
#
# post.modes <- posterior.mode(mod.Surv.MCMC.2$Sol)#
#
# intercepts.MCMC <- post.modes["(Intercept)"] + post.modes[3:22]#
#
# slopes.MCMC <- post.modes["z"] + post.modes[23:42]#
#
# par(mfrow=c(1,2),bty="l",pty="s",pch=19)#
#
# plot(intercepts.MCMC,coef(mod.Surv.glmer.2)$Yeart[,"(Intercept)"])#
# abline(0,1,col="red")#
# plot(slopes.MCMC,coef(mod.Surv.glmer.2)$Yeart[,"z"])#
# abline(0,1,col="red")#
#fit some flowering models#
#
flow.data <- subset(sim.data,Surv==1)#
#
mod.Flow <- glm(Flow ~ Yeart  , family = binomial, data = flow.data)#
mod.Flow.1 <- glm(Flow ~ Yeart + z  , family = binomial, data = flow.data)#
mod.Flow.2 <- glm(Flow ~ Yeart * z  , family = binomial, data = flow.data)#
#
anova(mod.Flow,mod.Flow.1,mod.Flow.2,test="Chisq")#
AIC(mod.Flow,mod.Flow.1,mod.Flow.2)#
#
#No interaction term, as expected, refit to get paramter estimates easily#
#
mod.Flow <- glm(Flow ~ Yeart + z -1 , family = binomial, data = flow.data)#
#
#Let's refit using glmer#
#
mod.Flow.glmer   <- glmer(Flow ~ 1 + (1|Yeart)  , family = binomial, data = flow.data)#
mod.Flow.glmer.1 <- glmer(Flow ~ z + (1|Yeart)  , family = binomial, data = flow.data)#
mod.Flow.glmer.2 <- glmer(Flow ~ z + (1|Yeart) + (0 + z|Yeart)  , family = binomial, data = flow.data)#
mod.Flow.glmer.3 <- glmer(Flow ~ z + (z|Yeart)  , family = binomial, data = flow.data)#
anova(mod.Flow.glmer,mod.Flow.glmer.1,mod.Flow.glmer.2,mod.Flow.glmer.3)#
#
#fit some growth models#
#
grow.data <- subset(sim.data,Surv==1 & Flow==0)#
#
mod.Grow <- lm(z1 ~ Yeart  , data = grow.data)#
mod.Grow.1 <- lm(z1 ~ Yeart +z , data = grow.data)#
mod.Grow.2 <- lm(z1 ~ Yeart *z , data = grow.data)#
#
anova(mod.Grow,mod.Grow.1,mod.Grow.2)#
AIC(mod.Grow,mod.Grow.1,mod.Grow.2)#
#
mod.Grow <- lm(z1 ~ Yeart/z-1  , data = grow.data)#
#
#Let's refit using lmer#
#
mod.Grow.lmer   <- lmer(z1 ~ 1 + (1|Yeart)  ,  data = grow.data, REML=FALSE)#
mod.Grow.lmer.1 <- lmer(z1 ~ z + (1|Yeart)  ,  data = grow.data, REML=FALSE)#
mod.Grow.lmer.2 <- lmer(z1 ~ z + (1|Yeart) + (0 + z|Yeart)  ,  data = grow.data, REML=FALSE)#
mod.Grow.lmer.3 <- lmer(z1 ~ z + (z|Yeart)  ,  data = grow.data, REML=FALSE)#
anova(mod.Grow.lmer,mod.Grow.lmer.1,mod.Grow.lmer.2,mod.Grow.lmer.3)#
#
#fit some recruit size models#
#
mod.Rcsz <- lm(z ~ 1  , data = recr.data)#
mod.Rcsz.1 <- lm(z ~ Yeart , data = recr.data)#
#
anova(mod.Rcsz,mod.Rcsz.1)#
AIC(mod.Rcsz,mod.Rcsz.1)#
#
mod.Rcsz <- lm(z ~ Yeart -1, data = recr.data)#
#
#Let's refit using lmer#
#
mod.Rcsz.lmer <- lmer(z ~ 1 + (1|Yeart) , data = recr.data)#
#
#quick check#
#
plot(as.numeric(coef(mod.Rcsz)),unlist(coef(mod.Rcsz.lmer)$Yeart))#
abline(0,1)#
#set up parameter vector with yearly estimates from fixed effects models (lm and glm)#
m.par.est <- matrix(NA,nrow=12,ncol=20)#
m.par.est[1,] <- coef(mod.Surv)[1:20]#
m.par.est[2,] <- coef(mod.Surv)[21:40]#
m.par.est[3,] <- coef(mod.Flow)[1:20]#
m.par.est[4,] <- coef(mod.Flow)[21]#
m.par.est[5,] <- coef(mod.Grow)[1:20]#
m.par.est[6,] <- coef(mod.Grow)[21:40]#
m.par.est[7,] <- summary(mod.Grow)$sigma#
m.par.est[8,] <- coef(mod.Rcsz)[1:20]#
m.par.est[9,] <- summary(mod.Rcsz)$sigma#
m.par.est[10,] <- 1#
m.par.est[11,] <- 2#
m.par.est[12,] <- 0.00095#
#
#set up parameter vector with yearly estimates from mixed effects models (lmer and glmer)#
m.par.est.mm <- matrix(NA,nrow=12,ncol=20)#
m.par.est.mm[1,] <- as.vector(unlist(coef(mod.Surv.glmer.2 )$Yeart["(Intercept)"]))#
m.par.est.mm[2,] <- as.vector(unlist(coef(mod.Surv.glmer.2 )$Yeart["z"]))#
m.par.est.mm[3,] <- as.vector(unlist(coef(mod.Flow.glmer.1 )$Yeart["(Intercept)"]))#
m.par.est.mm[4,] <- as.vector(unlist(coef(mod.Flow.glmer.1 )$Yeart["z"]))#
m.par.est.mm[5,] <- as.vector(unlist(coef(mod.Grow.lmer.2)$Yeart["(Intercept)"]))#
m.par.est.mm[6,] <- as.vector(unlist(coef(mod.Grow.lmer.2)$Yeart["z"]))#
m.par.est.mm[7,] <- as.vector(unlist(summary(mod.Grow.lmer.2)$sigma))#
m.par.est.mm[8,] <- as.vector(unlist(coef(mod.Rcsz.lmer)$Yeart["(Intercept)"]))#
m.par.est.mm[9,] <- as.vector(unlist(summary(mod.Rcsz.lmer)$sigma))#
m.par.est.mm[10,] <- 1#
m.par.est.mm[11,] <- 2#
m.par.est.mm[12,] <- 0.00095#
plot(m.par.est,m.par.est.mm)#
#
source("Carlina Demog Funs DI.R") #
#
rownames(m.par.est) <- names(m.par.true)#
rownames(m.par.est.mm) <- names(m.par.true)#
#
#test correlations between yearly parameters#
cor.test(m.par.est[1,],m.par.est[3,])#
cor.test(m.par.est[1,],m.par.est[5,])#
cor.test(m.par.est[1,],m.par.est[8,])#
#
cor.test(m.par.est[3,],m.par.est[5,])#
cor.test(m.par.est[3,],m.par.est[8,])#
#
cor.test(m.par.est["grow.int",],m.par.est["rcsz.int",])#
plot(m.par.est["grow.int",],m.par.est["rcsz.int",])#
######################################################################
#LTRE analysis#
######################################################################
#
nBigMatrix <- 100#
n.est <- 20000#
n.runin <- 500#
minsize <- 1.5#
maxsize <- 5#
n.years <-20#
#
LTRE.calcs<-function(params,delta){#
#
	K.year.i     <- array(NA,c(n.years,nBigMatrix,nBigMatrix))#
	lambda.i  <- rep(NA,n.years)#
	n.params <- dim(m.par.est)[1]#
	for(i in 1:n.years){#
		year.K         <- mk_K(nBigMatrix,params[,i],minsize,maxsize)#
		lambda.i[i] <- Re(eigen(year.K$K,only.values=TRUE)$values[1])#
		K.year.i[i,,] <- year.K$K#
		cat(lambda.i[i],"\n")#
	}#
#
	h <- year.K$h; #
	meshpts <- year.K$meshpts#
#Calculate mean kernel, and mean of each params#
#
	mean.kernel   <- apply(K.year.i,2:3,mean)#
	mean.params <- apply(params,1,mean)#
#Calculate sensitivities to mean kernel for each parameter#
#
parm.to.pert         <- 1:n.params#
d.lambda.d.theta <- rep(NA,n.params)#
#
for(i in 1:n.params){#
	p.pert                 <- 1 + (i==parm.to.pert)*delta*mean.params[i]#
	K.up                    <- mk_K(nBigMatrix,p.pert*mean.params,minsize,maxsize)$K#
	lambda.up         <- Re(eigen(K.up,only.values=TRUE)$values[1])#
	p.pert                 <- 1 - (i==parm.to.pert)*delta*mean.params[i]#
	K.down               <-mk_K(nBigMatrix,p.pert*mean.params,minsize,maxsize)$K#
	lambda.down    <- Re(eigen(K.down,only.values=TRUE)$values[1])#
	d.lambda.d.theta[i] = 	(lambda.up - lambda.down) / (2*delta*mean.params[i])#
	}#
	LTRE.data           <- cbind(params,d.lambda.d.theta)#
	LTRE.data.steve <- rbind(params,lambda.i)#
	 return(list(meshpts=meshpts, h=h, mean.kernel=mean.kernel,LTRE.data =LTRE.data ,LTRE.data.steve=LTRE.data.steve))#
#
}#
LTRE.calcs(m.par.est,0.001)
delta=0.0001
params <- m.par.est
K.year.i     <- array(NA,c(n.years,nBigMatrix,nBigMatrix))#
	lambda.i  <- rep(NA,n.years)#
	n.params <- dim(m.par.est)[1]#
	for(i in 1:n.years){#
		year.K         <- mk_K(nBigMatrix,params[,i],minsize,maxsize)#
		lambda.i[i] <- Re(eigen(year.K$K,only.values=TRUE)$values[1])#
		K.year.i[i,,] <- year.K$K#
		cat(lambda.i[i],"\n")#
	}
h <- year.K$h; #
	meshpts <- year.K$meshpts#
#Calculate mean kernel, and mean of each params#
#
	mean.kernel   <- apply(K.year.i,2:3,mean)#
	mean.params <- apply(params,1,mean)
#Calculate sensitivities to mean kernel for each parameter#
#
parm.to.pert         <- 1:n.params#
d.lambda.d.theta <- rep(NA,n.params)#
#
for(i in 1:n.params){#
	p.pert                 <- 1 + (i==parm.to.pert)*delta*mean.params[i]#
	K.up                    <- mk_K(nBigMatrix,p.pert*mean.params,minsize,maxsize)$K#
	lambda.up         <- Re(eigen(K.up,only.values=TRUE)$values[1])#
	p.pert                 <- 1 - (i==parm.to.pert)*delta*mean.params[i]#
	K.down               <-mk_K(nBigMatrix,p.pert*mean.params,minsize,maxsize)$K#
	lambda.down    <- Re(eigen(K.down,only.values=TRUE)$values[1])#
	d.lambda.d.theta[i] = 	(lambda.up - lambda.down) / (2*delta*mean.params[i])#
	}#
	LTRE.data <- rbind(params,lambda.i)#
	LTRE.data.df<- data.frame(t(LTRE.data)); #
    names(LTRE.data.df) <- row.names(TRE.data)#
    which.vary<- which(apply(LTRE.data.df,2,sd)>0);#
    LTRE.data.df <- LTRE.data.df[,which.vary];
LTRE.data <- rbind(params,lambda.i)
LTRE.data.df<- data.frame(t(LTRE.data));
LTRE.data.df
LTRE.data <- rbind(params,lambda.i)
LTRE.data
LTRE.data.df<- data.frame(t(LTRE.data));
LTRE.data.df
which.vary<- which(apply(LTRE.data.df,2,sd)>0);#
    LTRE.data.df <- LTRE.data.df[,which.vary];
LTRE.data.df
######################################################################
#LTRE analysis#
######################################################################
#
nBigMatrix <- 100#
n.est <- 20000#
n.runin <- 500#
minsize <- 1.5#
maxsize <- 5#
n.years <-20#
#
LTRE.calcs<-function(params,delta){#
#
	K.year.i     <- array(NA,c(n.years,nBigMatrix,nBigMatrix))#
	lambda.i  <- rep(NA,n.years)#
	n.params <- dim(m.par.est)[1]#
	for(i in 1:n.years){#
		year.K         <- mk_K(nBigMatrix,params[,i],minsize,maxsize)#
		lambda.i[i] <- Re(eigen(year.K$K,only.values=TRUE)$values[1])#
		K.year.i[i,,] <- year.K$K#
		cat(lambda.i[i],"\n")#
	}#
#
	h <- year.K$h; #
	meshpts <- year.K$meshpts#
#Calculate mean kernel, and mean of each params#
#
	mean.kernel   <- apply(K.year.i,2:3,mean)#
	mean.params <- apply(params,1,mean)#
#Calculate sensitivities to mean kernel for each parameter#
#
parm.to.pert         <- 1:n.params#
d.lambda.d.theta <- rep(NA,n.params)#
#
for(i in 1:n.params){#
	p.pert                 <- 1 + (i==parm.to.pert)*delta*mean.params[i]#
	K.up                    <- mk_K(nBigMatrix,p.pert*mean.params,minsize,maxsize)$K#
	lambda.up         <- Re(eigen(K.up,only.values=TRUE)$values[1])#
	p.pert                 <- 1 - (i==parm.to.pert)*delta*mean.params[i]#
	K.down               <-mk_K(nBigMatrix,p.pert*mean.params,minsize,maxsize)$K#
	lambda.down    <- Re(eigen(K.down,only.values=TRUE)$values[1])#
	d.lambda.d.theta[i] = 	(lambda.up - lambda.down) / (2*delta*mean.params[i])#
	}#
	LTRE.data <- rbind(params,lambda.i)#
	LTRE.data.df<- data.frame(t(LTRE.data)); #
    #names(LTRE.data.df) <- row.names(LTRE.data)#
    which.vary<- which(apply(LTRE.data.df,2,sd)>0);#
    LTRE.data.df <- LTRE.data.df[,which.vary];  #
	 return(list(meshpts=meshpts, h=h, mean.kernel=mean.kernel,LTRE.data =LTRE.data.df ,d.lambda.d.theta=d.lambda.d.theta))#
#
}#
LTRE.calcs(m.par.est,0.001)
LTRE.calcs(m.par.est,0.001)
LTRE.df <- LTRE.calcs(m.par.est,0.001)$LTRE.data
LTRE.calcs<-function(params,delta){#
#
	K.year.i     <- array(NA,c(n.years,nBigMatrix,nBigMatrix))#
	lambda.i  <- rep(NA,n.years)#
	n.params <- dim(m.par.est)[1]#
	for(i in 1:n.years){#
		year.K         <- mk_K(nBigMatrix,params[,i],minsize,maxsize)#
		lambda.i[i] <- Re(eigen(year.K$K,only.values=TRUE)$values[1])#
		K.year.i[i,,] <- year.K$K#
		#cat(lambda.i[i],"\n")#
	}#
#
	h <- year.K$h; #
	meshpts <- year.K$meshpts#
#Calculate mean kernel, and mean of each params#
#
	mean.kernel   <- apply(K.year.i,2:3,mean)#
	mean.params <- apply(params,1,mean)#
#Calculate sensitivities to mean kernel for each parameter#
#
parm.to.pert         <- 1:n.params#
d.lambda.d.theta <- rep(NA,n.params)#
#
for(i in 1:n.params){#
	p.pert                 <- 1 + (i==parm.to.pert)*delta*mean.params[i]#
	K.up                    <- mk_K(nBigMatrix,p.pert*mean.params,minsize,maxsize)$K#
	lambda.up         <- Re(eigen(K.up,only.values=TRUE)$values[1])#
	p.pert                 <- 1 - (i==parm.to.pert)*delta*mean.params[i]#
	K.down               <-mk_K(nBigMatrix,p.pert*mean.params,minsize,maxsize)$K#
	lambda.down    <- Re(eigen(K.down,only.values=TRUE)$values[1])#
	d.lambda.d.theta[i] = 	(lambda.up - lambda.down) / (2*delta*mean.params[i])#
	}#
	LTRE.data <- rbind(params,lambda.i)#
	LTRE.data.df<- data.frame(t(LTRE.data)); #
    #names(LTRE.data.df) <- row.names(LTRE.data)#
    which.vary<- which(apply(LTRE.data.df,2,sd)>0);#
    LTRE.data.df <- LTRE.data.df[,which.vary];  #
	 return(list(meshpts=meshpts, h=h, mean.kernel=mean.kernel,LTRE.data =LTRE.data.df ,d.lambda.d.theta=d.lambda.d.theta))#
#
}#
LTRE.df <- LTRE.calcs(m.par.est,0.001)$LTRE.data
LTRE <- LTRE.calcs(m.par.est,0.001)$LTRE.data
LTRE <- LTRE.calcs(m.par.est,0.001)
LTRE
LTRE$$LTRE.data
LTRE$LTRE.data
LTRE.calcs<-function(params,delta){#
#
	K.year.i     <- array(NA,c(n.years,nBigMatrix,nBigMatrix))#
	lambda.i  <- rep(NA,n.years)#
	n.params <- dim(m.par.est)[1]#
	for(i in 1:n.years){#
		year.K         <- mk_K(nBigMatrix,params[,i],minsize,maxsize)#
		lambda.i[i] <- Re(eigen(year.K$K,only.values=TRUE)$values[1])#
		K.year.i[i,,] <- year.K$K#
		#cat(lambda.i[i],"\n")#
	}#
#
	h <- year.K$h; #
	meshpts <- year.K$meshpts#
#Calculate mean kernel, and mean of each params#
#
	mean.kernel   <- apply(K.year.i,2:3,mean)#
	mean.params <- apply(params,1,mean)#
#Calculate sensitivities to mean kernel for each parameter#
#
parm.to.pert         <- 1:n.params#
d.lambda.d.theta <- rep(NA,n.params)#
#
for(i in 1:n.params){#
	p.pert                 <- 1 + (i==parm.to.pert)*delta*mean.params[i]#
	K.up                    <- mk_K(nBigMatrix,p.pert*mean.params,minsize,maxsize)$K#
	lambda.up         <- Re(eigen(K.up,only.values=TRUE)$values[1])#
	p.pert                 <- 1 - (i==parm.to.pert)*delta*mean.params[i]#
	K.down               <-mk_K(nBigMatrix,p.pert*mean.params,minsize,maxsize)$K#
	lambda.down    <- Re(eigen(K.down,only.values=TRUE)$values[1])#
	d.lambda.d.theta[i] = 	(lambda.up - lambda.down) / (2*delta*mean.params[i])#
	}#
	LTRE.data <- rbind(params,lambda.i)#
	LTRE.data.df<- data.frame(t(LTRE.data)); #
    #names(LTRE.data.df) <- row.names(LTRE.data)#
    which.vary<- which(apply(LTRE.data.df,2,sd)>0);#
    LTRE.data.df <- LTRE.data.df[,which.vary];  #
	d.lambda.d.theta <- d.lambda.d.theta[which.vary]#
	 return(list(meshpts=meshpts, h=h, mean.kernel=mean.kernel,LTRE.data =LTRE.data.df ,d.lambda.d.theta=d.lambda.d.theta))#
#
}#
LTRE.nums <- LTRE.calcs(m.par.est,0.001)
LTRE.nums
LTRE.calcs<-function(params,delta){#
#
	K.year.i     <- array(NA,c(n.years,nBigMatrix,nBigMatrix))#
	lambda.i  <- rep(NA,n.years)#
	n.params <- dim(m.par.est)[1]#
	for(i in 1:n.years){#
		year.K         <- mk_K(nBigMatrix,params[,i],minsize,maxsize)#
		lambda.i[i] <- Re(eigen(year.K$K,only.values=TRUE)$values[1])#
		K.year.i[i,,] <- year.K$K#
		#cat(lambda.i[i],"\n")#
	}#
#
	h <- year.K$h; #
	meshpts <- year.K$meshpts#
#Calculate mean kernel, and mean of each params#
#
	mean.kernel   <- apply(K.year.i,2:3,mean)#
	mean.params <- apply(params,1,mean)#
#Calculate sensitivities to mean kernel for each parameter#
#
parm.to.pert         <- 1:n.params#
d.lambda.d.theta <- rep(NA,n.params)#
#
for(i in 1:n.params){#
	p.pert                 <- 1 + (i==parm.to.pert)*delta*mean.params[i]#
	K.up                    <- mk_K(nBigMatrix,p.pert*mean.params,minsize,maxsize)$K#
	lambda.up         <- Re(eigen(K.up,only.values=TRUE)$values[1])#
	p.pert                 <- 1 - (i==parm.to.pert)*delta*mean.params[i]#
	K.down               <-mk_K(nBigMatrix,p.pert*mean.params,minsize,maxsize)$K#
	lambda.down    <- Re(eigen(K.down,only.values=TRUE)$values[1])#
	d.lambda.d.theta[i] = 	(lambda.up - lambda.down) / (2*delta*mean.params[i])#
	}#
	LTRE.data <- rbind(params,lambda.i)#
	LTRE.data.df<- data.frame(t(LTRE.data)); #
    #names(LTRE.data.df) <- row.names(LTRE.data)#
    which.vary<- which(apply(LTRE.data.df,2,sd)>0);#
    LTRE.data.df <- LTRE.data.df[,which.vary];  #
	#d.lambda.d.theta <- d.lambda.d.theta[which.vary]#
	 return(list(meshpts=meshpts, h=h, mean.kernel=mean.kernel,LTRE.data =LTRE.data.df ,d.lambda.d.theta=d.lambda.d.theta,which.vary=which.vary))#
#
}#
LTRE.nums <- LTRE.calcs(m.par.est,0.001)
LTRE.nums
which.vary<- which(apply(params,1,sd)>0);
which.vary
which.vary<- which(apply(params,1,sd)>0);	#
	LTRE.data <- rbind(params[which.vary,],lambda.i)
LTRE.data
LTRE.data.df<- data.frame(t(LTRE.data));
which.vary<- which(apply(params,1,sd)>0);	#
	LTRE.data <- rbind(params[which.vary,],lambda.i)#
	LTRE.data.df<- data.frame(t(LTRE.data)); #
    #names(LTRE.data.df) <- row.names(LTRE.data)#
	d.lambda.d.theta <- d.lambda.d.theta[which.vary]
d.lambda.d.theta
params <- m.par.est
delta = 0.0001
K.year.i     <- array(NA,c(n.years,nBigMatrix,nBigMatrix))#
	lambda.i  <- rep(NA,n.years)#
	n.params <- dim(m.par.est)[1]#
	for(i in 1:n.years){#
		year.K         <- mk_K(nBigMatrix,params[,i],minsize,maxsize)#
		lambda.i[i] <- Re(eigen(year.K$K,only.values=TRUE)$values[1])#
		K.year.i[i,,] <- year.K$K#
		#cat(lambda.i[i],"\n")#
	}#
#
	h <- year.K$h; #
	meshpts <- year.K$meshpts#
#Calculate mean kernel, and mean of each params#
#
	mean.kernel   <- apply(K.year.i,2:3,mean)#
	mean.params <- apply(params,1,mean)#
#Calculate sensitivities to mean kernel for each parameter#
#
parm.to.pert         <- 1:n.params#
d.lambda.d.theta <- rep(NA,n.params)
for(i in 1:n.params){#
	p.pert                 <- 1 + (i==parm.to.pert)*delta*mean.params[i]#
	K.up                    <- mk_K(nBigMatrix,p.pert*mean.params,minsize,maxsize)$K#
	lambda.up         <- Re(eigen(K.up,only.values=TRUE)$values[1])#
	p.pert                 <- 1 - (i==parm.to.pert)*delta*mean.params[i]#
	K.down               <-mk_K(nBigMatrix,p.pert*mean.params,minsize,maxsize)$K#
	lambda.down    <- Re(eigen(K.down,only.values=TRUE)$values[1])#
	d.lambda.d.theta[i] = 	(lambda.up - lambda.down) / (2*delta*mean.params[i])#
	}#
	rownames(d.lambda.d.theta) <- names(params)
d.lambda.d.theta
names(params)
rownames(d.lambda.d.theta) <- rownames(params)
names(d.lambda.d.theta) <- rownames(params)
d.lambda.d.theta
######################################################################
#LTRE analysis#
######################################################################
#
nBigMatrix <- 100#
n.est <- 20000#
n.runin <- 500#
minsize <- 1.5#
maxsize <- 5#
n.years <-20#
#
LTRE.calcs<-function(params,delta){#
#
	K.year.i     <- array(NA,c(n.years,nBigMatrix,nBigMatrix))#
	lambda.i  <- rep(NA,n.years)#
	n.params <- dim(m.par.est)[1]#
	for(i in 1:n.years){#
		year.K         <- mk_K(nBigMatrix,params[,i],minsize,maxsize)#
		lambda.i[i] <- Re(eigen(year.K$K,only.values=TRUE)$values[1])#
		K.year.i[i,,] <- year.K$K#
		#cat(lambda.i[i],"\n")#
	}#
#
	h <- year.K$h; #
	meshpts <- year.K$meshpts#
#Calculate mean kernel, and mean of each params#
#
	mean.kernel   <- apply(K.year.i,2:3,mean)#
	mean.params <- apply(params,1,mean)#
#Calculate sensitivities to mean kernel for each parameter#
#
parm.to.pert         <- 1:n.params#
d.lambda.d.theta <- (NA,n.params)#
#
for(i in 1:n.params){#
	p.pert                 <- 1 + (i==parm.to.pert)*delta*mean.params[i]#
	K.up                    <- mk_K(nBigMatrix,p.pert*mean.params,minsize,maxsize)$K#
	lambda.up         <- Re(eigen(K.up,only.values=TRUE)$values[1])#
	p.pert                 <- 1 - (i==parm.to.pert)*delta*mean.params[i]#
	K.down               <-mk_K(nBigMatrix,p.pert*mean.params,minsize,maxsize)$K#
	lambda.down    <- Re(eigen(K.down,only.values=TRUE)$values[1])#
	d.lambda.d.theta[i] = 	(lambda.up - lambda.down) / (2*delta*mean.params[i])#
	}#
	names(d.lambda.d.theta) <- rownames(params)#
	which.vary<- which(apply(params,1,sd)>0);	#
	LTRE.data <- rbind(params[which.vary,],lambda.i)#
	LTRE.data.df<- data.frame(t(LTRE.data)); #
    #names(LTRE.data.df) <- row.names(LTRE.data)#
	d.lambda.d.theta <- d.lambda.d.theta[which.vary]#
	 return(list(meshpts=meshpts, h=h, mean.kernel=mean.kernel,LTRE.data =LTRE.data.df ,d.lambda.d.theta=d.lambda.d.theta))#
#
}#
LTRE.nums <- LTRE.calcs(m.par.est,0.001)
LTRE.calcs<-function(params,delta){#
#
	K.year.i     <- array(NA,c(n.years,nBigMatrix,nBigMatrix))#
	lambda.i  <- rep(NA,n.years)#
	n.params <- dim(m.par.est)[1]#
	for(i in 1:n.years){#
		year.K         <- mk_K(nBigMatrix,params[,i],minsize,maxsize)#
		lambda.i[i] <- Re(eigen(year.K$K,only.values=TRUE)$values[1])#
		K.year.i[i,,] <- year.K$K#
		#cat(lambda.i[i],"\n")#
	}#
#
	h <- year.K$h; #
	meshpts <- year.K$meshpts#
#Calculate mean kernel, and mean of each params#
#
	mean.kernel   <- apply(K.year.i,2:3,mean)#
	mean.params <- apply(params,1,mean)#
#Calculate sensitivities to mean kernel for each parameter#
#
parm.to.pert         <- 1:n.params#
d.lambda.d.theta <- (NA,n.params)#
#
for(i in 1:n.params){#
	p.pert                 <- 1 + (i==parm.to.pert)*delta*mean.params[i]#
	K.up                    <- mk_K(nBigMatrix,p.pert*mean.params,minsize,maxsize)$K#
	lambda.up         <- Re(eigen(K.up,only.values=TRUE)$values[1])#
	p.pert                 <- 1 - (i==parm.to.pert)*delta*mean.params[i]#
	K.down               <-mk_K(nBigMatrix,p.pert*mean.params,minsize,maxsize)$K#
	lambda.down    <- Re(eigen(K.down,only.values=TRUE)$values[1])#
	d.lambda.d.theta[i] = 	(lambda.up - lambda.down) / (2*delta*mean.params[i])#
	}#
	names(d.lambda.d.theta) <- rownames(params)#
	which.vary<- which(apply(params,1,sd)>0);	#
	LTRE.data <- rbind(params[which.vary,],lambda.i)#
	LTRE.data.df<- data.frame(t(LTRE.data)); #
    #names(LTRE.data.df) <- row.names(LTRE.data)#
	d.lambda.d.theta <- d.lambda.d.theta[which.vary]#
	 return(list(meshpts=meshpts, h=h,mean.kernel=mean.kernel,LTRE.data=LTRE.data.df ,#
	 d.lambda.d.theta=d.lambda.d.theta))#
#
}#
LTRE.nums <- LTRE.calcs(m.par.est,0.001)
K.year.i     <- array(NA,c(n.years,nBigMatrix,nBigMatrix))#
	lambda.i  <- rep(NA,n.years)#
	n.params <- dim(m.par.est)[1]#
	for(i in 1:n.years){#
		year.K         <- mk_K(nBigMatrix,params[,i],minsize,maxsize)#
		lambda.i[i] <- Re(eigen(year.K$K,only.values=TRUE)$values[1])#
		K.year.i[i,,] <- year.K$K#
		#cat(lambda.i[i],"\n")#
	}#
#
	h <- year.K$h; #
	meshpts <- year.K$meshpts#
#Calculate mean kernel, and mean of each params#
#
	mean.kernel   <- apply(K.year.i,2:3,mean)#
	mean.params <- apply(params,1,mean)#
#Calculate sensitivities to mean kernel for each parameter#
#
parm.to.pert         <- 1:n.params#
d.lambda.d.theta <- (NA,n.params)#
#
for(i in 1:n.params){#
	p.pert                 <- 1 + (i==parm.to.pert)*delta*mean.params[i]#
	K.up                    <- mk_K(nBigMatrix,p.pert*mean.params,minsize,maxsize)$K#
	lambda.up         <- Re(eigen(K.up,only.values=TRUE)$values[1])#
	p.pert                 <- 1 - (i==parm.to.pert)*delta*mean.params[i]#
	K.down               <-mk_K(nBigMatrix,p.pert*mean.params,minsize,maxsize)$K#
	lambda.down    <- Re(eigen(K.down,only.values=TRUE)$values[1])#
	d.lambda.d.theta[i] = 	(lambda.up - lambda.down) / (2*delta*mean.params[i])#
	}#
	names(d.lambda.d.theta) <- rownames(params)
LTRE.calcs<-function(params,delta){#
#
	K.year.i     <- array(NA,c(n.years,nBigMatrix,nBigMatrix))#
	lambda.i  <- rep(NA,n.years)#
	n.params <- dim(m.par.est)[1]#
	for(i in 1:n.years){#
		year.K         <- mk_K(nBigMatrix,params[,i],minsize,maxsize)#
		lambda.i[i] <- Re(eigen(year.K$K,only.values=TRUE)$values[1])#
		K.year.i[i,,] <- year.K$K#
		#cat(lambda.i[i],"\n")#
	}#
#
	h <- year.K$h; #
	meshpts <- year.K$meshpts#
#Calculate mean kernel, and mean of each params#
#
	mean.kernel   <- apply(K.year.i,2:3,mean)#
	mean.params <- apply(params,1,mean)#
#Calculate sensitivities to mean kernel for each parameter#
#
parm.to.pert         <- 1:n.params#
d.lambda.d.theta <- rep(NA,n.params)#
#
for(i in 1:n.params){#
	p.pert                 <- 1 + (i==parm.to.pert)*delta*mean.params[i]#
	K.up                    <- mk_K(nBigMatrix,p.pert*mean.params,minsize,maxsize)$K#
	lambda.up         <- Re(eigen(K.up,only.values=TRUE)$values[1])#
	p.pert                 <- 1 - (i==parm.to.pert)*delta*mean.params[i]#
	K.down               <-mk_K(nBigMatrix,p.pert*mean.params,minsize,maxsize)$K#
	lambda.down    <- Re(eigen(K.down,only.values=TRUE)$values[1])#
	d.lambda.d.theta[i] = 	(lambda.up - lambda.down) / (2*delta*mean.params[i])#
	}#
	names(d.lambda.d.theta) <- rownames(params)#
	which.vary<- which(apply(params,1,sd)>0);	#
	LTRE.data <- rbind(params[which.vary,],lambda.i)#
	LTRE.data.df<- data.frame(t(LTRE.data)); #
    #names(LTRE.data.df) <- row.names(LTRE.data)#
	d.lambda.d.theta <- d.lambda.d.theta[which.vary]#
	 return(list(meshpts=meshpts, h=h,mean.kernel=mean.kernel,LTRE.data=LTRE.data.df,#
	 d.lambda.d.theta=d.lambda.d.theta))#
#
}#
LTRE.nums <- LTRE.calcs(m.par.est,0.001)
LTRE.nums$LTRE.data
LTRE.nums$d.lambda.d.theta
sens.mat <- outer(LTRE.nums$d.lambda.d.theta,LTRE.nums$d.lambda.d.theta,FUN="*")
sens.mat
sens.mat <- outer(LTRE.nums$d.lambda.d.theta,LTRE.nums$d.lambda.d.theta,FUN="*")#
#
var.cov <- cov(LTRE.nums$LTRE.data[,1:6])
var.cov
sens.mat*var.cov
sens.mat*var.cov#
var(LTRE.nums$LTRE.data[,7])#
sum(sens.mat*var.cov)
LTRE.nums$LTRE.data[,7]
######################################################################
#LTRE analysis#
######################################################################
#
nBigMatrix <- 100#
n.est <- 20000#
n.runin <- 500#
minsize <- 1.5#
maxsize <- 5#
n.years <-20#
#
LTRE.calcs<-function(params,delta){#
#
	K.year.i     <- array(NA,c(n.years,nBigMatrix,nBigMatrix))#
	lambda.i  <- rep(NA,n.years)#
	n.params <- dim(m.par.est)[1]#
	for(i in 1:n.years){#
		year.K         <- mk_K(nBigMatrix,params[,i],minsize,maxsize)#
		lambda.i[i] <- Re(eigen(year.K$K,only.values=TRUE)$values[1])#
		K.year.i[i,,] <- year.K$K#
		#cat(lambda.i[i],"\n")#
	}#
#
	h <- year.K$h; #
	meshpts <- year.K$meshpts#
#Calculate mean kernel, and mean of each params#
#
	mean.kernel   <- apply(K.year.i,2:3,mean)#
	mean.params <- apply(params,1,mean)#
#Calculate sensitivities to mean kernel for each parameter#
#
parm.to.pert         <- 1:n.params#
d.lambda.d.theta <- rep(NA,n.params)#
#
for(i in 1:n.params){#
	p.pert                 <- 1 + (i==parm.to.pert)*delta*mean.params[i]#
	K.up                    <- mk_K(nBigMatrix,p.pert*mean.params,minsize,maxsize)$K#
	lambda.up         <- Re(eigen(K.up,only.values=TRUE)$values[1])#
	p.pert                 <- 1 - (i==parm.to.pert)*delta*mean.params[i]#
	K.down               <-mk_K(nBigMatrix,p.pert*mean.params,minsize,maxsize)$K#
	lambda.down    <- Re(eigen(K.down,only.values=TRUE)$values[1])#
	d.lambda.d.theta[i] = 	(lambda.up - lambda.down) / (2*delta*mean.params[i])#
	}#
	names(d.lambda.d.theta) <- rownames(params)#
	which.vary<- which(apply(params,1,sd)>0);	#
	LTRE.data <- rbind(params[which.vary,],lambda.i)#
	LTRE.data.df<- data.frame(t(LTRE.data)); #
    #names(LTRE.data.df) <- row.names(LTRE.data)#
	d.lambda.d.theta <- d.lambda.d.theta[which.vary]#
	 return(list(meshpts=meshpts, h=h,mean.kernel=mean.kernel,LTRE.data=LTRE.data.df,#
	 d.lambda.d.theta=d.lambda.d.theta))#
#
}#
LTRE.nums <- LTRE.calcs(m.par.est,0.000001)#
#
sens.mat <- outer(LTRE.nums$d.lambda.d.theta,LTRE.nums$d.lambda.d.theta,FUN="*")#
#
var.cov <- cov(LTRE.nums$LTRE.data[,1:6])#
#
sens.mat*var.cov#
var(LTRE.nums$LTRE.data[,7])#
sum(sens.mat*var.cov)
load(file="Big param matrix.Rdata")#
#
rownames(store.params) <- names(m.par.true)#
#
n.years <-5000#
params.plus.lambda <- LTRE.calcs(store.params,0.001)
sens.mat <- outer(LTRE.nums$d.lambda.d.theta,LTRE.nums$d.lambda.d.theta,FUN="*")#
#
var.cov <- cov(LTRE.nums$LTRE.data[,1:6])#
#
sens.mat*var.cov#
var(LTRE.nums$LTRE.data[,7])#
sum(sens.mat*var.cov)
load(file="Big param matrix.Rdata")#
#
rownames(store.params) <- names(m.par.true)#
#
n.years <-5000#
LTRE.nums <- LTRE.calcs(store.params,0.001)
sens.mat <- outer(LTRE.nums$d.lambda.d.theta,LTRE.nums$d.lambda.d.theta,FUN="*")#
#
var.cov <- cov(LTRE.nums$LTRE.data[,1:6])#
#
sens.mat*var.cov#
var(LTRE.nums$LTRE.data[,7])#
sum(sens.mat*var.cov)
load(file="Big param matrix.Rdata")#
#
rownames(store.params) <- names(m.par.true)#
#
n.years <-5000#
LTRE.nums <- LTRE.calcs(store.params,0.000001)#
#
sens.mat <- outer(LTRE.nums$d.lambda.d.theta,LTRE.nums$d.lambda.d.theta,FUN="*")#
#
var.cov <- cov(LTRE.nums$LTRE.data[,1:6])#
#
sens.mat*var.cov#
var(LTRE.nums$LTRE.data[,7])#
sum(sens.mat*var.cov)
?cov2cor
cov2cor(var.cov)
load(file="Big param matrix.Rdata")
store.params
rownames(store.params) <- names(m.par.true)
store.params
n.years <-5000#
LTRE.nums <- LTRE.calcs(store.params,0.000001)#
#
sens.mat <- outer(LTRE.nums$d.lambda.d.theta,LTRE.nums$d.lambda.d.theta,FUN="*")#
#
var.cov <- cov(LTRE.nums$LTRE.data[,1:6])#
#
sens.mat*var.cov#
var(LTRE.nums$LTRE.data[,7])#
sum(sens.mat*var.cov)
LTRE.nums <- LTRE.calcs(m.par.est,0.000001)#
#
sens.mat <- outer(LTRE.nums$d.lambda.d.theta,LTRE.nums$d.lambda.d.theta,FUN="*")#
#
var.cov <- cov(LTRE.nums$LTRE.data[,1:6])#
#
sens.mat*var.cov#
var(LTRE.nums$LTRE.data[,7])#
sum(sens.mat*var.cov)
n.years <-20
LTRE.nums <- LTRE.calcs(m.par.est,0.000001)#
#
sens.mat <- outer(LTRE.nums$d.lambda.d.theta,LTRE.nums$d.lambda.d.theta,FUN="*")#
#
var.cov <- cov(LTRE.nums$LTRE.data[,1:6])#
#
sens.mat*var.cov#
var(LTRE.nums$LTRE.data[,7])#
sum(sens.mat*var.cov)
## ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~#
## Fit and run fixed and mixed effects effects Carlina stochastic IPM#
## ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~#
#
rm(list=ls(all=TRUE))#
#
library(doBy)#
library(lme4)#
library(MCMCglmm)#
set.seed(53241986)#
#
## Working directory must be set here, so the source()'s below run#
root=ifelse(.Platform$OS.type=="windows","c:/repos","~/Repos"); #
setwd(paste(root,"/ipm_book/Rcode/c2",sep="")); #
#
source("../utilities/Standard Graphical Pars.R");#
#
root=ifelse(.Platform$OS.type=="windows","c:/repos","~/Repos"); #
setwd(paste(root,"/ipm_book/Rcode/c7/Carlina",sep="")); #
#
#Read in the Carlina demographic data#
load("CarlinaIBMsim.Rdata")#
str(sim.data)#
#
store.sim.data <- sim.data#
#
#Select recruit data for the last 20 years (years 31 to 50)#
recr.data <- subset(sim.data,Recr==1)#
recr.data <- subset(recr.data,Yeart>30)#
recr.data <- transform(recr.data,Yeart = factor(Yeart - 30))#
#
#plant data for years 30 to 49, we do this to as the recruit size in #
#year t was generated by the year t-1 parameters#
sim.data <- subset(sim.data,Yeart>29 & Yeart<50)#
with(sim.data,table(Yeart,Recr))#
#Make Yeart a factor#
sim.data <- transform(sim.data,Yeart = factor(Yeart - 29))#
#fit some survival models#
#
mod.Surv <- glm(Surv ~ Yeart  , family = binomial, data = sim.data)#
mod.Surv.1 <- glm(Surv ~ Yeart + z  , family = binomial, data = sim.data)#
mod.Surv.2 <- glm(Surv ~ Yeart * z  , family = binomial, data = sim.data)#
#
anova(mod.Surv,mod.Surv.1,mod.Surv.2,test="Chisq")#
AIC(mod.Surv,mod.Surv.1,mod.Surv.2)#
#
#there is evidence of an interaction, as we might expect as both the intercept and slope vary from #
#year to year#
#
#Let's refit so we can easily get the parameter estimates#
#
mod.Surv <- glm(Surv ~ Yeart/ z -1 , family = binomial, data = sim.data)#
#
summary(mod.Surv)#
#
#Let's refit using glmer#
#
mod.Surv.glmer   <- glmer(Surv ~ 1 + (1|Yeart)  , family = binomial, data = sim.data)#
mod.Surv.glmer.1 <- glmer(Surv ~ z + (1|Yeart)  , family = binomial, data = sim.data)#
mod.Surv.glmer.2 <- glmer(Surv ~ z + (1|Yeart) + (0 + z|Yeart)  , family = binomial, data = sim.data)#
mod.Surv.glmer.3 <- glmer(Surv ~ z + (z|Yeart)  , family = binomial, data = sim.data)#
anova(mod.Surv.glmer,mod.Surv.glmer.1,mod.Surv.glmer.2,mod.Surv.glmer.3)#
# # d<-1#
# prior=list(R=list(V=1, fix=1), G=list(G1=list(V=diag(d), nu=d, alpha.mu=rep(0,d), alpha.V=diag(d)*1000)))#
#
# mod.Surv.MCMC   <- MCMCglmm(Surv ~ 1, random=~Yeart  , family = "categorical", data = sim.data,#
# slice=TRUE, pr=TRUE,prior=prior)#
#
# mod.Surv.MCMC.1   <- MCMCglmm(Surv ~ z, random=~Yeart  , family = "categorical", data = sim.data,#
# slice=TRUE, pr=TRUE,prior=prior)#
#
# d<-2#
# prior=list(R=list(V=1, fix=1), G=list(G1=list(V=diag(d), nu=d, alpha.mu=rep(0,d), alpha.V=diag(d)*1000)))#
#
# mod.Surv.MCMC.2  <- MCMCglmm(Surv ~ z, random=~idh(1+z):Yeart  , family = "categorical", data = sim.data,#
# slice=TRUE, pr=TRUE,prior=prior)#
#
# mod.Surv.MCMC.3   <- MCMCglmm(Surv ~ z, random=~us(1+z):Yeart  , family = "categorical", data = sim.data,#
# slice=TRUE, pr=TRUE,prior=prior)#
#
# post.modes <- posterior.mode(mod.Surv.MCMC.2$Sol)#
#
# intercepts.MCMC <- post.modes["(Intercept)"] + post.modes[3:22]#
#
# slopes.MCMC <- post.modes["z"] + post.modes[23:42]#
#
# par(mfrow=c(1,2),bty="l",pty="s",pch=19)#
#
# plot(intercepts.MCMC,coef(mod.Surv.glmer.2)$Yeart[,"(Intercept)"])#
# abline(0,1,col="red")#
# plot(slopes.MCMC,coef(mod.Surv.glmer.2)$Yeart[,"z"])#
# abline(0,1,col="red")#
#fit some flowering models#
#
flow.data <- subset(sim.data,Surv==1)#
#
mod.Flow <- glm(Flow ~ Yeart  , family = binomial, data = flow.data)#
mod.Flow.1 <- glm(Flow ~ Yeart + z  , family = binomial, data = flow.data)#
mod.Flow.2 <- glm(Flow ~ Yeart * z  , family = binomial, data = flow.data)#
#
anova(mod.Flow,mod.Flow.1,mod.Flow.2,test="Chisq")#
AIC(mod.Flow,mod.Flow.1,mod.Flow.2)#
#
#No interaction term, as expected, refit to get paramter estimates easily#
#
mod.Flow <- glm(Flow ~ Yeart + z -1 , family = binomial, data = flow.data)#
#
#Let's refit using glmer#
#
mod.Flow.glmer   <- glmer(Flow ~ 1 + (1|Yeart)  , family = binomial, data = flow.data)#
mod.Flow.glmer.1 <- glmer(Flow ~ z + (1|Yeart)  , family = binomial, data = flow.data)#
mod.Flow.glmer.2 <- glmer(Flow ~ z + (1|Yeart) + (0 + z|Yeart)  , family = binomial, data = flow.data)#
mod.Flow.glmer.3 <- glmer(Flow ~ z + (z|Yeart)  , family = binomial, data = flow.data)#
anova(mod.Flow.glmer,mod.Flow.glmer.1,mod.Flow.glmer.2,mod.Flow.glmer.3)#
#
#fit some growth models#
#
grow.data <- subset(sim.data,Surv==1 & Flow==0)#
#
mod.Grow <- lm(z1 ~ Yeart  , data = grow.data)#
mod.Grow.1 <- lm(z1 ~ Yeart +z , data = grow.data)#
mod.Grow.2 <- lm(z1 ~ Yeart *z , data = grow.data)#
#
anova(mod.Grow,mod.Grow.1,mod.Grow.2)#
AIC(mod.Grow,mod.Grow.1,mod.Grow.2)#
#
mod.Grow <- lm(z1 ~ Yeart/z-1  , data = grow.data)#
#
#Let's refit using lmer#
#
mod.Grow.lmer   <- lmer(z1 ~ 1 + (1|Yeart)  ,  data = grow.data, REML=FALSE)#
mod.Grow.lmer.1 <- lmer(z1 ~ z + (1|Yeart)  ,  data = grow.data, REML=FALSE)#
mod.Grow.lmer.2 <- lmer(z1 ~ z + (1|Yeart) + (0 + z|Yeart)  ,  data = grow.data, REML=FALSE)#
mod.Grow.lmer.3 <- lmer(z1 ~ z + (z|Yeart)  ,  data = grow.data, REML=FALSE)#
anova(mod.Grow.lmer,mod.Grow.lmer.1,mod.Grow.lmer.2,mod.Grow.lmer.3)#
#
#fit some recruit size models#
#
mod.Rcsz <- lm(z ~ 1  , data = recr.data)#
mod.Rcsz.1 <- lm(z ~ Yeart , data = recr.data)#
#
anova(mod.Rcsz,mod.Rcsz.1)#
AIC(mod.Rcsz,mod.Rcsz.1)#
#
mod.Rcsz <- lm(z ~ Yeart -1, data = recr.data)#
#
#Let's refit using lmer#
#
mod.Rcsz.lmer <- lmer(z ~ 1 + (1|Yeart) , data = recr.data)#
#
#quick check#
#
plot(as.numeric(coef(mod.Rcsz)),unlist(coef(mod.Rcsz.lmer)$Yeart))#
abline(0,1)#
#set up parameter vector with yearly estimates from fixed effects models (lm and glm)#
m.par.est <- matrix(NA,nrow=12,ncol=20)#
m.par.est[1,] <- coef(mod.Surv)[1:20]#
m.par.est[2,] <- coef(mod.Surv)[21:40]#
m.par.est[3,] <- coef(mod.Flow)[1:20]#
m.par.est[4,] <- coef(mod.Flow)[21]#
m.par.est[5,] <- coef(mod.Grow)[1:20]#
m.par.est[6,] <- coef(mod.Grow)[21:40]#
m.par.est[7,] <- summary(mod.Grow)$sigma#
m.par.est[8,] <- coef(mod.Rcsz)[1:20]#
m.par.est[9,] <- summary(mod.Rcsz)$sigma#
m.par.est[10,] <- 1#
m.par.est[11,] <- 2#
m.par.est[12,] <- 0.00095#
#
#set up parameter vector with yearly estimates from mixed effects models (lmer and glmer)#
m.par.est.mm <- matrix(NA,nrow=12,ncol=20)#
m.par.est.mm[1,] <- as.vector(unlist(coef(mod.Surv.glmer.2 )$Yeart["(Intercept)"]))#
m.par.est.mm[2,] <- as.vector(unlist(coef(mod.Surv.glmer.2 )$Yeart["z"]))#
m.par.est.mm[3,] <- as.vector(unlist(coef(mod.Flow.glmer.1 )$Yeart["(Intercept)"]))#
m.par.est.mm[4,] <- as.vector(unlist(coef(mod.Flow.glmer.1 )$Yeart["z"]))#
m.par.est.mm[5,] <- as.vector(unlist(coef(mod.Grow.lmer.2)$Yeart["(Intercept)"]))#
m.par.est.mm[6,] <- as.vector(unlist(coef(mod.Grow.lmer.2)$Yeart["z"]))#
m.par.est.mm[7,] <- as.vector(unlist(summary(mod.Grow.lmer.2)$sigma))#
m.par.est.mm[8,] <- as.vector(unlist(coef(mod.Rcsz.lmer)$Yeart["(Intercept)"]))#
m.par.est.mm[9,] <- as.vector(unlist(summary(mod.Rcsz.lmer)$sigma))#
m.par.est.mm[10,] <- 1#
m.par.est.mm[11,] <- 2#
m.par.est.mm[12,] <- 0.00095#
plot(m.par.est,m.par.est.mm)#
#
source("Carlina Demog Funs DI.R") #
#
rownames(m.par.est) <- names(m.par.true)#
rownames(m.par.est.mm) <- names(m.par.true)#
#
#test correlations between yearly parameters#
cor.test(m.par.est[1,],m.par.est[3,])#
cor.test(m.par.est[1,],m.par.est[5,])#
cor.test(m.par.est[1,],m.par.est[8,])#
#
cor.test(m.par.est[3,],m.par.est[5,])#
cor.test(m.par.est[3,],m.par.est[8,])#
#
cor.test(m.par.est["grow.int",],m.par.est["rcsz.int",])#
plot(m.par.est["grow.int",],m.par.est["rcsz.int",])#
######################################################################
#LTRE analysis#
######################################################################
#
nBigMatrix <- 100#
n.est <- 20000#
n.runin <- 500#
minsize <- 1.5#
maxsize <- 5#
n.years <-20#
#
LTRE.calcs<-function(params,delta){#
#
	K.year.i     <- array(NA,c(n.years,nBigMatrix,nBigMatrix))#
	lambda.i  <- rep(NA,n.years)#
	n.params <- dim(m.par.est)[1]#
	for(i in 1:n.years){#
		year.K         <- mk_K(nBigMatrix,params[,i],minsize,maxsize)#
		lambda.i[i] <- Re(eigen(year.K$K,only.values=TRUE)$values[1])#
		K.year.i[i,,] <- year.K$K#
		#cat(lambda.i[i],"\n")#
	}#
#
	h <- year.K$h; #
	meshpts <- year.K$meshpts#
#Calculate mean kernel, and mean of each params#
#
	mean.kernel   <- apply(K.year.i,2:3,mean)#
	mean.params <- apply(params,1,mean)#
#Calculate sensitivities to mean kernel for each parameter#
#
parm.to.pert         <- 1:n.params#
d.lambda.d.theta <- rep(NA,n.params)#
#
for(i in 1:n.params){#
	p.pert                 <- 1 + (i==parm.to.pert)*delta*mean.params[i]#
	K.up                    <- mk_K(nBigMatrix,p.pert*mean.params,minsize,maxsize)$K#
	lambda.up         <- Re(eigen(K.up,only.values=TRUE)$values[1])#
	p.pert                 <- 1 - (i==parm.to.pert)*delta*mean.params[i]#
	K.down               <-mk_K(nBigMatrix,p.pert*mean.params,minsize,maxsize)$K#
	lambda.down    <- Re(eigen(K.down,only.values=TRUE)$values[1])#
	d.lambda.d.theta[i] = 	(lambda.up - lambda.down) / (2*delta*mean.params[i])#
	}#
	names(d.lambda.d.theta) <- rownames(params)#
	which.vary<- which(apply(params,1,sd)>0);	#
	LTRE.data <- rbind(params[which.vary,],lambda.i)#
	LTRE.data.df<- data.frame(t(LTRE.data)); #
    #names(LTRE.data.df) <- row.names(LTRE.data)#
	d.lambda.d.theta <- d.lambda.d.theta[which.vary]#
	 return(list(meshpts=meshpts, h=h,mean.kernel=mean.kernel,LTRE.data=LTRE.data.df,#
	 d.lambda.d.theta=d.lambda.d.theta))#
#
}#
LTRE.nums <- LTRE.calcs(m.par.est,0.000001)#
#
sens.mat <- outer(LTRE.nums$d.lambda.d.theta,LTRE.nums$d.lambda.d.theta,FUN="*")#
#
var.cov <- cov(LTRE.nums$LTRE.data[,1:6])#
#
sens.mat*var.cov#
var(LTRE.nums$LTRE.data[,7])#
sum(sens.mat*var.cov)
parm.to.pert         <- 1:n.params
n.params <- dim(params)[1]
params=m.par.est
n.params <- dim(params)[1]
for(i in 1:n.years){#
		year.K         <- mk_K(nBigMatrix,params[,i],minsize,maxsize)#
		lambda.i[i] <- Re(eigen(year.K$K,only.values=TRUE)$values[1])#
		K.year.i[i,,] <- year.K$K#
		#cat(lambda.i[i],"\n")#
	}#
#
	h <- year.K$h; #
	meshpts <- year.K$meshpts#
#Calculate mean kernel, and mean of each params#
#
	mean.kernel   <- apply(K.year.i,2:3,mean)#
	mean.params <- apply(params,1,mean)#
#Calculate sensitivities to mean kernel for each parameter#
#
parm.to.pert         <- 1:n.params#
d.lambda.d.theta <- rep(NA,n.params)
K.year.i     <- array(NA,c(n.years,nBigMatrix,nBigMatrix))#
	lambda.i  <- rep(NA,n.years)#
	n.params <- dim(params)[1]#
	for(i in 1:n.years){#
		year.K         <- mk_K(nBigMatrix,params[,i],minsize,maxsize)#
		lambda.i[i] <- Re(eigen(year.K$K,only.values=TRUE)$values[1])#
		K.year.i[i,,] <- year.K$K#
		#cat(lambda.i[i],"\n")#
	}#
#
	h <- year.K$h; #
	meshpts <- year.K$meshpts
#Calculate mean kernel, and mean of each params#
#
	mean.kernel   <- apply(K.year.i,2:3,mean)#
	mean.params <- apply(params,1,mean)#
#Calculate sensitivities to mean kernel for each parameter#
#
parm.to.pert         <- 1:n.params#
d.lambda.d.theta <- rep(NA,n.params)
i=1
(i==parm.to.pert)*delta*mean.params[i]
delta=0.1
(i==parm.to.pert)*delta*mean.params[i]
1 + (i==parm.to.pert)*delta*mean.params[i]
LTRE.calcs<-function(params,delta){#
#
	K.year.i     <- array(NA,c(n.years,nBigMatrix,nBigMatrix))#
	lambda.i  <- rep(NA,n.years)#
	n.params <- dim(params)[1]#
	for(i in 1:n.years){#
		year.K         <- mk_K(nBigMatrix,params[,i],minsize,maxsize)#
		lambda.i[i] <- Re(eigen(year.K$K,only.values=TRUE)$values[1])#
		K.year.i[i,,] <- year.K$K#
		#cat(lambda.i[i],"\n")#
	}#
#
	h <- year.K$h; #
	meshpts <- year.K$meshpts#
#Calculate mean kernel, and mean of each params#
#
	mean.kernel   <- apply(K.year.i,2:3,mean)#
	mean.params <- apply(params,1,mean)#
#Calculate sensitivities to mean kernel for each parameter#
#
parm.to.pert         <- 1:n.params#
d.lambda.d.theta <- rep(NA,n.params)#
#
for(i in 1:n.params){#
	p.pert                 <- 1 + (i==parm.to.pert)*delta*mean.params[i]#
	K.up                    <- mk_K(nBigMatrix,p.pert*mean.params,minsize,maxsize)$K#
	lambda.up         <- Re(eigen(K.up,only.values=TRUE)$values[1])#
	p.pert                 <- 1 - (i==parm.to.pert)*delta*mean.params[i]#
	K.down               <-mk_K(nBigMatrix,p.pert*mean.params,minsize,maxsize)$K#
	lambda.down    <- Re(eigen(K.down,only.values=TRUE)$values[1])#
	d.lambda.d.theta[i] = 	(lambda.up - lambda.down) / (2*delta*mean.params[i])#
	}#
	names(d.lambda.d.theta) <- rownames(params)#
	which.vary<- which(apply(params,1,sd)>0);	#
	LTRE.data <- rbind(params[which.vary,],lambda.i)#
	LTRE.data.df<- data.frame(t(LTRE.data)); #
    #names(LTRE.data.df) <- row.names(LTRE.data)#
	d.lambda.d.theta <- d.lambda.d.theta[which.vary]#
	 return(list(meshpts=meshpts, h=h,mean.kernel=mean.kernel,LTRE.data=LTRE.data.df,#
	 d.lambda.d.theta=d.lambda.d.theta))#
#
}#
LTRE.nums <- LTRE.calcs(m.par.est,0.000001)#
#
sens.mat <- outer(LTRE.nums$d.lambda.d.theta,LTRE.nums$d.lambda.d.theta,FUN="*")#
#
var.cov <- cov(LTRE.nums$LTRE.data[,1:6])#
#
sens.mat*var.cov#
var(LTRE.nums$LTRE.data[,7])#
sum(sens.mat*var.cov)
LTRE.nums$d.lambda.d.theta[1]
LTRE.nums$d.lambda.d.theta[1]^2
sens.mat
LTRE.nums$d.lambda.d.theta[1]*LTRE.nums$d.lambda.d.theta[2]
fit <- lm(LTRE.nums$LTRE.data[,7]~LTRE.nums$LTRE.data[,1:6])
LTRE.nums$LTRE.data[,1:6]
matrix(LTRE.nums$LTRE.data[,1:6])
as.matrix(LTRE.nums$LTRE.data[,1:6])
fit <- lm(LTRE.nums$LTRE.data[,7]~as.matrix(LTRE.nums$LTRE.data[,1:6]))
fit
summary(fit)
coef(fit)[2:7]
as.numeric(coef(fit)[2:7])
LTRE.nums$d.lambda.d.theta
fit <- lm(LTRE.nums$LTRE.data[,7]~as.matrix(LTRE.nums$LTRE.data[,1:6]))#
#
slopes <- as.numeric(coef(fit)[2:7])#
#
sens.mat <- outer(slopes,slopes,FUN="*")#
sens.mat*var.cov#
var(LTRE.nums$LTRE.data[,7])#
sum(sens.mat*var.cov)
plot(LTRE.nums$d.lambda.d.theta,slopes)
load(file="Big param matrix.Rdata")#
#
rownames(store.params) <- names(m.par.true)#
#
n.years <-5000#
LTRE.nums <- LTRE.calcs(store.params,0.000001)#
#
sens.mat <- outer(LTRE.nums$d.lambda.d.theta,LTRE.nums$d.lambda.d.theta,FUN="*")#
#
var.cov <- cov(LTRE.nums$LTRE.data[,1:6])#
#
sens.mat*var.cov#
var(LTRE.nums$LTRE.data[,7])#
sum(sens.mat*var.cov)#
#
fit <- lm(LTRE.nums$LTRE.data[,7]~as.matrix(LTRE.nums$LTRE.data[,1:6]))#
#
slopes <- as.numeric(coef(fit)[2:7])#
#
plot(LTRE.nums$d.lambda.d.theta,slopes)#
#
sens.mat <- outer(slopes,slopes,FUN="*")#
sens.mat*var.cov#
var(LTRE.nums$LTRE.data[,7])#
sum(sens.mat*var.cov)
summary(fit)$sigma
(summary(fit)$sigma)^2
sum(sens.mat*var.cov) + (summary(fit)$sigma)^2
var(LTRE.nums$LTRE.data[,7])#
sum(sens.mat*var.cov) + (summary(fit)$sigma)^2
load(file="Big param matrix.Rdata")#
#
rownames(store.params) <- names(m.par.true)#
#
n.years <-5000#
LTRE.nums <- LTRE.calcs(store.params,0.000001)#
#
sens.mat <- outer(LTRE.nums$d.lambda.d.theta,LTRE.nums$d.lambda.d.theta,FUN="*")#
#
var.cov <- cov(LTRE.nums$LTRE.data[,1:6])#
#
sens.mat*var.cov#
var(LTRE.nums$LTRE.data[,7])#
sum(sens.mat*var.cov)#
#
fit <- lm(LTRE.nums$LTRE.data[,7]~as.matrix(LTRE.nums$LTRE.data[,1:6]))#
#
slopes <- as.numeric(coef(fit)[2:7])#
#
plot(LTRE.nums$d.lambda.d.theta,slopes)#
#
sens.mat <- outer(slopes,slopes,FUN="*")#
sens.mat*var.cov#
var(LTRE.nums$LTRE.data[,7])#
#
sum(sens.mat*var.cov) + (summary(fit)$sigma)^2
## ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~#
## Fit and run fixed and mixed effects effects Carlina stochastic IPM#
## ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~#
#
rm(list=ls(all=TRUE))#
#
library(doBy)#
library(lme4)#
library(MCMCglmm)#
set.seed(53241986)#
#
## Working directory must be set here, so the source()'s below run#
root=ifelse(.Platform$OS.type=="windows","c:/repos","~/Repos"); #
setwd(paste(root,"/ipm_book/Rcode/c2",sep="")); #
#
source("../utilities/Standard Graphical Pars.R");#
#
root=ifelse(.Platform$OS.type=="windows","c:/repos","~/Repos"); #
setwd(paste(root,"/ipm_book/Rcode/c7/Carlina",sep="")); #
#
#Read in the Carlina demographic data#
load("CarlinaIBMsim.Rdata")#
str(sim.data)#
#
store.sim.data <- sim.data#
#
#Select recruit data for the last 20 years (years 31 to 50)#
recr.data <- subset(sim.data,Recr==1)#
recr.data <- subset(recr.data,Yeart>30)#
recr.data <- transform(recr.data,Yeart = factor(Yeart - 30))#
#
#plant data for years 30 to 49, we do this to as the recruit size in #
#year t was generated by the year t-1 parameters#
sim.data <- subset(sim.data,Yeart>29 & Yeart<50)#
with(sim.data,table(Yeart,Recr))#
#Make Yeart a factor#
sim.data <- transform(sim.data,Yeart = factor(Yeart - 29))#
#fit some survival models#
#
mod.Surv <- glm(Surv ~ Yeart  , family = binomial, data = sim.data)#
mod.Surv.1 <- glm(Surv ~ Yeart + z  , family = binomial, data = sim.data)#
mod.Surv.2 <- glm(Surv ~ Yeart * z  , family = binomial, data = sim.data)#
#
anova(mod.Surv,mod.Surv.1,mod.Surv.2,test="Chisq")#
AIC(mod.Surv,mod.Surv.1,mod.Surv.2)#
#
#there is evidence of an interaction, as we might expect as both the intercept and slope vary from #
#year to year#
#
#Let's refit so we can easily get the parameter estimates#
#
mod.Surv <- glm(Surv ~ Yeart/ z -1 , family = binomial, data = sim.data)#
#
summary(mod.Surv)#
#
#Let's refit using glmer#
#
mod.Surv.glmer   <- glmer(Surv ~ 1 + (1|Yeart)  , family = binomial, data = sim.data)#
mod.Surv.glmer.1 <- glmer(Surv ~ z + (1|Yeart)  , family = binomial, data = sim.data)#
mod.Surv.glmer.2 <- glmer(Surv ~ z + (1|Yeart) + (0 + z|Yeart)  , family = binomial, data = sim.data)#
mod.Surv.glmer.3 <- glmer(Surv ~ z + (z|Yeart)  , family = binomial, data = sim.data)#
anova(mod.Surv.glmer,mod.Surv.glmer.1,mod.Surv.glmer.2,mod.Surv.glmer.3)#
# # d<-1#
# prior=list(R=list(V=1, fix=1), G=list(G1=list(V=diag(d), nu=d, alpha.mu=rep(0,d), alpha.V=diag(d)*1000)))#
#
# mod.Surv.MCMC   <- MCMCglmm(Surv ~ 1, random=~Yeart  , family = "categorical", data = sim.data,#
# slice=TRUE, pr=TRUE,prior=prior)#
#
# mod.Surv.MCMC.1   <- MCMCglmm(Surv ~ z, random=~Yeart  , family = "categorical", data = sim.data,#
# slice=TRUE, pr=TRUE,prior=prior)#
#
# d<-2#
# prior=list(R=list(V=1, fix=1), G=list(G1=list(V=diag(d), nu=d, alpha.mu=rep(0,d), alpha.V=diag(d)*1000)))#
#
# mod.Surv.MCMC.2  <- MCMCglmm(Surv ~ z, random=~idh(1+z):Yeart  , family = "categorical", data = sim.data,#
# slice=TRUE, pr=TRUE,prior=prior)#
#
# mod.Surv.MCMC.3   <- MCMCglmm(Surv ~ z, random=~us(1+z):Yeart  , family = "categorical", data = sim.data,#
# slice=TRUE, pr=TRUE,prior=prior)#
#
# post.modes <- posterior.mode(mod.Surv.MCMC.2$Sol)#
#
# intercepts.MCMC <- post.modes["(Intercept)"] + post.modes[3:22]#
#
# slopes.MCMC <- post.modes["z"] + post.modes[23:42]#
#
# par(mfrow=c(1,2),bty="l",pty="s",pch=19)#
#
# plot(intercepts.MCMC,coef(mod.Surv.glmer.2)$Yeart[,"(Intercept)"])#
# abline(0,1,col="red")#
# plot(slopes.MCMC,coef(mod.Surv.glmer.2)$Yeart[,"z"])#
# abline(0,1,col="red")#
#fit some flowering models#
#
flow.data <- subset(sim.data,Surv==1)#
#
mod.Flow <- glm(Flow ~ Yeart  , family = binomial, data = flow.data)#
mod.Flow.1 <- glm(Flow ~ Yeart + z  , family = binomial, data = flow.data)#
mod.Flow.2 <- glm(Flow ~ Yeart * z  , family = binomial, data = flow.data)#
#
anova(mod.Flow,mod.Flow.1,mod.Flow.2,test="Chisq")#
AIC(mod.Flow,mod.Flow.1,mod.Flow.2)#
#
#No interaction term, as expected, refit to get paramter estimates easily#
#
mod.Flow <- glm(Flow ~ Yeart + z -1 , family = binomial, data = flow.data)#
#
#Let's refit using glmer#
#
mod.Flow.glmer   <- glmer(Flow ~ 1 + (1|Yeart)  , family = binomial, data = flow.data)#
mod.Flow.glmer.1 <- glmer(Flow ~ z + (1|Yeart)  , family = binomial, data = flow.data)#
mod.Flow.glmer.2 <- glmer(Flow ~ z + (1|Yeart) + (0 + z|Yeart)  , family = binomial, data = flow.data)#
mod.Flow.glmer.3 <- glmer(Flow ~ z + (z|Yeart)  , family = binomial, data = flow.data)#
anova(mod.Flow.glmer,mod.Flow.glmer.1,mod.Flow.glmer.2,mod.Flow.glmer.3)#
#
#fit some growth models#
#
grow.data <- subset(sim.data,Surv==1 & Flow==0)#
#
mod.Grow <- lm(z1 ~ Yeart  , data = grow.data)#
mod.Grow.1 <- lm(z1 ~ Yeart +z , data = grow.data)#
mod.Grow.2 <- lm(z1 ~ Yeart *z , data = grow.data)#
#
anova(mod.Grow,mod.Grow.1,mod.Grow.2)#
AIC(mod.Grow,mod.Grow.1,mod.Grow.2)#
#
mod.Grow <- lm(z1 ~ Yeart/z-1  , data = grow.data)#
#
#Let's refit using lmer#
#
mod.Grow.lmer   <- lmer(z1 ~ 1 + (1|Yeart)  ,  data = grow.data, REML=FALSE)#
mod.Grow.lmer.1 <- lmer(z1 ~ z + (1|Yeart)  ,  data = grow.data, REML=FALSE)#
mod.Grow.lmer.2 <- lmer(z1 ~ z + (1|Yeart) + (0 + z|Yeart)  ,  data = grow.data, REML=FALSE)#
mod.Grow.lmer.3 <- lmer(z1 ~ z + (z|Yeart)  ,  data = grow.data, REML=FALSE)#
anova(mod.Grow.lmer,mod.Grow.lmer.1,mod.Grow.lmer.2,mod.Grow.lmer.3)#
#
#fit some recruit size models#
#
mod.Rcsz <- lm(z ~ 1  , data = recr.data)#
mod.Rcsz.1 <- lm(z ~ Yeart , data = recr.data)#
#
anova(mod.Rcsz,mod.Rcsz.1)#
AIC(mod.Rcsz,mod.Rcsz.1)#
#
mod.Rcsz <- lm(z ~ Yeart -1, data = recr.data)#
#
#Let's refit using lmer#
#
mod.Rcsz.lmer <- lmer(z ~ 1 + (1|Yeart) , data = recr.data)#
#
#quick check#
#
plot(as.numeric(coef(mod.Rcsz)),unlist(coef(mod.Rcsz.lmer)$Yeart))#
abline(0,1)#
#set up parameter vector with yearly estimates from fixed effects models (lm and glm)#
m.par.est <- matrix(NA,nrow=12,ncol=20)#
m.par.est[1,] <- coef(mod.Surv)[1:20]#
m.par.est[2,] <- coef(mod.Surv)[21:40]#
m.par.est[3,] <- coef(mod.Flow)[1:20]#
m.par.est[4,] <- coef(mod.Flow)[21]#
m.par.est[5,] <- coef(mod.Grow)[1:20]#
m.par.est[6,] <- coef(mod.Grow)[21:40]#
m.par.est[7,] <- summary(mod.Grow)$sigma#
m.par.est[8,] <- coef(mod.Rcsz)[1:20]#
m.par.est[9,] <- summary(mod.Rcsz)$sigma#
m.par.est[10,] <- 1#
m.par.est[11,] <- 2#
m.par.est[12,] <- 0.00095#
#
#set up parameter vector with yearly estimates from mixed effects models (lmer and glmer)#
m.par.est.mm <- matrix(NA,nrow=12,ncol=20)#
m.par.est.mm[1,] <- as.vector(unlist(coef(mod.Surv.glmer.2 )$Yeart["(Intercept)"]))#
m.par.est.mm[2,] <- as.vector(unlist(coef(mod.Surv.glmer.2 )$Yeart["z"]))#
m.par.est.mm[3,] <- as.vector(unlist(coef(mod.Flow.glmer.1 )$Yeart["(Intercept)"]))#
m.par.est.mm[4,] <- as.vector(unlist(coef(mod.Flow.glmer.1 )$Yeart["z"]))#
m.par.est.mm[5,] <- as.vector(unlist(coef(mod.Grow.lmer.2)$Yeart["(Intercept)"]))#
m.par.est.mm[6,] <- as.vector(unlist(coef(mod.Grow.lmer.2)$Yeart["z"]))#
m.par.est.mm[7,] <- as.vector(unlist(summary(mod.Grow.lmer.2)$sigma))#
m.par.est.mm[8,] <- as.vector(unlist(coef(mod.Rcsz.lmer)$Yeart["(Intercept)"]))#
m.par.est.mm[9,] <- as.vector(unlist(summary(mod.Rcsz.lmer)$sigma))#
m.par.est.mm[10,] <- 1#
m.par.est.mm[11,] <- 2#
m.par.est.mm[12,] <- 0.00095#
plot(m.par.est,m.par.est.mm)#
#
source("Carlina Demog Funs DI.R") #
#
rownames(m.par.est) <- names(m.par.true)#
rownames(m.par.est.mm) <- names(m.par.true)#
#
#test correlations between yearly parameters#
cor.test(m.par.est[1,],m.par.est[3,])#
cor.test(m.par.est[1,],m.par.est[5,])#
cor.test(m.par.est[1,],m.par.est[8,])#
#
cor.test(m.par.est[3,],m.par.est[5,])#
cor.test(m.par.est[3,],m.par.est[8,])#
#
cor.test(m.par.est["grow.int",],m.par.est["rcsz.int",])#
plot(m.par.est["grow.int",],m.par.est["rcsz.int",])#
######################################################################
#LTRE analysis#
######################################################################
#
nBigMatrix <- 100#
n.est <- 20000#
n.runin <- 500#
minsize <- 1.5#
maxsize <- 5#
n.years <-20#
#
LTRE.calcs<-function(params,delta){#
#
	K.year.i     <- array(NA,c(n.years,nBigMatrix,nBigMatrix))#
	lambda.i  <- rep(NA,n.years)#
	n.params <- dim(params)[1]#
	for(i in 1:n.years){#
		year.K         <- mk_K(nBigMatrix,params[,i],minsize,maxsize)#
		lambda.i[i] <- Re(eigen(year.K$K,only.values=TRUE)$values[1])#
		K.year.i[i,,] <- year.K$K#
		#cat(lambda.i[i],"\n")#
	}#
#
	h <- year.K$h; #
	meshpts <- year.K$meshpts#
#Calculate mean kernel, and mean of each params#
#
	mean.kernel   <- apply(K.year.i,2:3,mean)#
	mean.params <- apply(params,1,mean)#
#Calculate sensitivities to mean kernel for each parameter#
#
parm.to.pert         <- 1:n.params#
d.lambda.d.theta <- rep(NA,n.params)#
#
for(i in 1:n.params){#
	p.pert                 <- 1 + (i==parm.to.pert)*delta*mean.params[i]#
	K.up                    <- mk_K(nBigMatrix,p.pert*mean.params,minsize,maxsize)$K#
	lambda.up         <- Re(eigen(K.up,only.values=TRUE)$values[1])#
	p.pert                 <- 1 - (i==parm.to.pert)*delta*mean.params[i]#
	K.down               <-mk_K(nBigMatrix,p.pert*mean.params,minsize,maxsize)$K#
	lambda.down    <- Re(eigen(K.down,only.values=TRUE)$values[1])#
	d.lambda.d.theta[i] = 	(lambda.up - lambda.down) / (2*delta*mean.params[i])#
	}#
	names(d.lambda.d.theta) <- rownames(params)#
	which.vary<- which(apply(params,1,sd)>0);	#
	LTRE.data <- rbind(params[which.vary,],lambda.i)#
	LTRE.data.df<- data.frame(t(LTRE.data)); #
	d.lambda.d.theta <- d.lambda.d.theta[which.vary]#
	 return(list(meshpts=meshpts, h=h,mean.kernel=mean.kernel,LTRE.data=LTRE.data.df,#
	 d.lambda.d.theta=d.lambda.d.theta))#
#
}#
LTRE.nums <- LTRE.calcs(m.par.est,0.000001)#
#
sens.mat <- outer(LTRE.nums$d.lambda.d.theta,LTRE.nums$d.lambda.d.theta,FUN="*")#
#
var.cov <- cov(LTRE.nums$LTRE.data[,1:6])#
#
sens.mat*var.cov#
var(LTRE.nums$LTRE.data[,7])#
sum(sens.mat*var.cov)#
#
fit <- lm(LTRE.nums$LTRE.data[,7]~as.matrix(LTRE.nums$LTRE.data[,1:6]))#
#
slopes <- as.numeric(coef(fit)[2:7])#
#
plot(LTRE.nums$d.lambda.d.theta,slopes)#
#
sens.mat <- outer(slopes,slopes,FUN="*")#
sens.mat*var.cov#
var(LTRE.nums$LTRE.data[,7])#
sum(sens.mat*var.cov) + (summary(fit)$sigma)^2
load(file="Big param matrix.Rdata")#
#
rownames(store.params) <- names(m.par.true)#
#
n.years <-5000#
LTRE.nums <- LTRE.calcs(store.params,0.000001)#
#
sens.mat <- outer(LTRE.nums$d.lambda.d.theta,LTRE.nums$d.lambda.d.theta,FUN="*")#
#
var.cov <- cov(LTRE.nums$LTRE.data[,1:6])#
#
sens.mat*var.cov#
var(LTRE.nums$LTRE.data[,7])#
sum(sens.mat*var.cov)#
#
fit <- lm(LTRE.nums$LTRE.data[,7]~as.matrix(LTRE.nums$LTRE.data[,1:6]))#
#
slopes <- as.numeric(coef(fit)[2:7])#
#
plot(LTRE.nums$d.lambda.d.theta,slopes)#
#
sens.mat <- outer(slopes,slopes,FUN="*")#
sens.mat*var.cov#
var(LTRE.nums$LTRE.data[,7])#
#
sum(sens.mat*var.cov) + (summary(fit)$sigma)^2
fit <- lm(log(LTRE.nums$LTRE.data[,7])~as.matrix(LTRE.nums$LTRE.data[,1:6]))#
#
slopes <- as.numeric(coef(fit)[2:7])#
#
plot(LTRE.nums$d.lambda.d.theta,slopes)#
#
sens.mat <- outer(slopes,slopes,FUN="*")#
sens.mat*var.cov#
var(log(LTRE.nums$LTRE.data[,7]))#
#
sum(sens.mat*var.cov) + (summary(fit)$sigma)^2
fit <- lm(LTRE.nums$LTRE.data[,7]~as.matrix(LTRE.nums$LTRE.data[,1:6]))#
#
slopes <- as.numeric(coef(fit)[2:7])#
#
plot(LTRE.nums$d.lambda.d.theta,slopes)#
#
sens.mat <- outer(slopes,slopes,FUN="*")#
sens.mat*var.cov#
var(LTRE.nums$LTRE.data[,7])
terms <- predict(fit.lm,type="terms")
terms <- predict(fit,type="terms")
terms
fit <- lm(LTRE.nums$LTRE.data[,7]~LTRE.nums$LTRE.data[,1]+LTRE.nums$LTRE.data[,2]+LTRE.nums$LTRE.data[,3]+LTRE.nums$LTRE.data[,4]+LTRE.nums$LTRE.data[,5]+LTRE.nums$LTRE.data[,6]))#
terms <- predict(fit,type="terms")
fit <- lm(LTRE.nums$LTRE.data[,7]~LTRE.nums$LTRE.data[,1]+LTRE.nums$LTRE.data[,2]+LTRE.nums$LTRE.data[,3]+LTRE.nums$LTRE.data[,4]+LTRE.nums$LTRE.data[,5]+LTRE.nums$LTRE.data[,6])#
terms <- predict(fit,type="terms")
terms
cov(terms)
sens.mat*var.cov
all.equal(sens.mat*var.cov,cov(terms))
all.equal(sens.mat*var.cov,as.matrix(cov(terms)))
cov(terms)
matrix(cov(terms),nrow=6)
all.equal(sens.mat*var.cov,matrix(cov(terms),nrow=6))
all.equal(sens.mat*var.cov,matrix,(cov(terms),nrow=6))
all.equal(sens.mat*var.cov,matrix(cov(terms),nrow=6))
## ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~#
## Fit and run fixed and mixed effects effects Carlina stochastic IPM#
## ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~#
#
rm(list=ls(all=TRUE))#
#
library(doBy)#
library(lme4)#
library(MCMCglmm)#
set.seed(53241986)#
#
## Working directory must be set here, so the source()'s below run#
root=ifelse(.Platform$OS.type=="windows","c:/repos","~/Repos"); #
setwd(paste(root,"/ipm_book/Rcode/c2",sep="")); #
#
source("../utilities/Standard Graphical Pars.R");#
#
root=ifelse(.Platform$OS.type=="windows","c:/repos","~/Repos"); #
setwd(paste(root,"/ipm_book/Rcode/c7/Carlina",sep="")); #
#
#Read in the Carlina demographic data#
load("CarlinaIBMsim.Rdata")#
str(sim.data)#
#
store.sim.data <- sim.data#
#
#Select recruit data for the last 20 years (years 31 to 50)#
recr.data <- subset(sim.data,Recr==1)#
recr.data <- subset(recr.data,Yeart>30)#
recr.data <- transform(recr.data,Yeart = factor(Yeart - 30))#
#
#plant data for years 30 to 49, we do this to as the recruit size in #
#year t was generated by the year t-1 parameters#
sim.data <- subset(sim.data,Yeart>29 & Yeart<50)#
with(sim.data,table(Yeart,Recr))#
#Make Yeart a factor#
sim.data <- transform(sim.data,Yeart = factor(Yeart - 29))#
#fit some survival models#
#
mod.Surv <- glm(Surv ~ Yeart  , family = binomial, data = sim.data)#
mod.Surv.1 <- glm(Surv ~ Yeart + z  , family = binomial, data = sim.data)#
mod.Surv.2 <- glm(Surv ~ Yeart * z  , family = binomial, data = sim.data)#
#
anova(mod.Surv,mod.Surv.1,mod.Surv.2,test="Chisq")#
AIC(mod.Surv,mod.Surv.1,mod.Surv.2)#
#
#there is evidence of an interaction, as we might expect as both the intercept and slope vary from #
#year to year#
#
#Let's refit so we can easily get the parameter estimates#
#
mod.Surv <- glm(Surv ~ Yeart/ z -1 , family = binomial, data = sim.data)#
#
summary(mod.Surv)#
#
#Let's refit using glmer#
#
mod.Surv.glmer   <- glmer(Surv ~ 1 + (1|Yeart)  , family = binomial, data = sim.data)#
mod.Surv.glmer.1 <- glmer(Surv ~ z + (1|Yeart)  , family = binomial, data = sim.data)#
mod.Surv.glmer.2 <- glmer(Surv ~ z + (1|Yeart) + (0 + z|Yeart)  , family = binomial, data = sim.data)#
mod.Surv.glmer.3 <- glmer(Surv ~ z + (z|Yeart)  , family = binomial, data = sim.data)#
anova(mod.Surv.glmer,mod.Surv.glmer.1,mod.Surv.glmer.2,mod.Surv.glmer.3)#
# # d<-1#
# prior=list(R=list(V=1, fix=1), G=list(G1=list(V=diag(d), nu=d, alpha.mu=rep(0,d), alpha.V=diag(d)*1000)))#
#
# mod.Surv.MCMC   <- MCMCglmm(Surv ~ 1, random=~Yeart  , family = "categorical", data = sim.data,#
# slice=TRUE, pr=TRUE,prior=prior)#
#
# mod.Surv.MCMC.1   <- MCMCglmm(Surv ~ z, random=~Yeart  , family = "categorical", data = sim.data,#
# slice=TRUE, pr=TRUE,prior=prior)#
#
# d<-2#
# prior=list(R=list(V=1, fix=1), G=list(G1=list(V=diag(d), nu=d, alpha.mu=rep(0,d), alpha.V=diag(d)*1000)))#
#
# mod.Surv.MCMC.2  <- MCMCglmm(Surv ~ z, random=~idh(1+z):Yeart  , family = "categorical", data = sim.data,#
# slice=TRUE, pr=TRUE,prior=prior)#
#
# mod.Surv.MCMC.3   <- MCMCglmm(Surv ~ z, random=~us(1+z):Yeart  , family = "categorical", data = sim.data,#
# slice=TRUE, pr=TRUE,prior=prior)#
#
# post.modes <- posterior.mode(mod.Surv.MCMC.2$Sol)#
#
# intercepts.MCMC <- post.modes["(Intercept)"] + post.modes[3:22]#
#
# slopes.MCMC <- post.modes["z"] + post.modes[23:42]#
#
# par(mfrow=c(1,2),bty="l",pty="s",pch=19)#
#
# plot(intercepts.MCMC,coef(mod.Surv.glmer.2)$Yeart[,"(Intercept)"])#
# abline(0,1,col="red")#
# plot(slopes.MCMC,coef(mod.Surv.glmer.2)$Yeart[,"z"])#
# abline(0,1,col="red")#
#fit some flowering models#
#
flow.data <- subset(sim.data,Surv==1)#
#
mod.Flow <- glm(Flow ~ Yeart  , family = binomial, data = flow.data)#
mod.Flow.1 <- glm(Flow ~ Yeart + z  , family = binomial, data = flow.data)#
mod.Flow.2 <- glm(Flow ~ Yeart * z  , family = binomial, data = flow.data)#
#
anova(mod.Flow,mod.Flow.1,mod.Flow.2,test="Chisq")#
AIC(mod.Flow,mod.Flow.1,mod.Flow.2)#
#
#No interaction term, as expected, refit to get paramter estimates easily#
#
mod.Flow <- glm(Flow ~ Yeart + z -1 , family = binomial, data = flow.data)#
#
#Let's refit using glmer#
#
mod.Flow.glmer   <- glmer(Flow ~ 1 + (1|Yeart)  , family = binomial, data = flow.data)#
mod.Flow.glmer.1 <- glmer(Flow ~ z + (1|Yeart)  , family = binomial, data = flow.data)#
mod.Flow.glmer.2 <- glmer(Flow ~ z + (1|Yeart) + (0 + z|Yeart)  , family = binomial, data = flow.data)#
mod.Flow.glmer.3 <- glmer(Flow ~ z + (z|Yeart)  , family = binomial, data = flow.data)#
anova(mod.Flow.glmer,mod.Flow.glmer.1,mod.Flow.glmer.2,mod.Flow.glmer.3)#
#
#fit some growth models#
#
grow.data <- subset(sim.data,Surv==1 & Flow==0)#
#
mod.Grow <- lm(z1 ~ Yeart  , data = grow.data)#
mod.Grow.1 <- lm(z1 ~ Yeart +z , data = grow.data)#
mod.Grow.2 <- lm(z1 ~ Yeart *z , data = grow.data)#
#
anova(mod.Grow,mod.Grow.1,mod.Grow.2)#
AIC(mod.Grow,mod.Grow.1,mod.Grow.2)#
#
mod.Grow <- lm(z1 ~ Yeart/z-1  , data = grow.data)#
#
#Let's refit using lmer#
#
mod.Grow.lmer   <- lmer(z1 ~ 1 + (1|Yeart)  ,  data = grow.data, REML=FALSE)#
mod.Grow.lmer.1 <- lmer(z1 ~ z + (1|Yeart)  ,  data = grow.data, REML=FALSE)#
mod.Grow.lmer.2 <- lmer(z1 ~ z + (1|Yeart) + (0 + z|Yeart)  ,  data = grow.data, REML=FALSE)#
mod.Grow.lmer.3 <- lmer(z1 ~ z + (z|Yeart)  ,  data = grow.data, REML=FALSE)#
anova(mod.Grow.lmer,mod.Grow.lmer.1,mod.Grow.lmer.2,mod.Grow.lmer.3)#
#
#fit some recruit size models#
#
mod.Rcsz <- lm(z ~ 1  , data = recr.data)#
mod.Rcsz.1 <- lm(z ~ Yeart , data = recr.data)#
#
anova(mod.Rcsz,mod.Rcsz.1)#
AIC(mod.Rcsz,mod.Rcsz.1)#
#
mod.Rcsz <- lm(z ~ Yeart -1, data = recr.data)#
#
#Let's refit using lmer#
#
mod.Rcsz.lmer <- lmer(z ~ 1 + (1|Yeart) , data = recr.data)#
#
#quick check#
#
plot(as.numeric(coef(mod.Rcsz)),unlist(coef(mod.Rcsz.lmer)$Yeart))#
abline(0,1)#
#set up parameter vector with yearly estimates from fixed effects models (lm and glm)#
m.par.est <- matrix(NA,nrow=12,ncol=20)#
m.par.est[1,] <- coef(mod.Surv)[1:20]#
m.par.est[2,] <- coef(mod.Surv)[21:40]#
m.par.est[3,] <- coef(mod.Flow)[1:20]#
m.par.est[4,] <- coef(mod.Flow)[21]#
m.par.est[5,] <- coef(mod.Grow)[1:20]#
m.par.est[6,] <- coef(mod.Grow)[21:40]#
m.par.est[7,] <- summary(mod.Grow)$sigma#
m.par.est[8,] <- coef(mod.Rcsz)[1:20]#
m.par.est[9,] <- summary(mod.Rcsz)$sigma#
m.par.est[10,] <- 1#
m.par.est[11,] <- 2#
m.par.est[12,] <- 0.00095#
#
#set up parameter vector with yearly estimates from mixed effects models (lmer and glmer)#
m.par.est.mm <- matrix(NA,nrow=12,ncol=20)#
m.par.est.mm[1,] <- as.vector(unlist(coef(mod.Surv.glmer.2 )$Yeart["(Intercept)"]))#
m.par.est.mm[2,] <- as.vector(unlist(coef(mod.Surv.glmer.2 )$Yeart["z"]))#
m.par.est.mm[3,] <- as.vector(unlist(coef(mod.Flow.glmer.1 )$Yeart["(Intercept)"]))#
m.par.est.mm[4,] <- as.vector(unlist(coef(mod.Flow.glmer.1 )$Yeart["z"]))#
m.par.est.mm[5,] <- as.vector(unlist(coef(mod.Grow.lmer.2)$Yeart["(Intercept)"]))#
m.par.est.mm[6,] <- as.vector(unlist(coef(mod.Grow.lmer.2)$Yeart["z"]))#
m.par.est.mm[7,] <- as.vector(unlist(summary(mod.Grow.lmer.2)$sigma))#
m.par.est.mm[8,] <- as.vector(unlist(coef(mod.Rcsz.lmer)$Yeart["(Intercept)"]))#
m.par.est.mm[9,] <- as.vector(unlist(summary(mod.Rcsz.lmer)$sigma))#
m.par.est.mm[10,] <- 1#
m.par.est.mm[11,] <- 2#
m.par.est.mm[12,] <- 0.00095#
plot(m.par.est,m.par.est.mm)#
#
source("Carlina Demog Funs DI.R") #
#
rownames(m.par.est) <- names(m.par.true)#
rownames(m.par.est.mm) <- names(m.par.true)#
#
#test correlations between yearly parameters#
cor.test(m.par.est[1,],m.par.est[3,])#
cor.test(m.par.est[1,],m.par.est[5,])#
cor.test(m.par.est[1,],m.par.est[8,])#
#
cor.test(m.par.est[3,],m.par.est[5,])#
cor.test(m.par.est[3,],m.par.est[8,])#
#
cor.test(m.par.est["grow.int",],m.par.est["rcsz.int",])#
plot(m.par.est["grow.int",],m.par.est["rcsz.int",])#
######################################################################
#LTRE analysis#
######################################################################
#
nBigMatrix <- 100#
n.est <- 20000#
n.runin <- 500#
minsize <- 1.5#
maxsize <- 5#
n.years <-20#
#
LTRE.calcs<-function(params,delta){#
#
	K.year.i     <- array(NA,c(n.years,nBigMatrix,nBigMatrix))#
	lambda.i  <- rep(NA,n.years)#
	n.params <- dim(params)[1]#
	for(i in 1:n.years){#
		year.K         <- mk_K(nBigMatrix,params[,i],minsize,maxsize)#
		lambda.i[i] <- Re(eigen(year.K$K,only.values=TRUE)$values[1])#
		K.year.i[i,,] <- year.K$K#
		#cat(lambda.i[i],"\n")#
	}#
#
	h <- year.K$h; #
	meshpts <- year.K$meshpts#
#Calculate mean kernel, and mean of each params#
#
	mean.kernel   <- apply(K.year.i,2:3,mean)#
	mean.params <- apply(params,1,mean)#
#Calculate sensitivities to mean kernel for each parameter#
#
parm.to.pert         <- 1:n.params#
d.lambda.d.theta <- rep(NA,n.params)#
#
for(i in 1:n.params){#
	p.pert                 <- 1 + (i==parm.to.pert)*delta*mean.params[i]#
	K.up                    <- mk_K(nBigMatrix,p.pert*mean.params,minsize,maxsize)$K#
	lambda.up         <- Re(eigen(K.up,only.values=TRUE)$values[1])#
	p.pert                 <- 1 - (i==parm.to.pert)*delta*mean.params[i]#
	K.down               <-mk_K(nBigMatrix,p.pert*mean.params,minsize,maxsize)$K#
	lambda.down    <- Re(eigen(K.down,only.values=TRUE)$values[1])#
	d.lambda.d.theta[i] = 	(lambda.up - lambda.down) / (2*delta*mean.params[i])#
	}#
	names(d.lambda.d.theta) <- rownames(params)#
	which.vary<- which(apply(params,1,sd)>0);	#
	LTRE.data <- rbind(params[which.vary,],lambda.i)#
	LTRE.data.df<- data.frame(t(LTRE.data)); #
	d.lambda.d.theta <- d.lambda.d.theta[which.vary]#
	 return(list(meshpts=meshpts, h=h,mean.kernel=mean.kernel,LTRE.data=LTRE.data.df,#
	 d.lambda.d.theta=d.lambda.d.theta))#
#
}#
LTRE.nums <- LTRE.calcs(m.par.est,0.000001)#
#
sens.mat <- outer(LTRE.nums$d.lambda.d.theta,LTRE.nums$d.lambda.d.theta,FUN="*")#
#
var.cov <- cov(LTRE.nums$LTRE.data[,1:6])#
#
sens.mat*var.cov#
var(LTRE.nums$LTRE.data[,7])#
sum(sens.mat*var.cov)#
#
fit <- lm(LTRE.nums$LTRE.data[,7]~as.matrix(LTRE.nums$LTRE.data[,1:6]))#
#
slopes <- as.numeric(coef(fit)[2:7])#
#
plot(LTRE.nums$d.lambda.d.theta,slopes)#
#
sens.mat <- outer(slopes,slopes,FUN="*")#
sens.mat*var.cov#
var(LTRE.nums$LTRE.data[,7])#
sum(sens.mat*var.cov) + (summary(fit)$sigma)^2
## ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~#
## Run LTRE analysis using large parameter set from fitted mixed models#
## ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~#
#
rm(list=ls(all=TRUE))#
require(mgcv)#
require(randomForest)#
#
set.seed(53241986)#
#
## Working directory must be set here, so the source()'s below run#
root=ifelse(.Platform$OS.type=="windows","c:/repos","~/Repos"); #
setwd(paste(root,"/ipm_book/Rcode/c2",sep="")); #
#
source("../utilities/Standard Graphical Pars.R");#
#
root=ifelse(.Platform$OS.type=="windows","c:/repos","~/Repos"); #
setwd(paste(root,"/ipm_book/Rcode/c7/Carlina",sep="")); #
source("Carlina Demog Funs DI.R")
source("Carlina Demog Funs DI.R") #
#
######################################################################
#LTRE analysis#
######################################################################
#
nBigMatrix <- 100#
n.est <- 20000#
n.runin <- 500#
minsize <- 1.5#
maxsize <- 5#
n.years <-20#
#
#function adds yearly lambda to param matrix, also removed parameters that#
#don't vary with time, and calculates d lambda / d parameters numerically#
#returns data.frame with parameters and lambda as columns for analysis#
#plus other stuff#
LTRE.calcs<-function(params,delta=.Machine$double.eps^(1/3)){#
#
	K.year.i     <- array(NA,c(n.years,nBigMatrix,nBigMatrix))#
	lambda.i  <- rep(NA,n.years)#
	n.params <- dim(params)[1]#
	for(i in 1:n.years){#
		year.K         <- mk_K(nBigMatrix,params[,i],minsize,maxsize)#
		lambda.i[i] <- Re(eigen(year.K$K,only.values=TRUE)$values[1])#
		K.year.i[i,,] <- year.K$K#
		#cat(lambda.i[i],"\n")#
	}#
#
	h <- year.K$h; #
	meshpts <- year.K$meshpts#
#Calculate mean kernel, and mean of each params#
#
	mean.kernel   <- apply(K.year.i,2:3,mean)#
	mean.params <- apply(params,1,mean)#
#Calculate sensitivities to mean kernel for each parameter#
#
parm.to.pert         <- 1:n.params#
d.lambda.d.theta <- rep(NA,n.params)#
#
for(i in 1:n.params){#
	p.pert                 <-  (i==parm.to.pert)*delta#
	K.up                    <- mk_K(nBigMatrix,mean.params+p.pert,minsize,maxsize)$K#
	lambda.up         <- Re(eigen(K.up,only.values=TRUE)$values[1])#
	K.down               <-mk_K(nBigMatrix,mean.params-p.pert,minsize,maxsize)$K#
	lambda.down    <- Re(eigen(K.down,only.values=TRUE)$values[1])#
	d.lambda.d.theta[i] = 	(lambda.up - lambda.down) / (2*delta)#
	}#
	names(d.lambda.d.theta) <- rownames(params)#
	which.vary<- which(apply(params,1,sd)>0);	#
	LTRE.data <- rbind(params[which.vary,],lambda.i)#
	LTRE.data.df<- data.frame(t(LTRE.data)); #
	d.lambda.d.theta <- d.lambda.d.theta[which.vary]#
	 return(list(meshpts=meshpts, h=h,mean.kernel=mean.kernel,LTRE.data=LTRE.data.df,#
	 d.lambda.d.theta=d.lambda.d.theta))#
#
}#
#
load(file="Big param matrix.Rdata")#
#
n.years <-5000#
LTRE.nums <- LTRE.calcs(store.params)#
#
ps.and.l <- LTRE.nums$LTRE.data#
d.lambda.d.theta <- LTRE.nums$d.lambda.d.theta#
#
sens.mat <- outer(d.lambda.d.theta,d.lambda.d.theta,FUN="*")#
#
var.cov <- cov(ps.and.l[,1:6])#
#
var.terms.1.order <-sens.mat*var.cov#
var(ps.and.l[,"lambda.i"])#
sum(var.terms.1.order)
apply(var.terms.1.order,1,sum)/sum(var.terms.1.order)
var.terms.1.order
fit <- lm(lambda.i~surv.int+surv.z+flow.int+grow.int+grow.z+rcsz.int, data=ps.and.l)#
#
slopes <- as.numeric(coef(fit)[2:7])#
#
plot(d.lambda.d.theta,slopes)#
abline(0,1)#
#
sens.mat <- outer(slopes,slopes,FUN="*")#
var.terms.lm <- sens.mat*var.cov#
sum(var.terms.lm) + (summary(fit)$sigma)^2#
#
var(ps.and.l[,"lambda.i"])
apply(var.terms.1.order,1,sum)/sum(abs(var.terms.1.order))
terms <- predict(fit,type="terms")#
#
cov(terms)#
#
sum(cov(terms)) + (summary(fit)$sigma)^2
var(ps.and.l[,"lambda.i"])#
sum(var.terms.1.order)
var(ps.and.l[,"lambda.i"])#
round(sum(var.terms.1.order),2)
1.24-0.98
0.26/0.98
?predict
?predict.glm
ps.and.l <- LTRE.nums$LTRE.data#
d.lambda.d.theta <- LTRE.nums$d.lambda.d.theta#
#
sens.mat <- outer(d.lambda.d.theta,d.lambda.d.theta,FUN="*")#
#
var.cov <- cov(ps.and.l[,1:6])#
#
var.terms.1.order <-sens.mat*var.cov#
var(ps.and.l[,"lambda.i"])#
sum(var.terms.1.order)#
#
fit <- lm(lambda.i~surv.int+surv.z+flow.int+grow.int+grow.z+rcsz.int, data=ps.and.l)#
#
slopes <- as.numeric(coef(fit)[2:7])#
#
plot(d.lambda.d.theta,slopes)#
abline(0,1)#
#
sens.mat <- outer(slopes,slopes,FUN="*")#
var.terms.lm <- sens.mat*var.cov#
sum(var.terms.lm) + (summary(fit)$sigma)^2#
#
var(ps.and.l[,"lambda.i"])#
terms <- predict(fit,type="terms")#
#
cov(terms)#
#
sum(cov(terms)) + (summary(fit)$sigma)^2#
#
fit <- gam(ps.and.l[,"lambda.i"] ~ s(ps.and.l[,"surv.int"]) + #
                                                            s(ps.and.l[,"surv.z"])    +#
                                                            s(ps.and.l[,"flow.int"]) +#
                                                            s(ps.and.l[,"grow.int"])+#
                                                            s(ps.and.l[,"grow.z"])   +#
                                                            s(ps.and.l[,"rcsz.int"]) , data=ps.and.l)#
#
terms <- predict(fit,type="terms")#
#
var.terms.gam <- cov(terms)#
#
rownames(var.terms.gam) <- rownames(var.terms.1.order)#
colnames(var.terms.gam)  <- colnames(var.terms.1.order)#
#
var.terms.gam#
sum(var.terms.gam)+summary(fit)$scale#
#
var(ps.and.l[,"lambda.i"])
## ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~#
## Run LTRE analysis using large parameter set from fitted mixed models#
## ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~#
#
rm(list=ls(all=TRUE))#
require(mgcv)#
require(randomForest)#
#
set.seed(53241986)#
#
## Working directory must be set here, so the source()'s below run#
root=ifelse(.Platform$OS.type=="windows","c:/repos","~/Repos"); #
setwd(paste(root,"/ipm_book/Rcode/c2",sep="")); #
#
source("../utilities/Standard Graphical Pars.R");#
#
root=ifelse(.Platform$OS.type=="windows","c:/repos","~/Repos"); #
setwd(paste(root,"/ipm_book/Rcode/c7/Carlina",sep="")); #
source("Carlina Demog Funs DI.R") #
#
######################################################################
#LTRE analysis#
######################################################################
#
nBigMatrix <- 100#
n.est <- 20000#
n.runin <- 500#
minsize <- 1.5#
maxsize <- 5#
n.years <-20#
#
#function adds yearly lambda to param matrix, also removed parameters that#
#don't vary with time, and calculates d lambda / d parameters numerically#
#returns data.frame with parameters and lambda as columns for analysis#
#plus other stuff#
LTRE.calcs<-function(params,delta=.Machine$double.eps^(1/3)){#
#
	K.year.i     <- array(NA,c(n.years,nBigMatrix,nBigMatrix))#
	lambda.i  <- rep(NA,n.years)#
	n.params <- dim(params)[1]#
#Calculate lambda for each yearly kernel#
	for(i in 1:n.years){#
		year.K         <- mk_K(nBigMatrix,params[,i],minsize,maxsize)#
		lambda.i[i] <- Re(eigen(year.K$K,only.values=TRUE)$values[1])#
		K.year.i[i,,] <- year.K$K#
		#cat(lambda.i[i],"\n")#
	}#
#
	h <- year.K$h; #
	meshpts <- year.K$meshpts#
#Calculate mean kernel, and mean of each params#
#
	mean.kernel   <- apply(K.year.i,2:3,mean)#
	mean.params   <- apply(params,1,mean)#
#Calculate sensitivities to mean kernel for each parameter#
#
parm.to.pert         <- 1:n.params#
d.lambda.d.theta <- rep(NA,n.params)#
#
for(i in 1:n.params){#
	p.pert                 <-  (i==parm.to.pert)*delta#
	K.up                   <- mk_K(nBigMatrix,mean.params+p.pert,minsize,maxsize)$K#
	lambda.up              <- Re(eigen(K.up,only.values=TRUE)$values[1])#
	K.down                 <-  mk_K(nBigMatrix,mean.params-p.pert,minsize,maxsize)$K#
	lambda.down            <-  Re(eigen(K.down,only.values=TRUE)$values[1])#
	d.lambda.d.theta[i] = 	(lambda.up - lambda.down) / (2*delta)#
	}#
	names(d.lambda.d.theta) <- rownames(params)#
#remove parameters that don't vary#
#
	which.vary<- which(apply(params,1,sd)>0);	#
	LTRE.data <- rbind(params[which.vary,],lambda.i)#
	LTRE.data.df<- data.frame(t(LTRE.data)); #
	d.lambda.d.theta <- d.lambda.d.theta[which.vary]#
return(list(meshpts=meshpts, h=h,mean.kernel=mean.kernel,LTRE.data=LTRE.data.df,#
	 d.lambda.d.theta=d.lambda.d.theta))#
#
}#
#
load(file="Big param matrix.Rdata")#
#
n.years <-5000#
LTRE.nums <- LTRE.calcs(store.params)#
#
ps.and.l <- LTRE.nums$LTRE.data#
d.lambda.d.theta <- LTRE.nums$d.lambda.d.theta#
#
sens.mat <- outer(d.lambda.d.theta,d.lambda.d.theta,FUN="*")#
#
var.cov <- cov(ps.and.l[,1:6])#
#
var.terms.1.order <-sens.mat*var.cov#
var(ps.and.l[,"lambda.i"])#
sum(var.terms.1.order)#
#
fit <- lm(lambda.i~surv.int+surv.z+flow.int+grow.int+grow.z+rcsz.int, data=ps.and.l)#
#
slopes <- as.numeric(coef(fit)[2:7])#
#
plot(d.lambda.d.theta,slopes)#
abline(0,1)#
#
sens.mat <- outer(slopes,slopes,FUN="*")#
var.terms.lm <- sens.mat*var.cov#
sum(var.terms.lm) + (summary(fit)$sigma)^2#
#
var(ps.and.l[,"lambda.i"])#
terms <- predict(fit,type="terms")#
#
cov(terms)#
#
sum(cov(terms)) + (summary(fit)$sigma)^2#
#
fit <- gam(ps.and.l[,"lambda.i"] ~ s(ps.and.l[,"surv.int"]) + #
                                                            s(ps.and.l[,"surv.z"])    +#
                                                            s(ps.and.l[,"flow.int"]) +#
                                                            s(ps.and.l[,"grow.int"])+#
                                                            s(ps.and.l[,"grow.z"])   +#
                                                            s(ps.and.l[,"rcsz.int"]) , data=ps.and.l)#
#
terms <- predict(fit,type="terms")#
#
var.terms.gam <- cov(terms)#
#
rownames(var.terms.gam) <- rownames(var.terms.1.order)#
colnames(var.terms.gam)  <- colnames(var.terms.1.order)#
#
var.terms.gam#
sum(var.terms.gam)+summary(fit)$scale#
#
var(ps.and.l[,"lambda.i"])
require(randomForest)
################################################
#Random Forests#
################################################
#
# 'Tune' the random forest parameter mtry#
out<- tuneRF(x=subset(ps.and.l,select=-lambda.i), y=ps.and.l$lambda.i,ntreeTry=500,stepfactor=1,improve=0.02); #
plot(out); ## tells us to use mtry=4 #
#
# Fit the model, computing importance measures#
x=subset(ps.and.l,select=-lambda.i)#
lambdaRF <- randomForest(x=x,y=ps.and.l$lambda.i,ntree=500,importance=TRUE,mtry=4)#
#
graphics.off(); #
dev.new(width=8,height=5); set_graph_pars("panel2");#
# verify that the number of trees is enough#
# the plot of R^2 vs. number of trees used should saturate #
plot(lambdaRF$rsq,type="l",xlab="Number of trees",ylab="Model r^2"); add_panel_label("a")#
#
# Plot the importance results #
varImpPlot(lambdaRF,type=1,scale=FALSE,main="")#
add_panel_label("b")
ps.and.l <- LTRE.nums$LTRE.data#
d.lambda.d.theta <- LTRE.nums$d.lambda.d.theta#
#
sens.mat <- outer(d.lambda.d.theta,d.lambda.d.theta,FUN="*")#
#
var.cov <- cov(ps.and.l[,1:6])#
#
var.terms.1.order <-sens.mat*var.cov#
var(ps.and.l[,"lambda.i"])#
sum(var.terms.1.order)#
#
fit <- lm(lambda.i~surv.int+surv.z+flow.int+grow.int+grow.z+rcsz.int, data=ps.and.l)#
#
slopes <- as.numeric(coef(fit)[2:7])#
#
plot(d.lambda.d.theta,slopes)#
abline(0,1)#
#
sens.mat <- outer(slopes,slopes,FUN="*")#
var.terms.lm <- sens.mat*var.cov#
sum(var.terms.lm) + (summary(fit)$sigma)^2#
#
apply(var.terms.lm,1,sum)/sum(var.terms.lm)
ps.and.l <- LTRE.nums$LTRE.data#
d.lambda.d.theta <- LTRE.nums$d.lambda.d.theta#
#
sens.mat <- outer(d.lambda.d.theta,d.lambda.d.theta,FUN="*")#
#
var.cov <- cov(ps.and.l[,1:6])#
#
var.terms.1.order <-sens.mat*var.cov#
var(ps.and.l[,"lambda.i"])#
sum(var.terms.1.order)#
#
apply(var.terms.1.order,1,sum)/sum(ar.terms.1.order)
apply(var.terms.1.order,1,sum)/sum(var.terms.1.order)
graphics.off(); #
dev.new(width=8,height=5); set_graph_pars("panel2");#
# verify that the number of trees is enough#
# the plot of R^2 vs. number of trees used should saturate #
plot(lambdaRF$rsq,type="l",xlab="Number of trees",ylab="Model r^2"); add_panel_label("a")#
#
# Plot the importance results #
varImpPlot(lambdaRF,type=1,scale=FALSE,main="")#
add_panel_label("b")
sort(apply(var.terms.1.order,1,sum)/sum(var.terms.1.order))
order.terms.1.order <- sort(apply(var.terms.1.order,1,sum)/sum(var.terms.1.order))#
#
order.terms.1.order
fit <- lm(lambda.i~surv.int+surv.z+flow.int+grow.int+grow.z+rcsz.int, data=ps.and.l)#
#
slopes <- as.numeric(coef(fit)[2:7])#
#
plot(d.lambda.d.theta,slopes)#
abline(0,1)#
#
sens.mat <- outer(slopes,slopes,FUN="*")#
var.terms.lm <- sens.mat*var.cov#
sum(var.terms.lm) + (summary(fit)$sigma)^2#
#
order.terms.lm <- sort(apply(var.terms.lm,1,sum)/sum(var.terms.lm))#
order.terms.lm
var(ps.and.l[,"lambda.i"])#
#
terms <- predict(fit,type="terms")#
#
cov(terms)#
#
sum(cov(terms)) + (summary(fit)$sigma)^2#
#
fit <- gam(ps.and.l[,"lambda.i"] ~ s(ps.and.l[,"surv.int"]) + #
                                                            s(ps.and.l[,"surv.z"])    +#
                                                            s(ps.and.l[,"flow.int"]) +#
                                                            s(ps.and.l[,"grow.int"])+#
                                                            s(ps.and.l[,"grow.z"])   +#
                                                            s(ps.and.l[,"rcsz.int"]) , data=ps.and.l)#
#
terms <- predict(fit,type="terms")#
#
var.terms.gam <- cov(terms)#
#
rownames(var.terms.gam) <- rownames(var.terms.1.order)#
colnames(var.terms.gam)  <- colnames(var.terms.1.order)#
#
var.terms.gam#
sum(var.terms.gam)+summary(fit)$scale#
#
var(ps.and.l[,"lambda.i"])#
#
order.terms.gam <- sort(apply(var.terms.gam,1,sum)/sum(var.terms.gam))#
order.terms.gam
graphics.off(); #
dev.new(width=8,height=5); set_graph_pars("panel2");#
# verify that the number of trees is enough#
# the plot of R^2 vs. number of trees used should saturate #
plot(lambdaRF$rsq,type="l",xlab="Number of trees",ylab="Model r^2"); add_panel_label("a")#
#
# Plot the importance results #
varImpPlot(lambdaRF,type=1,scale=FALSE,main="")#
add_panel_label("b")
order.terms.1.order
order.terms.gam
## ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~#
## Run LTRE analysis using large parameter set from fitted mixed models#
## ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~#
#
rm(list=ls(all=TRUE))#
require(mgcv)#
require(randomForest)#
#
set.seed(53241986)#
#
## Working directory must be set here, so the source()'s below run#
root=ifelse(.Platform$OS.type=="windows","c:/repos","~/Repos"); #
setwd(paste(root,"/ipm_book/Rcode/c2",sep="")); #
#
source("../utilities/Standard Graphical Pars.R");#
#
root=ifelse(.Platform$OS.type=="windows","c:/repos","~/Repos"); #
setwd(paste(root,"/ipm_book/Rcode/c7/Carlina",sep="")); #
source("Carlina Demog Funs DI.R") #
#
######################################################################
#LTRE analysis#
######################################################################
#
nBigMatrix <- 100#
n.est <- 20000#
n.runin <- 500#
minsize <- 1.5#
maxsize <- 5#
n.years <-20#
#
#function adds yearly lambda to param matrix, also removes parameters that#
#don't vary with time, and calculates d lambda / d parameters numerically#
#returns data.frame with parameters and lambda as columns for analysis#
#plus other stuff#
LTRE.calcs<-function(params,delta=.Machine$double.eps^(1/3)){#
#
	K.year.i     <- array(NA,c(n.years,nBigMatrix,nBigMatrix))#
	lambda.i  <- rep(NA,n.years)#
	n.params <- dim(params)[1]#
#Calculate lambda for each yearly kernel#
	for(i in 1:n.years){#
		year.K         <- mk_K(nBigMatrix,params[,i],minsize,maxsize)#
		lambda.i[i] <- Re(eigen(year.K$K,only.values=TRUE)$values[1])#
		K.year.i[i,,] <- year.K$K#
		#cat(lambda.i[i],"\n")#
	}#
#
	h <- year.K$h; #
	meshpts <- year.K$meshpts#
#Calculate mean kernel, and mean of each params#
#
	mean.kernel   <- apply(K.year.i,2:3,mean)#
	mean.params   <- apply(params,1,mean)#
#Calculate sensitivities to mean kernel for each parameter#
#
parm.to.pert         <- 1:n.params#
d.lambda.d.theta <- rep(NA,n.params)#
#
for(i in 1:n.params){#
	p.pert                 <-  (i==parm.to.pert)*delta#
	K.up                   <- mk_K(nBigMatrix,mean.params+p.pert,minsize,maxsize)$K#
	lambda.up              <- Re(eigen(K.up,only.values=TRUE)$values[1])#
	K.down                 <-  mk_K(nBigMatrix,mean.params-p.pert,minsize,maxsize)$K#
	lambda.down            <-  Re(eigen(K.down,only.values=TRUE)$values[1])#
	d.lambda.d.theta[i] = 	(lambda.up - lambda.down) / (2*delta)#
	}#
	names(d.lambda.d.theta) <- rownames(params)#
#remove parameters that don't vary#
#
	which.vary<- which(apply(params,1,sd)>0);	#
	LTRE.data <- rbind(params[which.vary,],lambda.i)#
	LTRE.data.df<- data.frame(t(LTRE.data)); #
	d.lambda.d.theta <- d.lambda.d.theta[which.vary]#
return(list(meshpts=meshpts, h=h,mean.kernel=mean.kernel,LTRE.data=LTRE.data.df,#
	 d.lambda.d.theta=d.lambda.d.theta))#
#
}#
#
load(file="Big param matrix.Rdata")
n.years <-5000#
LTRE.nums <- LTRE.calcs(store.params)#
#
ps.and.l <- LTRE.nums$LTRE.data#
d.lambda.d.theta <- LTRE.nums$d.lambda.d.theta#
#
sens.mat <- outer(d.lambda.d.theta,d.lambda.d.theta,FUN="*")#
#
var.cov <- cov(ps.and.l[,1:6])#
#
var.terms.1.order <-sens.mat*var.cov#
var(ps.and.l[,"lambda.i"])#
sum(var.terms.1.order)
order.terms.1.order <- sort(apply(var.terms.1.order,1,sum)/sum(var.terms.1.order))#
#
order.terms.1.order #
#
fit <- lm(lambda.i~surv.int+surv.z+flow.int+grow.int+grow.z+rcsz.int, data=ps.and.l)#
#
slopes <- as.numeric(coef(fit)[2:7])#
#
plot(d.lambda.d.theta,slopes)#
abline(0,1)#
#
sens.mat <- outer(slopes,slopes,FUN="*")#
var.terms.lm <- sens.mat*var.cov#
sum(var.terms.lm) + (summary(fit)$sigma)^2#
#
order.terms.lm <- sort(apply(var.terms.lm,1,sum)/sum(var.terms.lm))#
order.terms.lm#
#
var(ps.and.l[,"lambda.i"])#
#
terms <- predict(fit,type="terms")#
#
cov(terms)#
#
sum(cov(terms)) + (summary(fit)$sigma)^2#
#
fit <- gam(ps.and.l[,"lambda.i"] ~ s(ps.and.l[,"surv.int"]) + #
                                                            s(ps.and.l[,"surv.z"])    +#
                                                            s(ps.and.l[,"flow.int"]) +#
                                                            s(ps.and.l[,"grow.int"])+#
                                                            s(ps.and.l[,"grow.z"])   +#
                                                            s(ps.and.l[,"rcsz.int"]) , data=ps.and.l)#
#
terms <- predict(fit,type="terms")#
#
var.terms.gam <- cov(terms)#
#
rownames(var.terms.gam) <- rownames(var.terms.1.order)#
colnames(var.terms.gam)  <- colnames(var.terms.1.order)#
#
var.terms.gam#
sum(var.terms.gam)+summary(fit)$scale#
#
var(ps.and.l[,"lambda.i"])#
#
order.terms.gam <- sort(apply(var.terms.gam,1,sum)/sum(var.terms.gam))#
order.terms.gam#
#
################################################
#Random Forests#
################################################
#
# 'Tune' the random forest parameter mtry#
out<- tuneRF(x=subset(ps.and.l,select=-lambda.i), y=ps.and.l$lambda.i,ntreeTry=500,stepfactor=1,improve=0.02); #
plot(out); ## tells us to use mtry=4 #
#
# Fit the model, computing importance measures#
x=subset(ps.and.l,select=-lambda.i)#
lambdaRF <- randomForest(x=x,y=ps.and.l$lambda.i,ntree=500,importance=TRUE,mtry=4)#
#
graphics.off(); #
dev.new(width=8,height=5); set_graph_pars("panel2");#
# verify that the number of trees is enough#
# the plot of R^2 vs. number of trees used should saturate #
plot(lambdaRF$rsq,type="l",xlab="Number of trees",ylab="Model r^2"); add_panel_label("a")#
#
# Plot the importance results #
varImpPlot(lambdaRF,type=1,scale=FALSE,main="")#
add_panel_label("b")#
#
order.terms.1.order #
order.terms.lm#
order.terms.gam
(var(ps.and.l[,"lambda.i"])-sum(var.terms.1.order))/var(ps.and.l[,"lambda.i"])*100
(var(ps.and.l[,"lambda.i"])-sum(var.terms.lm))/var(ps.and.l[,"lambda.i"])*100
(var(ps.and.l[,"lambda.i"])-sum(var.terms.lm)- (summary(fit)$sigma)^2)/var(ps.and.l[,"lambda.i"])*100
var.terms.gam#
sum(var.terms.gam)+summary(fit)$scale#
#
var(ps.and.l[,"lambda.i"])
order.terms.1.order #
order.terms.lm#
order.terms.gam
#extract parameters/lambda and sensitivities to formulae look neater#
#
ps.and.l <- LTRE.nums$LTRE.data#
d.lambda.d.theta <- LTRE.nums$d.lambda.d.theta#
#
#calculate sensitivity matrix#
sens.mat <- outer(d.lambda.d.theta,d.lambda.d.theta,FUN="*")#
#
#calculate sensitivity matrix - cols 1 to 6 contain parameters#
var.cov <- cov(ps.and.l[,1:6])#
#
#Calculate 1st order approx#
var.terms.1.order <-sens.mat*var.cov#
#
#sum terms#
var(ps.and.l[,"lambda.i"])#
sum(var.terms.1.order)#
#
#error#
(var(ps.and.l[,"lambda.i"])-sum(var.terms.1.order))/var(ps.and.l[,"lambda.i"])*100#
#
#Calculate Cont terms and sort#
order.terms.1.order <- sort(apply(var.terms.1.order,1,sum)/sum(var.terms.1.order))#
order.terms.1.order #
#
#fit lm model for variation in lambda#
#
fit <- lm(lambda.i~surv.int+surv.z+flow.int+grow.int+grow.z+rcsz.int, data=ps.and.l)#
#
#extract slopes#
slopes <- as.numeric(coef(fit)[2:7])#
#
#check slopes ~ sensitivites#
plot(d.lambda.d.theta,slopes)#
abline(0,1)#
#
#cacluate sensitivity matrix using slopes#
sens.mat <- outer(slopes,slopes,FUN="*")#
#
#calculate weighted var-covar matrix#
var.terms.lm <- sens.mat*var.cov#
#
#check total variance equal#
sum(var.terms.lm) + (summary(fit)$sigma)^2#
var(ps.and.l[,"lambda.i"])
#do calculation with predict#
terms <- predict(fit,type="terms")#
#
cov(terms)#
#
sum(cov(terms)) + (summary(fit)$sigma)^2
#fit gam model#
fit <- gam(ps.and.l[,"lambda.i"] ~ s(ps.and.l[,"surv.int"]) + #
                                                            s(ps.and.l[,"surv.z"])    +#
                                                            s(ps.and.l[,"flow.int"]) +#
                                                            s(ps.and.l[,"grow.int"])+#
                                                            s(ps.and.l[,"grow.z"])   +#
                                                            s(ps.and.l[,"rcsz.int"]) , data=ps.and.l)#
#
#extract terms using predict#
terms <- predict(fit,type="terms")#
#
#weighted var-cov matrix#
var.terms.gam <- cov(terms)#
#
rownames(var.terms.gam) <- rownames(var.terms.1.order)#
colnames(var.terms.gam)  <- colnames(var.terms.1.order)#
#
var.terms.gam#
sum(var.terms.gam)+summary(fit)$scale#
#
var(ps.and.l[,"lambda.i"])#
#
#calculate Cont terms and sort#
order.terms.gam <- sort(apply(var.terms.gam,1,sum)/sum(var.terms.gam))#
order.terms.gam#
#
################################################
#Random Forests#
################################################
#
# 'Tune' the random forest parameter mtry#
out<- tuneRF(x=subset(ps.and.l,select=-lambda.i), y=ps.and.l$lambda.i,ntreeTry=500,stepfactor=1,improve=0.02); #
plot(out); ## tells us to use mtry=4 #
#
# Fit the model, computing importance measures#
x=subset(ps.and.l,select=-lambda.i)#
lambdaRF <- randomForest(x=x,y=ps.and.l$lambda.i,ntree=500,importance=TRUE,mtry=4)#
#
graphics.off(); #
dev.new(width=8,height=5); set_graph_pars("panel2");#
# verify that the number of trees is enough#
# the plot of R^2 vs. number of trees used should saturate #
plot(lambdaRF$rsq,type="l",xlab="Number of trees",ylab="Model r^2"); add_panel_label("a")#
#
# Plot the importance results #
varImpPlot(lambdaRF,type=1,scale=FALSE,main="")#
add_panel_label("b")#
#
#sorted Cont terms#
order.terms.1.order #
order.terms.lm#
order.terms.gam
## ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~#
## Fit and run fixed and mixed effects effects Carlina stochastic IPM#
## ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~#
#
rm(list=ls(all=TRUE))#
#
library(doBy)#
library(lme4)#
library(MCMCglmm)#
library(parallel)#
library(rjags)#
set.seed(53241986)#
#
## Working directory must be set here, so the source()'s below run#
root=ifelse(.Platform$OS.type=="windows","c:/repos","~/Repos"); #
setwd(paste(root,"/ipm_book/Rcode/c2",sep="")); #
#
source("../utilities/Standard Graphical Pars.R");#
#
root=ifelse(.Platform$OS.type=="windows","c:/repos","~/Repos"); #
setwd(paste(root,"/ipm_book/Rcode/c7/Carlina",sep="")); #
#
#Read in the Carlina demographic data#
load("CarlinaIBMsim.Rdata")#
str(sim.data)#
#
store.sim.data <- sim.data#
#
#Select recruit data for the last 20 years (years 31 to 50)#
recr.data <- subset(sim.data,Recr==1)#
recr.data <- subset(recr.data,Yeart>30)#
recr.data <- transform(recr.data,Yeart = factor(Yeart - 30))#
#
#plant data for years 30 to 49, we do this to as the recruit size in #
#year t was generated by the year t-1 parameters#
sim.data <- subset(sim.data,Yeart>29 & Yeart<50)#
with(sim.data,table(Yeart,Recr))#
#Make Yeart a factor#
sim.data <- transform(sim.data,Yeart = factor(Yeart - 29))#
#fit some survival models#
#
mod.Surv <- glm(Surv ~ Yeart  , family = binomial, data = sim.data)#
mod.Surv.1 <- glm(Surv ~ Yeart + z  , family = binomial, data = sim.data)#
mod.Surv.2 <- glm(Surv ~ Yeart * z  , family = binomial, data = sim.data)#
#
anova(mod.Surv,mod.Surv.1,mod.Surv.2,test="Chisq")#
AIC(mod.Surv,mod.Surv.1,mod.Surv.2)#
#
#there is evidence of an interaction, as we might expect as both the intercept and slope vary from #
#year to year#
#
#Let's refit so we can easily get the parameter estimates#
#
mod.Surv <- glm(Surv ~ Yeart/ z -1 , family = binomial, data = sim.data)#
#
summary(mod.Surv)#
#
#Let's refit using glmer#
#
mod.Surv.glmer   <- glmer(Surv ~ 1 + (1|Yeart)  , family = binomial, data = sim.data)#
mod.Surv.glmer.1 <- glmer(Surv ~ z + (1|Yeart)  , family = binomial, data = sim.data)#
mod.Surv.glmer.2 <- glmer(Surv ~ z + (1|Yeart) + (0 + z|Yeart)  , family = binomial, data = sim.data)#
mod.Surv.glmer.3 <- glmer(Surv ~ z + (z|Yeart)  , family = binomial, data = sim.data)#
anova(mod.Surv.glmer,mod.Surv.glmer.1,mod.Surv.glmer.2,mod.Surv.glmer.3)#
# # d<-1#
# prior=list(R=list(V=1, fix=1), G=list(G1=list(V=diag(d), nu=d, alpha.mu=rep(0,d), alpha.V=diag(d)*1000)))#
#
# mod.Surv.MCMC   <- MCMCglmm(Surv ~ 1, random=~Yeart  , family = "categorical", data = sim.data,#
# slice=TRUE, pr=TRUE,prior=prior)#
#
# mod.Surv.MCMC.1   <- MCMCglmm(Surv ~ z, random=~Yeart  , family = "categorical", data = sim.data,#
# slice=TRUE, pr=TRUE,prior=prior)#
#
# d<-2#
# prior=list(R=list(V=1, fix=1), G=list(G1=list(V=diag(d), nu=d, alpha.mu=rep(0,d), alpha.V=diag(d)*1000)))#
#
# mod.Surv.MCMC.2  <- MCMCglmm(Surv ~ z, random=~idh(1+z):Yeart  , family = "categorical", data = sim.data,#
# slice=TRUE, pr=TRUE,prior=prior)#
#
# mod.Surv.MCMC.3   <- MCMCglmm(Surv ~ z, random=~us(1+z):Yeart  , family = "categorical", data = sim.data,#
# slice=TRUE, pr=TRUE,prior=prior)#
#
# post.modes <- posterior.mode(mod.Surv.MCMC.2$Sol)#
#
# intercepts.MCMC <- post.modes["(Intercept)"] + post.modes[3:22]#
#
# slopes.MCMC <- post.modes["z"] + post.modes[23:42]#
#
# par(mfrow=c(1,2),bty="l",pty="s",pch=19)#
#
# plot(intercepts.MCMC,coef(mod.Surv.glmer.2)$Yeart[,"(Intercept)"])#
# abline(0,1,col="red")#
# plot(slopes.MCMC,coef(mod.Surv.glmer.2)$Yeart[,"z"])#
# abline(0,1,col="red")#
#fit some flowering models#
#
flow.data <- subset(sim.data,Surv==1)#
#
mod.Flow <- glm(Flow ~ Yeart  , family = binomial, data = flow.data)#
mod.Flow.1 <- glm(Flow ~ Yeart + z  , family = binomial, data = flow.data)#
mod.Flow.2 <- glm(Flow ~ Yeart * z  , family = binomial, data = flow.data)#
#
anova(mod.Flow,mod.Flow.1,mod.Flow.2,test="Chisq")#
AIC(mod.Flow,mod.Flow.1,mod.Flow.2)#
#
#No interaction term, as expected, refit to get paramter estimates easily#
#
mod.Flow <- glm(Flow ~ Yeart + z -1 , family = binomial, data = flow.data)#
#
#Let's refit using glmer#
#
mod.Flow.glmer   <- glmer(Flow ~ 1 + (1|Yeart)  , family = binomial, data = flow.data)#
mod.Flow.glmer.1 <- glmer(Flow ~ z + (1|Yeart)  , family = binomial, data = flow.data)#
mod.Flow.glmer.2 <- glmer(Flow ~ z + (1|Yeart) + (0 + z|Yeart)  , family = binomial, data = flow.data)#
mod.Flow.glmer.3 <- glmer(Flow ~ z + (z|Yeart)  , family = binomial, data = flow.data)#
anova(mod.Flow.glmer,mod.Flow.glmer.1,mod.Flow.glmer.2,mod.Flow.glmer.3)#
#
#fit some growth models#
#
grow.data <- subset(sim.data,Surv==1 & Flow==0)#
#
mod.Grow <- lm(z1 ~ Yeart  , data = grow.data)#
mod.Grow.1 <- lm(z1 ~ Yeart +z , data = grow.data)#
mod.Grow.2 <- lm(z1 ~ Yeart *z , data = grow.data)#
#
anova(mod.Grow,mod.Grow.1,mod.Grow.2)#
AIC(mod.Grow,mod.Grow.1,mod.Grow.2)#
#
mod.Grow <- lm(z1 ~ Yeart/z-1  , data = grow.data)#
#
#Let's refit using lmer#
#
mod.Grow.lmer   <- lmer(z1 ~ 1 + (1|Yeart)  ,  data = grow.data, REML=FALSE)#
mod.Grow.lmer.1 <- lmer(z1 ~ z + (1|Yeart)  ,  data = grow.data, REML=FALSE)#
mod.Grow.lmer.2 <- lmer(z1 ~ z + (1|Yeart) + (0 + z|Yeart)  ,  data = grow.data, REML=FALSE)#
mod.Grow.lmer.3 <- lmer(z1 ~ z + (z|Yeart)  ,  data = grow.data, REML=FALSE)#
anova(mod.Grow.lmer,mod.Grow.lmer.1,mod.Grow.lmer.2,mod.Grow.lmer.3)#
#
#fit some recruit size models#
#
mod.Rcsz <- lm(z ~ 1  , data = recr.data)#
mod.Rcsz.1 <- lm(z ~ Yeart , data = recr.data)#
#
anova(mod.Rcsz,mod.Rcsz.1)#
AIC(mod.Rcsz,mod.Rcsz.1)#
#
mod.Rcsz <- lm(z ~ Yeart -1, data = recr.data)#
#
#Let's refit using lmer#
#
mod.Rcsz.lmer <- lmer(z ~ 1 + (1|Yeart) , data = recr.data)#
#
#quick check#
#
plot(as.numeric(coef(mod.Rcsz)),unlist(coef(mod.Rcsz.lmer)$Yeart))#
abline(0,1)#
#
#plot intercepts quick check#
#
plot(coef(mod.Grow)[1:20],coef(mod.Rcsz))#
cor.test(coef(mod.Grow)[1:20],coef(mod.Rcsz))#
#
###################################################################################
#fit bivariate model using Cam formulation#
N1 <- length(grow.data$z)#
N2 <- N1+length(recr.data$z)#
#
lst <- grow.data$z #
lst1 <- grow.data$z1#
rec.size <- recr.data$z#
yeart <- grow.data$Yeart#
rec.yeart <- recr.data$Yeart#
#
#plot(coef(lm(rec.size~rec.yeart-1)),coef(lm(lst1~yeart/lst-1))[1:20])#
#
######################################################################################################################################
#fit it with BUGS Cam formulation#
#
n.chains=3;#
#
rho=0.76#
a=rnorm(1,1,0.1)#
b=rnorm(1,1,0.1)#
#
X=c(lst,rep(NA,length(rec.size))); Y=c(lst1,rec.size); ngroup=20; group=c(yeart,rec.yeart);#
#
#data=list("N1","N2","X","Y","ngroup","group"); #
#
data = list(N1=N1,N2=N2,X=X,Y=Y,ngroup=ngroup,group=group)#
#
inits=function() {#
	list(mu.A=1.17,mu.Ar=3.18,B=0.71,#
	     prec.B=100,prec.e=12,prec.er=4,a=a,b=b,rho=rho,#
	     x=rnorm(ngroup,0.0,1),y=rnorm(ngroup,0.0,1),B.g=rnorm(ngroup)#
	    )#
}#
#
# # inits=function() {#
	# list(mu.A=rnorm(1,0,1),mu.Ar=rnorm(1,0,1),B=rnorm(1,0,1),#
	     # prec.B=rnorm(1,1,0.1),prec.e=rnorm(1,1,0.1),prec.er=rnorm(1,1,0.1),a=a,b=b,rho=rho,#
	     # x=rnorm(ngroup,0.0,1),y=rnorm(ngroup,0.0,1),B.g=rnorm(ngroup)#
	    # )#
# }#
parameters=c("mu.A","mu.Ar","B","sd.B","rho","sd.e","sd.er","sd.A","sd.Ar");#
#
# glmm.sim=jags.model(data=data,inits=inits,#
# file="model grow rec Car.txt",n.chains=n.chains)#
# samps <- coda.samples(glmm.sim,parameters,n.iter=400000,thin=1000)#
#save(samps,file="MCMCsamples.Rdata")#
load(file="MCMCsamples.Rdata")#
plot(samps[,1:4])#
plot(samps[,5:8])#
plot(samps[,9])#
#gelman.plot(samps)#
autocorr.plot(samps)
burn.in <- 100000#
summary.posterior <- summary(window(samps, start=burn.in))#
summary.posterior#
#
#set up parameter vector for fixed effects from mixed models#
m.par.est <- c(## survival#
				surv.int = as.numeric(fixef(mod.Surv.glmer.2)["(Intercept)"]),#
				surv.z = as.numeric(fixef(mod.Surv.glmer.2)["z"]),#
				## flowering#
				flow.int = as.numeric(fixef(mod.Flow.glmer.1)["(Intercept)"]),#
				flow.z = as.numeric(fixef(mod.Flow.glmer.1)["z"]),#
				## growth#
				grow.int = summary.posterior$statistics["mu.A","Mean"],#
				grow.z = summary.posterior$statistics["B","Mean"],#
				grow.sd = summary.posterior$statistics["sd.e","Mean"],#
				## recruit size#
				rcsz.int = summary.posterior$statistics["mu.Ar","Mean"],#
				rcsz.sd = summary.posterior$statistics["sd.er","Mean"],#
				## seed size#
				seed.int = 1,#
				seed.z = 2,#
				## recruitment probability#
				p.r = 0.00095)#
#
m.par.sd.est <- c(## survival#
                surv.int.sd        =   as.data.frame(VarCorr(mod.Surv.glmer.2))[1,5],#
                surv.z.sd           =   as.data.frame(VarCorr(mod.Surv.glmer.2))[2,5],#
                ## flowering#
                flow.int.sd       =  as.data.frame(VarCorr(mod.Flow.glmer.1))[1,5],#
                flow.z.sd          =   0.0,#
                ## growth#
                grow.int.sd         =   summary.posterior$statistics["sd.A","Mean"],#
                grow.z.sd            =   summary.posterior$statistics["sd.B","Mean"],#
                grow.sd.sd          =  0.0,#
                ## recruit size#
                rcsz.int.sd       =   summary.posterior$statistics["sd.Ar","Mean"], #
                rcsz.sd.sd       =   0.0,#
                ## seed size#
                seed.int.sd  =   0.0,#
                seed.z.sd     =   0.0,#
                ## recruitment probability#
                p.r.sd           =   0.00) #
#
save(m.par.est,m.par.sd.est,file="Mixed model parameters.Rdata")#
#
source("Carlina Demog Funs DI.R")
cov.growth.rec <-summary.posterior$statistics["rho","Mean"] * summary.posterior$statistics["sd.A","Mean"] * summary.posterior$statistics["sd.Ar","Mean"]#
#
cov.matrix <- matrix(c(summary.posterior$statistics["sd.A","Mean"]^2, cov.growth.rec, cov.growth.rec, #
summary.posterior$statistics["sd.Ar","Mean"]^2),nrow=2)
cov.matrix#
VarCovar.grow.rec#
cov2cor(cov.matrix)#
cov2cor(VarCovar.grow.rec)#
#
chol.cov.matrix <- chol(cov.matrix)
#Test we've recreated what we started with#
#
par(mfrow=c(1,2),pty="s", bty="l",pch=19)#
plot(m.par.est,m.par.true)#
abline(0,1)#
plot(m.par.sd.est,m.par.sd.true)#
abline(0,1)
iterate_model<-function(params,params.sd,chol.mat,n.est) { #
	nt<-rep(1/nBigMatrix,nBigMatrix)#
	Rt<-rep(NA,n.est)#
	size.dist<-rep(0,nBigMatrix)#
	size.dist.fl<-rep(0,nBigMatrix)#
#
	for (year.t in 1:n.est){#
		if(year.t%%1000==0) cat("iterate: ", year.t,"\n");#
#
		#params.t<-rnorm(11,params,sd.vec)#
#
		params.t=params + qnorm(runif(12,0.001,0.999))*params.sd#
#
		#sample from multivariate normal distribution for growth intercepts and yearly mean recruit size#
		#params.t[c(5,10)]<-mvrnorm(1,mean.grow.rec,VarCovar.grow.rec)#
#
		params.t[c("grow.int","rcsz.int")] <- params[c("grow.int","rcsz.int")]+matrix(qnorm(runif(2,0.001,0.999)),1,2) %*% chol.mat #
		year.K<-mk_K(nBigMatrix,params.t,minsize,maxsize)#
		nt1<-year.K$K %*% nt#
		sum.nt1<-sum(nt1)#
		Rt[year.t]<-log(sum.nt1)#
#
		dist.fl.year <- nt * s_z(year.K$meshpts,params.t) * p_bz(year.K$meshpts,params.t)#
#
		size.dist <- size.dist + nt#
#
		size.dist.fl<-size.dist.fl+dist.fl.year#
#
		nt <- nt1 / sum(nt1)#
	}#
	size.dist <- size.dist / sum(size.dist)#
	size.dist.fl <- size.dist.fl / sum(size.dist.fl)#
	return(list(size.dist=size.dist,size.dist.fl=size.dist.fl,#
		    Rt=Rt,meshpts=year.K$meshpts))#
}#
##run model and plot some graphs#
nBigMatrix <- 100#
n.est <- 50000#
n.runin <- 500#
minsize <- 1.5#
maxsize <- 5#
###########################
#First let's iterate the model using the estimated mixed model parameters#
#
iter <- iterate_model(m.par.est,m.par.sd.est,chol.cov.matrix,n.est)#
#
stoch.lambda <- mean(iter$Rt[n.runin:n.est],na.rm=T)#
SE.stoch.lambda <- sd(iter$Rt[n.runin:n.est],na.rm=T) / sqrt(length(iter$Rt[n.runin:n.est]))#
cat("Stochastic growth rate IPM =",stoch.lambda,"\n")#
cat("Approximate 95% CI for stochastic growth rate from IPM ",stoch.lambda + 2*SE.stoch.lambda, " to ", stoch.lambda - 2*SE.stoch.lambda,"\n")#
#
#from IPM (log scale)#
sum(iter$size.dist * iter$meshpts)#
#
#from IPM#
sum(iter$size.dist.fl * exp(iter$meshpts))
## ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~#
## Fit and run fixed and mixed effects effects Carlina stochastic IPM#
## ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~#
#
rm(list=ls(all=TRUE))#
#
library(doBy)#
library(lme4)#
library(MCMCglmm)#
library(parallel)#
library(rjags)#
set.seed(53241986)#
#
## Working directory must be set here, so the source()'s below run#
root=ifelse(.Platform$OS.type=="windows","c:/repos","~/Repos"); #
setwd(paste(root,"/ipm_book/Rcode/c2",sep="")); #
#
source("../utilities/Standard Graphical Pars.R");#
#
root=ifelse(.Platform$OS.type=="windows","c:/repos","~/Repos"); #
setwd(paste(root,"/ipm_book/Rcode/c7/Carlina",sep="")); #
#
#Read in the Carlina demographic data#
load("CarlinaIBMsim.Rdata")#
str(sim.data)#
#
store.sim.data <- sim.data#
#
#Select recruit data for the last 20 years (years 31 to 50)#
recr.data <- subset(sim.data,Recr==1)#
recr.data <- subset(recr.data,Yeart>30)#
recr.data <- transform(recr.data,Yeart = factor(Yeart - 30))#
#
#plant data for years 30 to 49, we do this to as the recruit size in #
#year t was generated by the year t-1 parameters#
sim.data <- subset(sim.data,Yeart>29 & Yeart<50)#
with(sim.data,table(Yeart,Recr))#
#Make Yeart a factor#
sim.data <- transform(sim.data,Yeart = factor(Yeart - 29))#
#
#fit some survival models#
#
mod.Surv <- glm(Surv ~ Yeart  , family = binomial, data = sim.data)#
mod.Surv.1 <- glm(Surv ~ Yeart + z  , family = binomial, data = sim.data)#
mod.Surv.2 <- glm(Surv ~ Yeart * z  , family = binomial, data = sim.data)#
#
anova(mod.Surv,mod.Surv.1,mod.Surv.2,test="Chisq")#
AIC(mod.Surv,mod.Surv.1,mod.Surv.2)#
#
#there is evidence of an interaction, as we might expect as both the intercept and slope vary from #
#year to year#
#
#Let's refit so we can easily get the parameter estimates#
#
mod.Surv <- glm(Surv ~ Yeart/ z -1 , family = binomial, data = sim.data)#
#
summary(mod.Surv)#
#
#Let's refit using glmer#
#
mod.Surv.glmer   <- glmer(Surv ~ 1 + (1|Yeart)  , family = binomial, data = sim.data)#
mod.Surv.glmer.1 <- glmer(Surv ~ z + (1|Yeart)  , family = binomial, data = sim.data)#
mod.Surv.glmer.2 <- glmer(Surv ~ z + (1|Yeart) + (0 + z|Yeart)  , family = binomial, data = sim.data)#
mod.Surv.glmer.3 <- glmer(Surv ~ z + (z|Yeart)  , family = binomial, data = sim.data)#
anova(mod.Surv.glmer,mod.Surv.glmer.1,mod.Surv.glmer.2,mod.Surv.glmer.3)#
# # d<-1#
# prior=list(R=list(V=1, fix=1), G=list(G1=list(V=diag(d), nu=d, alpha.mu=rep(0,d), alpha.V=diag(d)*1000)))#
#
# mod.Surv.MCMC   <- MCMCglmm(Surv ~ 1, random=~Yeart  , family = "categorical", data = sim.data,#
# slice=TRUE, pr=TRUE,prior=prior)#
#
# mod.Surv.MCMC.1   <- MCMCglmm(Surv ~ z, random=~Yeart  , family = "categorical", data = sim.data,#
# slice=TRUE, pr=TRUE,prior=prior)#
#
# d<-2#
# prior=list(R=list(V=1, fix=1), G=list(G1=list(V=diag(d), nu=d, alpha.mu=rep(0,d), alpha.V=diag(d)*1000)))#
#
# mod.Surv.MCMC.2  <- MCMCglmm(Surv ~ z, random=~idh(1+z):Yeart  , family = "categorical", data = sim.data,#
# slice=TRUE, pr=TRUE,prior=prior)#
#
# mod.Surv.MCMC.3   <- MCMCglmm(Surv ~ z, random=~us(1+z):Yeart  , family = "categorical", data = sim.data,#
# slice=TRUE, pr=TRUE,prior=prior)#
#
# post.modes <- posterior.mode(mod.Surv.MCMC.2$Sol)#
#
# intercepts.MCMC <- post.modes["(Intercept)"] + post.modes[3:22]#
#
# slopes.MCMC <- post.modes["z"] + post.modes[23:42]#
#
# par(mfrow=c(1,2),bty="l",pty="s",pch=19)#
#
# plot(intercepts.MCMC,coef(mod.Surv.glmer.2)$Yeart[,"(Intercept)"])#
# abline(0,1,col="red")#
# plot(slopes.MCMC,coef(mod.Surv.glmer.2)$Yeart[,"z"])#
# abline(0,1,col="red")#
#fit some flowering models#
#
flow.data <- subset(sim.data,Surv==1)#
#
mod.Flow <- glm(Flow ~ Yeart  , family = binomial, data = flow.data)#
mod.Flow.1 <- glm(Flow ~ Yeart + z  , family = binomial, data = flow.data)#
mod.Flow.2 <- glm(Flow ~ Yeart * z  , family = binomial, data = flow.data)#
#
anova(mod.Flow,mod.Flow.1,mod.Flow.2,test="Chisq")#
AIC(mod.Flow,mod.Flow.1,mod.Flow.2)#
#
#No interaction term, as expected, refit to get paramter estimates easily#
#
mod.Flow <- glm(Flow ~ Yeart + z -1 , family = binomial, data = flow.data)#
#
#Let's refit using glmer#
#
mod.Flow.glmer   <- glmer(Flow ~ 1 + (1|Yeart)  , family = binomial, data = flow.data)#
mod.Flow.glmer.1 <- glmer(Flow ~ z + (1|Yeart)  , family = binomial, data = flow.data)#
mod.Flow.glmer.2 <- glmer(Flow ~ z + (1|Yeart) + (0 + z|Yeart)  , family = binomial, data = flow.data)#
mod.Flow.glmer.3 <- glmer(Flow ~ z + (z|Yeart)  , family = binomial, data = flow.data)#
anova(mod.Flow.glmer,mod.Flow.glmer.1,mod.Flow.glmer.2,mod.Flow.glmer.3)#
#
#fit some growth models#
#
grow.data <- subset(sim.data,Surv==1 & Flow==0)#
#
mod.Grow <- lm(z1 ~ Yeart  , data = grow.data)#
mod.Grow.1 <- lm(z1 ~ Yeart +z , data = grow.data)#
mod.Grow.2 <- lm(z1 ~ Yeart *z , data = grow.data)#
#
anova(mod.Grow,mod.Grow.1,mod.Grow.2)#
AIC(mod.Grow,mod.Grow.1,mod.Grow.2)#
#
mod.Grow <- lm(z1 ~ Yeart/z-1  , data = grow.data)#
#
#Let's refit using lmer#
#
mod.Grow.lmer   <- lmer(z1 ~ 1 + (1|Yeart)  ,  data = grow.data, REML=FALSE)#
mod.Grow.lmer.1 <- lmer(z1 ~ z + (1|Yeart)  ,  data = grow.data, REML=FALSE)#
mod.Grow.lmer.2 <- lmer(z1 ~ z + (1|Yeart) + (0 + z|Yeart)  ,  data = grow.data, REML=FALSE)#
mod.Grow.lmer.3 <- lmer(z1 ~ z + (z|Yeart)  ,  data = grow.data, REML=FALSE)#
anova(mod.Grow.lmer,mod.Grow.lmer.1,mod.Grow.lmer.2,mod.Grow.lmer.3)#
#
#fit some recruit size models#
#
mod.Rcsz <- lm(z ~ 1  , data = recr.data)#
mod.Rcsz.1 <- lm(z ~ Yeart , data = recr.data)#
#
anova(mod.Rcsz,mod.Rcsz.1)#
AIC(mod.Rcsz,mod.Rcsz.1)#
#
mod.Rcsz <- lm(z ~ Yeart -1, data = recr.data)#
#
#Let's refit using lmer#
#
mod.Rcsz.lmer <- lmer(z ~ 1 + (1|Yeart) , data = recr.data)#
#
#quick check#
#
plot(as.numeric(coef(mod.Rcsz)),unlist(coef(mod.Rcsz.lmer)$Yeart))#
abline(0,1)#
#
#set up parameter vector with yearly estimates from fixed effects models (lm and glm)#
m.par.est <- matrix(NA,nrow=12,ncol=20)#
m.par.est[1,] <- coef(mod.Surv)[1:20]#
m.par.est[2,] <- coef(mod.Surv)[21:40]#
m.par.est[3,] <- coef(mod.Flow)[1:20]#
m.par.est[4,] <- coef(mod.Flow)[21]#
m.par.est[5,] <- coef(mod.Grow)[1:20]#
m.par.est[6,] <- coef(mod.Grow)[21:40]#
m.par.est[7,] <- summary(mod.Grow)$sigma#
m.par.est[8,] <- coef(mod.Rcsz)[1:20]#
m.par.est[9,] <- summary(mod.Rcsz)$sigma#
m.par.est[10,] <- 1#
m.par.est[11,] <- 2#
m.par.est[12,] <- 0.00095#
#
#set up parameter vector with yearly estimates from mixed effects models (lmer and glmer)#
m.par.est.mm <- matrix(NA,nrow=12,ncol=20)#
m.par.est.mm[1,] <- as.vector(unlist(coef(mod.Surv.glmer.2 )$Yeart["(Intercept)"]))#
m.par.est.mm[2,] <- as.vector(unlist(coef(mod.Surv.glmer.2 )$Yeart["z"]))#
m.par.est.mm[3,] <- as.vector(unlist(coef(mod.Flow.glmer.1 )$Yeart["(Intercept)"]))#
m.par.est.mm[4,] <- as.vector(unlist(coef(mod.Flow.glmer.1 )$Yeart["z"]))#
m.par.est.mm[5,] <- as.vector(unlist(coef(mod.Grow.lmer.2)$Yeart["(Intercept)"]))#
m.par.est.mm[6,] <- as.vector(unlist(coef(mod.Grow.lmer.2)$Yeart["z"]))#
m.par.est.mm[7,] <- as.vector(unlist(summary(mod.Grow.lmer.2)$sigma))#
m.par.est.mm[8,] <- as.vector(unlist(coef(mod.Rcsz.lmer)$Yeart["(Intercept)"]))#
m.par.est.mm[9,] <- as.vector(unlist(summary(mod.Rcsz.lmer)$sigma))#
m.par.est.mm[10,] <- 1#
m.par.est.mm[11,] <- 2#
m.par.est.mm[12,] <- 0.00095
m.par.est
plot(as.numeric(coef(mod.Rcsz)),unlist(coef(mod.Rcsz.lmer)$Yeart))#
abline(0,1)#
#
#set up parameter vector with yearly estimates from fixed effects models (lm and glm)#
m.par.est <- matrix(NA,nrow=12,ncol=20)#
m.par.est[1,] <- coef(mod.Surv)[1:20]#
m.par.est[2,] <- coef(mod.Surv)[21:40]#
m.par.est[3,] <- coef(mod.Flow)[1:20]#
m.par.est[4,] <- coef(mod.Flow)[21]#
m.par.est[5,] <- coef(mod.Grow)[1:20]#
m.par.est[6,] <- coef(mod.Grow)[21:40]#
m.par.est[7,] <- summary(mod.Grow)$sigma#
m.par.est[8,] <- coef(mod.Rcsz)[1:20]#
m.par.est[9,] <- summary(mod.Rcsz)$sigma#
m.par.est[10,] <- 1#
m.par.est[11,] <- 2#
m.par.est[12,] <- 0.00095#
#
#set up parameter vector with yearly estimates from mixed effects models (lmer and glmer)#
m.par.est.mm <- matrix(NA,nrow=12,ncol=20)#
m.par.est.mm[1,] <- as.vector(unlist(coef(mod.Surv.glmer.2 )$Yeart["(Intercept)"]))#
m.par.est.mm[2,] <- as.vector(unlist(coef(mod.Surv.glmer.2 )$Yeart["z"]))#
m.par.est.mm[3,] <- as.vector(unlist(coef(mod.Flow.glmer.1 )$Yeart["(Intercept)"]))#
m.par.est.mm[4,] <- as.vector(unlist(coef(mod.Flow.glmer.1 )$Yeart["z"]))#
m.par.est.mm[5,] <- as.vector(unlist(coef(mod.Grow.lmer.2)$Yeart["(Intercept)"]))#
m.par.est.mm[6,] <- as.vector(unlist(coef(mod.Grow.lmer.2)$Yeart["z"]))#
m.par.est.mm[7,] <- as.vector(unlist(summary(mod.Grow.lmer.2)$sigma))#
m.par.est.mm[8,] <- as.vector(unlist(coef(mod.Rcsz.lmer)$Yeart["(Intercept)"]))#
m.par.est.mm[9,] <- as.vector(unlist(summary(mod.Rcsz.lmer)$sigma))#
m.par.est.mm[10,] <- 1#
m.par.est.mm[11,] <- 2#
m.par.est.mm[12,] <- 0.00095#
#
plot(m.par.est,m.par.est.mm)#
#
source("Carlina Demog Funs DI.R") #
#
rownames(m.par.est) <- names(m.par.true)#
rownames(m.par.est.mm) <- names(m.par.true)#
#
save(m.par.est,m.par.est.mm,file="Yearly parameters.Rdata")
#test correlations between yearly parameters#
cor.test(m.par.est[1,],m.par.est[3,])#
cor.test(m.par.est[1,],m.par.est[5,])#
cor.test(m.par.est[1,],m.par.est[8,])#
#
cor.test(m.par.est[3,],m.par.est[5,])#
cor.test(m.par.est[3,],m.par.est[8,])#
#
cor.test(m.par.est["grow.int",],m.par.est["rcsz.int",])#
plot(m.par.est["grow.int",],m.par.est["rcsz.int",])#
######################################################################
#Run stochastic fixed effects IPM#
######################################################################
#
iterate_model<-function(params,n.years,n.est) {#
#
#Construct the yearly kernels#
#
	K.year.i <- array(NA,c(n.years,nBigMatrix,nBigMatrix))#
	for(i in 1:n.years){#
		year.K<-mk_K(nBigMatrix,params[,i],minsize,maxsize)#
		K.year.i[i,,] <- year.K$K#
	}#
    h <- year.K$h; #
#
#Calculate mean kernel, v and w#
#
	mean.kernel <- apply(K.year.i,2:3,mean)#
	w <- Re(eigen(mean.kernel)$vectors[,1]); #
	v <- Re(eigen(t(mean.kernel))$vectors[,1]);#
#
	# scale eigenvectors <v,w>=1 #
	w <- abs(w)/sum(h*abs(w))#
	v <- abs(v)#
	v <- v/(h*sum(v*w))#
    cat(h*sum(v*w)," should = 1","\n")#
	v.Ktw <- rep(NA,n.years)#
	for(i in 1:n.years) {#
		v.Ktw[i] <- sum(v*(K.year.i[i,,] %*% w))*h#
	}#
#initialize variables	#
#
	nt<-rep(1/nBigMatrix,nBigMatrix)#
	rt.V <- rt.N <- rep(NA,n.est)#
	size.dist<-rep(0,nBigMatrix)#
	size.dist.fl<-rep(0,nBigMatrix)#
#
#Iterate model#
#
	for (year.t in 1:n.est){#
		if(year.t%%10000==0) cat("iterate: ", year.t,"\n");#
#
		#Select year at random#
		year.i <- sample(1:n.years,1)#
		#iterate model with year-specific kernel#
		nt1<-K.year.i[year.i,,] %*% nt#
		sum.nt1<-sum(nt1)#
		#Calculate log growth rates  #
		rt.V[year.t] <- log(sum(nt1*v)/sum(nt*v))#
		rt.N[year.t] <- log(sum(nt1)/sum(nt))#
		#distribution of flowering plant sizes in year t#
		dist.fl.year <- nt * s_z(year.K$meshpts,params[,year.i]) * p_bz(year.K$meshpts,params[,year.i])#
#
		#distribution of plant sizes in year t#
		size.dist <- size.dist + nt#
		size.dist.fl<-size.dist.fl+dist.fl.year#
#
		nt <- nt1 / sum.nt1  #
		# cat(Rt[year.t],"  ",sum.nt,"\n")#
	}#
#
   	size.dist <- size.dist / sum(size.dist)#
	size.dist.fl <- size.dist.fl / sum(size.dist.fl)#
	return(list(size.dist=size.dist,size.dist.fl=size.dist.fl,#
		    rt.N=rt.N,rt.V=rt.V,meshpts=year.K$meshpts,#
		    mean.kernel=mean.kernel,v.Ktw=v.Ktw))#
}#
#
nBigMatrix <- 100#
n.est <- 20000#
n.runin <- 500#
minsize <- 1.5#
maxsize <- 5#
#
iter <- iterate_model(m.par.est,20,n.est)#
#
rt.N <- iter$rt.N;#
rt.V <- iter$rt.V; #
#
Ls.Nt <- mean(rt.N)#
SE.Ls.Nt <- sqrt(var(rt.N)/length(rt.N))#
acf(rt.N,plot=FALSE)$acf[2:5];#
#
Ls.Vt <- mean(rt.V)#
SE.Ls.Vt <- sqrt(var(rt.V)/length(rt.V))#
acf(rt.V,plot=FALSE)$acf[2:5];#
#
cat("log Lambda S using Nt ",Ls.Nt," 95% c.i ",Ls.Nt+2*SE.Ls.Nt," ",Ls.Nt-2*SE.Ls.Nt,"\n")#
cat("log Lambda S using Vt ",Ls.Vt," 95% c.i ",Ls.Vt+2*SE.Ls.Vt," ",Ls.Nt-2*SE.Ls.Vt,"\n")#
#
lam.1 <- Re(eigen(iter$mean.kernel)$values[1])#
#
var.v.Ktw <- var(iter$v.Ktw)#
#
approx.Ls <- log(lam.1) - var.v.Ktw/(2*lam.1*lam.1)#
#
cat("SFA Stochastic log Lambda = ",approx.Ls,"\n")#
#
n.est <- 500000#
#
iter <- iterate_model(m.par.est,20,n.est)#
#
rt.N <- iter$rt.N;#
rt.V <- iter$rt.V; #
#
Ls.Nt <- mean(rt.N)#
SE.Ls.Nt <- sqrt(var(rt.N)/length(rt.N))#
acf(rt.N,plot=FALSE)$acf[2:5];#
#
Ls.Vt <- mean(rt.V)#
SE.Ls.Vt <- sqrt(var(rt.V)/length(rt.V))#
acf(rt.V,plot=FALSE)$acf[2:5];#
#
cat("log Lambda S using Nt ",Ls.Nt," 95% c.i ",Ls.Nt+2*SE.Ls.Nt," ",Ls.Nt-2*SE.Ls.Nt,"\n")#
cat("log Lambda S using Vt ",Ls.Vt," 95% c.i ",Ls.Vt+2*SE.Ls.Vt," ",Ls.Vt-2*SE.Ls.Vt,"\n")#
#well that's not too bad #
#
#how fast do they converge?#
#
Ls.N <- cumsum(rt.N)/(1:n.est)#
Ls.V <- cumsum(rt.V)/(1:n.est)#
#
n.samp <- n.est#
#
plot(Ls.N[1:n.samp],type="l")#
points(Ls.V[1:n.samp],type="l",col="red")#
abline(h=mean(rt.N))#
#what about log(V(t+1)) being approximately independent of log(V(t))?#
#
set_graph_pars("panel2");#
#
pacf(rt.N,lag.max=10,ylim=c(-0.1,0.1),main="")#
add_panel_label("a")#
pacf(rt.V,lag.max=10,ylim=c(-0.1,0.1),main="")#
add_panel_label("b")
## ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~#
## Fit and run fixed and mixed effects effects Carlina stochastic IPM#
## ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~#
#
rm(list=ls(all=TRUE))#
#
library(doBy)#
library(lme4)#
library(MCMCglmm)#
library(parallel)#
library(rjags)#
set.seed(53241986)#
#
## Working directory must be set here, so the source()'s below run#
root=ifelse(.Platform$OS.type=="windows","c:/repos","~/Repos"); #
setwd(paste(root,"/ipm_book/Rcode/c2",sep="")); #
#
source("../utilities/Standard Graphical Pars.R");#
#
root=ifelse(.Platform$OS.type=="windows","c:/repos","~/Repos"); #
setwd(paste(root,"/ipm_book/Rcode/c7/Carlina",sep="")); #
#
load("Yearly parameters.Rdata")
source("Carlina Demog Funs DI.R")
#test correlations between yearly parameters#
cor.test(m.par.est[1,],m.par.est[3,])#
cor.test(m.par.est[1,],m.par.est[5,])#
cor.test(m.par.est[1,],m.par.est[8,])#
#
cor.test(m.par.est[3,],m.par.est[5,])#
cor.test(m.par.est[3,],m.par.est[8,])#
#
cor.test(m.par.est["grow.int",],m.par.est["rcsz.int",])#
plot(m.par.est["grow.int",],m.par.est["rcsz.int",])
######################################################################
#Stochastic perturbation analysis#
######################################################################
#
nBigMatrix <- 100#
n.est <- 20000#
n.runin <- 500#
minsize <- 1.5#
maxsize <- 5#
n.years <-20#
stoc_pert_analysis<-function(params,n.est,n.runin){#
	year.i <- sample(1:n.years,n.est+1,replace=TRUE)#
#
	K.year.i <- array(NA,c(n.years,nBigMatrix,nBigMatrix))#
	for(i in 1:n.years){#
		year.K<-mk_K(nBigMatrix,params[,i],minsize,maxsize)#
		K.year.i[i,,] <- year.K$K#
	}#
#
	h <- year.K$h; #
#Calculate mean kernel, v and w#
#
	mean.kernel <- apply(K.year.i,2:3,mean)#
#
	w <- Re(eigen(mean.kernel)$vectors[,1]); #
	v <- Re(eigen(t(mean.kernel))$vectors[,1]);#
#
	# scale eigenvectors <v,w>=1 #
	w <- abs(w)/sum(h*abs(w))#
	v <- abs(v)#
	v <- v/(h*sum(v*w))#
    cat(h*sum(v*w)," should = 1","\n")#
#
#Esimate Lambda s#
#initialize variables	#
#
	nt<-rep(1/nBigMatrix,nBigMatrix)#
	rt.V <- rt.N <- rep(NA,n.est)#
#Iterate model#
#
	for (year.t in 1:n.est){#
		if(year.t%%10000==0) cat("iterate: ", year.t,"\n");#
		#iterate model with year-specific kernel#
		nt1<-K.year.i[year.i[year.t],,] %*% nt#
		sum.nt1<-sum(nt1)#
		#Calculate log growth rates  #
		rt.V[year.t] <- log(sum(nt1*v)/sum(nt*v))#
		rt.N[year.t] <- log(sum(nt1)/sum(nt))#
		nt <- nt1 / sum.nt1  #
	}#
#
Ls <- exp(mean(rt.V))#
### Get wt and Rt time series ####
	wt<-matrix(1/nBigMatrix, nrow=n.est+1, ncol=nBigMatrix);#
	for (i in 1:n.est) {#
		K             <- K.year.i[year.i[i],,]#
		wt[i+1,]  <-K %*% wt[i,]#
		wt[i+1,]  <-wt[i+1,]/sum(wt[i+1,]);#
		if(i%%10000==0) cat("wt ",i,"\n")#
#
	}#
### Get vt time series ####
	vt<-matrix(1/nBigMatrix, nrow=n.est+1, ncol=nBigMatrix);#
	for (i in (n.est+1):2) {#
		K           <- K.year.i[year.i[i],,]#
		vt[i-1,]  <- vt[i,] %*% K#
		vt[i-1,]  <- vt[i-1,]/sum(vt[i-1,]);#
		if(i%%10000==0) cat("vt  ",i,"\n")#
#
	}#
#
sens.s <- matrix(0,nrow=nBigMatrix,ncol=nBigMatrix);#
#
elas.s <- sens.s#
for (i in n.runin:(n.est-n.runin)) {#
#
		#standard calculations needed for the various formulae#
		K                 <-   K.year.i[year.i[i],,]#
		vt1.wt        <-   vt[i+1,]%*%t(wt[i,])#
		vt1.K.wt   <- sum(vt[i+1,] * (K %*% wt[i,]))#
#
		#calculation of the standard sensitivities and elasticities#
		sens.s<-sens.s+vt1.wt/vt1.K.wt;#
#
		elas.s<-elas.s+K*(vt1.wt/vt1.K.wt);#
}#
#
 elas.s <- elas.s/(n.est-2*n.runin+1)#
 sens.s <- Ls*sens.s/(n.est-2*n.runin+1)#
 return(list(meshpts=year.K$meshpts,h=h,sens.s=sens.s,elas.s=elas.s,mean.kernel=mean.kernel,Ls=Ls))#
#
}#
#
pert.K <- stoc_pert_analysis(m.par.est,n.est,n.runin)#
#
meshpts <- pert.K$meshpts#
h.inv.2 <- 1 / (pert.K$h^2)#
#
Kmean.elas <- pert.K$mean.kernel * pert.K$sens.s / pert.K$Ls#
#
K.sens <- pert.K$sens.s * h.inv.2#
K.elas <-  pert.K$elas.s * h.inv.2#
K.mean.elas <- Kmean.elas * h.inv.2#
K.sd.elas <- K.elas - K.mean.elas#
#
cat("sum elasticities = ",sum(pert.K$elas.s)," should be 1")#
#
## plot these#
ikeep <- which(meshpts>1.5 & meshpts<5) # use to extract a region to plot#
#postscript("KernSensElas.eps", #
#          width=8, height=8, horizontal=FALSE, paper="special")#
## plot the sensitivity kernel and the three elasticity surfaces#
set_graph_pars("panel4")#
image(meshpts[ikeep], meshpts[ikeep], t(K.sens[ikeep,ikeep]),#
      col=grey(seq(0.6, 1, length=100)),#
      xlab="Size (t), z", ylab="Size (t+1), z\'")#
contour(meshpts[ikeep], meshpts[ikeep], t(K.sens[ikeep,ikeep]), add=TRUE)#
add_panel_label("a")#
image(meshpts[ikeep], meshpts[ikeep], t(K.elas[ikeep,ikeep]),#
      col=grey(seq(0.6, 1, length=100)),#
      xlab="Size (t), z", ylab="Size (t+1), z\'")#
contour(meshpts[ikeep], meshpts[ikeep], t(K.elas[ikeep,ikeep]), #
        add=TRUE,levels=c(0.05,0.2,0.4))#
add_panel_label("b")#
image(meshpts[ikeep], meshpts[ikeep], t(K.mean.elas[ikeep,ikeep]),#
      col=grey(seq(0.6, 1, length=100)),#
      xlab="Size (t), z", ylab="Size (t+1), z\'")#
contour(meshpts[ikeep], meshpts[ikeep], t(K.mean.elas[ikeep,ikeep]), #
        add=TRUE,levels=c(0.1,0.4,0.8))#
add_panel_label("c")#
image(meshpts[ikeep], meshpts[ikeep], t(K.sd.elas[ikeep,ikeep]),#
      col=grey(seq(0.6, 1, length=100)),#
      xlab="Size (t), z", ylab="Size (t+1), z\'")#
contour(meshpts[ikeep], meshpts[ikeep], t(K.sd.elas[ikeep,ikeep]), #
        add=TRUE,levels=c(-0.05,-0.2,-0.4))#
add_panel_label("d")#
dev.copy2eps(file="~/Repos/ipm_book/c7/figures/CarlinaElasSens.eps")
## ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~#
## Fit and run fixed and mixed effects effects Carlina stochastic IPM#
## ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~#
#
rm(list=ls(all=TRUE))#
#
library(doBy)#
library(lme4)#
library(MCMCglmm)#
library(parallel)#
library(rjags)#
set.seed(53241986)#
#
## Working directory must be set here, so the source()'s below run#
root=ifelse(.Platform$OS.type=="windows","c:/repos","~/Repos"); #
setwd(paste(root,"/ipm_book/Rcode/c2",sep="")); #
#
source("../utilities/Standard Graphical Pars.R");#
#
root=ifelse(.Platform$OS.type=="windows","c:/repos","~/Repos"); #
setwd(paste(root,"/ipm_book/Rcode/c7/Carlina",sep="")); #
#
load("Yearly parameters.Rdata")#
#
source("Carlina Demog Funs DI.R")
######################################################################
#Stochastic perturbation analysis#
######################################################################
#
nBigMatrix <- 100#
n.est <- 20000#
n.runin <- 500#
minsize <- 1.5#
maxsize <- 5#
n.years <-20#
stoc_pert_analysis<-function(params,n.est,n.runin){#
	year.i <- sample(1:n.years,n.est+1,replace=TRUE)#
#
	K.year.i <- array(NA,c(n.years,nBigMatrix,nBigMatrix))#
	for(i in 1:n.years){#
		year.K<-mk_K(nBigMatrix,params[,i],minsize,maxsize)#
		K.year.i[i,,] <- year.K$K#
	}#
#
	h <- year.K$h; #
#Calculate mean kernel, v and w#
#
	mean.kernel <- apply(K.year.i,2:3,mean)#
#
	w <- Re(eigen(mean.kernel)$vectors[,1]); #
	v <- Re(eigen(t(mean.kernel))$vectors[,1]);#
#
	# scale eigenvectors <v,w>=1 #
	w <- abs(w)/sum(h*abs(w))#
	v <- abs(v)#
	v <- v/(h*sum(v*w))#
    cat(h*sum(v*w)," should = 1","\n")#
#
#Esimate Lambda s#
#initialize variables	#
#
	nt<-rep(1/nBigMatrix,nBigMatrix)#
	rt.V <- rt.N <- rep(NA,n.est)#
#Iterate model#
#
	for (year.t in 1:n.est){#
		if(year.t%%10000==0) cat("iterate: ", year.t,"\n");#
		#iterate model with year-specific kernel#
		nt1<-K.year.i[year.i[year.t],,] %*% nt#
		sum.nt1<-sum(nt1)#
		#Calculate log growth rates  #
		rt.V[year.t] <- log(sum(nt1*v)/sum(nt*v))#
		rt.N[year.t] <- log(sum(nt1)/sum(nt))#
		nt <- nt1 / sum.nt1  #
	}#
#
Ls <- exp(mean(rt.V))#
### Get wt and Rt time series ####
	wt<-matrix(1/nBigMatrix, nrow=n.est+1, ncol=nBigMatrix);#
	for (i in 1:n.est) {#
		K             <- K.year.i[year.i[i],,]#
		wt[i+1,]  <-K %*% wt[i,]#
		wt[i+1,]  <-wt[i+1,]/sum(wt[i+1,]);#
		if(i%%10000==0) cat("wt ",i,"\n")#
#
	}#
### Get vt time series ####
	vt<-matrix(1/nBigMatrix, nrow=n.est+1, ncol=nBigMatrix);#
	for (i in (n.est+1):2) {#
		K           <- K.year.i[year.i[i],,]#
		vt[i-1,]  <- vt[i,] %*% K#
		vt[i-1,]  <- vt[i-1,]/sum(vt[i-1,]);#
		if(i%%10000==0) cat("vt  ",i,"\n")#
#
	}#
#
sens.s <- matrix(0,nrow=nBigMatrix,ncol=nBigMatrix);#
#
elas.s <- sens.s#
for (i in n.runin:(n.est-n.runin)) {#
#
		#standard calculations needed for the various formulae#
		K                 <-   K.year.i[year.i[i],,]#
		vt1.wt        <-   vt[i+1,]%*%t(wt[i,])#
		vt1.K.wt   <- sum(vt[i+1,] * (K %*% wt[i,]))#
#
		#calculation of the standard sensitivities and elasticities#
		sens.s<-sens.s+vt1.wt/vt1.K.wt;#
#
		elas.s<-elas.s+K*(vt1.wt/vt1.K.wt);#
}#
#
 elas.s <- elas.s/(n.est-2*n.runin+1)#
 sens.s <- Ls*sens.s/(n.est-2*n.runin+1)#
 return(list(meshpts=year.K$meshpts,h=h,sens.s=sens.s,elas.s=elas.s,mean.kernel=mean.kernel,Ls=Ls))#
#
}#
#
pert.K <- stoc_pert_analysis(m.par.est,n.est,n.runin)#
#
meshpts <- pert.K$meshpts#
h.inv.2 <- 1 / (pert.K$h^2)#
#
Kmean.elas <- pert.K$mean.kernel * pert.K$sens.s / pert.K$Ls#
#
K.sens <- pert.K$sens.s * h.inv.2#
K.elas <-  pert.K$elas.s * h.inv.2#
K.mean.elas <- Kmean.elas * h.inv.2#
K.sd.elas <- K.elas - K.mean.elas#
#
cat("sum elasticities = ",sum(pert.K$elas.s)," should be 1")#
#
## plot these#
ikeep <- which(meshpts>1.5 & meshpts<5) # use to extract a region to plot#
#postscript("KernSensElas.eps", #
#          width=8, height=8, horizontal=FALSE, paper="special")#
## plot the sensitivity kernel and the three elasticity surfaces#
set_graph_pars("panel4")#
image(meshpts[ikeep], meshpts[ikeep], t(K.sens[ikeep,ikeep]),#
      col=grey(seq(0.6, 1, length=100)),#
      xlab="Size (t), z", ylab="Size (t+1), z\'")#
contour(meshpts[ikeep], meshpts[ikeep], t(K.sens[ikeep,ikeep]), add=TRUE)#
add_panel_label("a")#
image(meshpts[ikeep], meshpts[ikeep], t(K.elas[ikeep,ikeep]),#
      col=grey(seq(0.6, 1, length=100)),#
      xlab="Size (t), z", ylab="Size (t+1), z\'")#
contour(meshpts[ikeep], meshpts[ikeep], t(K.elas[ikeep,ikeep]), #
        add=TRUE,levels=c(0.05,0.2,0.4))#
add_panel_label("b")#
image(meshpts[ikeep], meshpts[ikeep], t(K.mean.elas[ikeep,ikeep]),#
      col=grey(seq(0.6, 1, length=100)),#
      xlab="Size (t), z", ylab="Size (t+1), z\'")#
contour(meshpts[ikeep], meshpts[ikeep], t(K.mean.elas[ikeep,ikeep]), #
        add=TRUE,levels=c(0.1,0.4,0.8))#
add_panel_label("c")#
image(meshpts[ikeep], meshpts[ikeep], t(K.sd.elas[ikeep,ikeep]),#
      col=grey(seq(0.6, 1, length=100)),#
      xlab="Size (t), z", ylab="Size (t+1), z\'")#
contour(meshpts[ikeep], meshpts[ikeep], t(K.sd.elas[ikeep,ikeep]), #
        add=TRUE,levels=c(-0.05,-0.2,-0.4))#
add_panel_label("d")#
dev.copy2eps(file="~/Repos/ipm_book/c7/figures/CarlinaElasSens.eps")
## ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~#
## Fit and run fixed and mixed effects effects Carlina stochastic IPM#
## ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~#
#
rm(list=ls(all=TRUE))#
#
library(doBy)#
library(lme4)#
library(MCMCglmm)#
library(parallel)#
library(rjags)#
set.seed(53241986)#
#
## Working directory must be set here, so the source()'s below run#
root=ifelse(.Platform$OS.type=="windows","c:/repos","~/Repos"); #
setwd(paste(root,"/ipm_book/Rcode/c2",sep="")); #
#
source("../utilities/Standard Graphical Pars.R");#
#
root=ifelse(.Platform$OS.type=="windows","c:/repos","~/Repos"); #
setwd(paste(root,"/ipm_book/Rcode/c7/Carlina",sep="")); #
#
load("Yearly parameters.Rdata")#
#
source("Carlina Demog Funs DI.R") #
######################################################################
#Stochastic perturbation analysis#
######################################################################
#
nBigMatrix <- 100#
n.est <- 20000#
n.runin <- 500#
minsize <- 1.5#
maxsize <- 5#
n.years <-20#
stoc_pert_analysis<-function(params,n.est,n.runin){#
	year.i <- sample(1:n.years,n.est+1,replace=TRUE)#
#
	K.year.i <- array(NA,c(n.years,nBigMatrix,nBigMatrix))#
	for(i in 1:n.years){#
		year.K<-mk_K(nBigMatrix,params[,i],minsize,maxsize)#
		K.year.i[i,,] <- year.K$K#
	}#
#
	h <- year.K$h; #
#Calculate mean kernel, v and w#
#
	mean.kernel <- apply(K.year.i,2:3,mean)#
#
	w <- Re(eigen(mean.kernel)$vectors[,1]); #
	v <- Re(eigen(t(mean.kernel))$vectors[,1]);#
#
	# scale eigenvectors <v,w>=1 #
	w <- abs(w)/sum(h*abs(w))#
	v <- abs(v)#
	v <- v/(h*sum(v*w))#
    cat(h*sum(v*w)," should = 1","\n")#
#
#Esimate Lambda s#
#initialize variables	#
#
	nt<-rep(1/nBigMatrix,nBigMatrix)#
	rt.V <- rt.N <- rep(NA,n.est)#
#Iterate model#
#
	for (year.t in 1:n.est){#
		if(year.t%%10000==0) cat("iterate: ", year.t,"\n");#
		#iterate model with year-specific kernel#
		nt1<-K.year.i[year.i[year.t],,] %*% nt#
		sum.nt1<-sum(nt1)#
		#Calculate log growth rates  #
		rt.V[year.t] <- log(sum(nt1*v)/sum(nt*v))#
		rt.N[year.t] <- log(sum(nt1)/sum(nt))#
		nt <- nt1 / sum.nt1  #
	}#
#
Ls <- exp(mean(rt.V))#
### Get wt and Rt time series ####
	wt<-matrix(1/nBigMatrix, nrow=n.est+1, ncol=nBigMatrix);#
	for (i in 1:n.est) {#
		K             <- K.year.i[year.i[i],,]#
		wt[i+1,]  <-K %*% wt[i,]#
		wt[i+1,]  <-wt[i+1,]/sum(wt[i+1,]);#
		if(i%%10000==0) cat("wt ",i,"\n")#
#
	}#
### Get vt time series ####
	vt<-matrix(1/nBigMatrix, nrow=n.est+1, ncol=nBigMatrix);#
	for (i in (n.est+1):2) {#
		K           <- K.year.i[year.i[i],,]#
		vt[i-1,]  <- vt[i,] %*% K#
		vt[i-1,]  <- vt[i-1,]/sum(vt[i-1,]);#
		if(i%%10000==0) cat("vt  ",i,"\n")#
#
	}#
#
sens.s <- matrix(0,nrow=nBigMatrix,ncol=nBigMatrix);#
#
elas.s <- sens.s#
for (i in n.runin:(n.est-n.runin)) {#
#
		#standard calculations needed for the various formulae#
		K                 <-   K.year.i[year.i[i],,]#
		vt1.wt        <-   vt[i+1,]%*%t(wt[i,])#
		vt1.K.wt   <- sum(vt[i+1,] * (K %*% wt[i,]))#
#
		#calculation of the standard sensitivities and elasticities#
		sens.s<-sens.s+vt1.wt/vt1.K.wt;#
#
		elas.s<-elas.s+K*(vt1.wt/vt1.K.wt);#
}#
#
 elas.s <- elas.s/(n.est-2*n.runin+1)#
 sens.s <- Ls*sens.s/(n.est-2*n.runin+1)#
 return(list(meshpts=year.K$meshpts,h=h,sens.s=sens.s,elas.s=elas.s,mean.kernel=mean.kernel,Ls=Ls))#
#
}#
#
pert.K <- stoc_pert_analysis(m.par.est,n.est,n.runin)#
#
meshpts <- pert.K$meshpts#
h.inv.2 <- 1 / (pert.K$h^2)#
#
Kmean.elas <- pert.K$mean.kernel * pert.K$sens.s / pert.K$Ls#
#
K.sens <- pert.K$sens.s * h.inv.2#
K.elas <-  pert.K$elas.s * h.inv.2#
K.mean.elas <- Kmean.elas * h.inv.2#
K.sd.elas <- K.elas - K.mean.elas#
#
cat("sum elasticities = ",sum(pert.K$elas.s)," should be 1")#
#
## plot these#
ikeep <- which(meshpts>1.5 & meshpts<5) # use to extract a region to plot#
#postscript("KernSensElas.eps", #
#          width=8, height=8, horizontal=FALSE, paper="special")#
## plot the sensitivity kernel and the three elasticity surfaces#
set_graph_pars("panel4")#
image(meshpts[ikeep], meshpts[ikeep], t(K.sens[ikeep,ikeep]),#
      col=grey(seq(0.6, 1, length=100)),#
      xlab="Size (t), z", ylab="Size (t+1), z\'")#
contour(meshpts[ikeep], meshpts[ikeep], t(K.sens[ikeep,ikeep]), add=TRUE)#
add_panel_label("a")#
image(meshpts[ikeep], meshpts[ikeep], t(K.elas[ikeep,ikeep]),#
      col=grey(seq(0.6, 1, length=100)),#
      xlab="Size (t), z", ylab="Size (t+1), z\'")#
contour(meshpts[ikeep], meshpts[ikeep], t(K.elas[ikeep,ikeep]), #
        add=TRUE,levels=c(0.05,0.2,0.4))#
add_panel_label("b")#
image(meshpts[ikeep], meshpts[ikeep], t(K.mean.elas[ikeep,ikeep]),#
      col=grey(seq(0.6, 1, length=100)),#
      xlab="Size (t), z", ylab="Size (t+1), z\'")#
contour(meshpts[ikeep], meshpts[ikeep], t(K.mean.elas[ikeep,ikeep]), #
        add=TRUE,levels=c(0.1,0.4,0.8))#
add_panel_label("c")#
image(meshpts[ikeep], meshpts[ikeep], t(K.sd.elas[ikeep,ikeep]),#
      col=grey(seq(0.6, 1, length=100)),#
      xlab="Size (t), z", ylab="Size (t+1), z\'")#
contour(meshpts[ikeep], meshpts[ikeep], t(K.sd.elas[ikeep,ikeep]), #
        add=TRUE,levels=c(-0.05,-0.2,-0.4))#
add_panel_label("d")#
dev.copy2eps(file="~/Repos/ipm_book/c7/figures/CarlinaElasSens.eps")
## ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~#
## Fit and run fixed and mixed effects effects Carlina stochastic IPM#
## ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~#
#
rm(list=ls(all=TRUE))#
#
library(doBy)#
library(lme4)#
library(MCMCglmm)#
set.seed(53241986)#
#
## Working directory must be set here, so the source()'s below run#
root=ifelse(.Platform$OS.type=="windows","c:/repos","~/Repos"); #
setwd(paste(root,"/ipm_book/Rcode/c2",sep="")); #
#
source("../utilities/Standard Graphical Pars.R");#
#
root=ifelse(.Platform$OS.type=="windows","c:/repos","~/Repos"); #
setwd(paste(root,"/ipm_book/Rcode/c7/Carlina",sep="")); #
#
load("Yearly parameters.Rdata")#
#
source("Carlina Demog Funs DI.R") #
#
######################################################################
#Stochastic perturbation analysis#
######################################################################
#
nBigMatrix <- 100#
n.est <- 20000#
n.runin <- 500#
minsize <- 1.5#
maxsize <- 5#
n.years <-20#
#
stoc_pert_analysis<-function(params,n.est,n.runin,C.t,C.t.mean){#
	year.i <- sample(1:n.years,n.est+1,replace=TRUE)#
#
	K.year.i <- array(NA,c(n.years,nBigMatrix,nBigMatrix))#
	for(i in 1:n.years){#
		year.K<-mk_K(nBigMatrix,params[,i],minsize,maxsize)#
		K.year.i[i,,] <- year.K$K#
	}#
#
	h <- year.K$h; #
	meshpts <- year.K$meshpts#
#Calculate mean kernel, v and w#
#
	mean.kernel <- apply(K.year.i,2:3,mean)#
#
	w <- Re(eigen(mean.kernel)$vectors[,1]); #
	v <- Re(eigen(t(mean.kernel))$vectors[,1]);#
#
	# scale eigenvectors <v,w>=1 #
	w <- abs(w)/sum(h*abs(w))#
	v <- abs(v)#
	v <- v/(h*sum(v*w))#
    cat(h*sum(v*w)," should = 1","\n")#
#
#Esimate Lambda s#
#initialize variables	#
#
	nt<-rep(1/nBigMatrix,nBigMatrix)#
	rt.V <- rt.N <- rep(NA,n.est)#
#Iterate model#
#
	for (year.t in 1:n.est){#
		if(year.t%%10000==0) cat("iterate: ", year.t,"\n");#
		#iterate model with year-specific kernel#
		nt1<-K.year.i[year.i[year.t],,] %*% nt#
		sum.nt1<-sum(nt1)#
		#Calculate log growth rates  #
		rt.V[year.t] <- log(sum(nt1*v)/sum(nt*v))#
		rt.N[year.t] <- log(sum(nt1)/sum(nt))#
		nt <- nt1 / sum.nt1  #
	}#
#
Ls <- exp(mean(rt.V))#
### Get wt and Rt time series ####
	wt<-matrix(1/nBigMatrix, nrow=n.est+1, ncol=nBigMatrix);#
	for (i in 1:n.est) {#
		K             <- K.year.i[year.i[i],,]#
		wt[i+1,]  <-K %*% wt[i,]#
		wt[i+1,]  <-wt[i+1,]/sum(wt[i+1,]);#
		if(i%%10000==0) cat("wt ",i,"\n")#
#
	}#
### Get vt time series ####
	vt<-matrix(1/nBigMatrix, nrow=n.est+1, ncol=nBigMatrix);#
	for (i in (n.est+1):2) {#
		K           <- K.year.i[year.i[i],,]#
		vt[i-1,]  <- vt[i,] %*% K#
		vt[i-1,]  <- vt[i-1,]/sum(vt[i-1,]);#
		if(i%%10000==0) cat("vt  ",i,"\n")#
#
	}#
#
elas.s <- matrix(0,nBigMatrix,nBigMatrix)#
elas.s.mean <- matrix(0,nBigMatrix,nBigMatrix)#
#
for (year.t in n.runin:(n.est-n.runin)) {#
#
		#standard calculations needed for the various formulae#
#
	vt1.wt                       <- outer(vt[year.t+1,],wt[year.t,],FUN="*")#
	vt1.C.wt                    <- vt1.wt * C.t[year.i[year.t],,]  #
	 vt1.C.wt.mean        <- vt1.wt * C.t.mean[year.i[year.t],,] #
	K           <- K.year.i[year.i[year.t],,]#
	vt1.K.wt    <- sum(vt[year.t+1,] * (K %*% wt[year.t,]))#
#
		#calculation of the standard elasticities#
		elas.s            <-elas.s + (vt1.C.wt) / vt1.K.wt;#
		elas.s.mean <-elas.s.mean + (vt1.C.wt.mean) / vt1.K.wt;#
}#
#
 elas.s             <- elas.s/(n.est-2*n.runin+1)#
 elas.s.mean  <- elas.s.mean/(n.est-2*n.runin+1)#
 return(list(meshpts=year.K$meshpts, h=h, elas.s=elas.s, elas.s.mean=elas.s.mean, mean.kernel=mean.kernel, Ls=Ls))#
#
}#
#
#########################################################################
#Let's do the probability of flowering function#
#########################################################################
#
#Select the parameters to use#
params.to.use <- m.par.est#
#
#Calculate meshpts and h for evaluating the perturbation kernels#
year.K      <-  mk_K(nBigMatrix,params.to.use[,1],minsize,maxsize)#
meshpts <-  year.K$meshpts#
h              <-  year.K$h#
#
#First calculate the mean function and perturbation kernels#
#
p_bz.mean <- 0#
#
for(i in 1:n.years){#
	p_bz.mean <- p_bz.mean + p_bz(meshpts, params.to.use[,i])#
}#
#
p_bz.mean <- p_bz.mean/n.years#
#
Ct_z1z <- function(z1,z,m.par){#
		return( p_bz(z, m.par) * s_z(z, m.par) *#
				 ( m.par["p.r"] * b_z(z, m.par) * c_0z1(z1, m.par) - G_z1z(z1, z, m.par)) )#
	 }#
C.pert <- array(NA,c(n.years,nBigMatrix,nBigMatrix))#
	for(i in 1:n.years){#
		year.C <-h * (outer(meshpts, meshpts, Ct_z1z, m.par = params.to.use[,i]))#
		C.pert[i,,] <- year.C#
	}#
#
Ct_z1z_mean <- function(z1,z,m.par){#
		return( p_bz.mean * s_z(z, m.par) *#
				  ( m.par["p.r"] * b_z(z, m.par) * c_0z1(z1, m.par) - G_z1z(z1, z, m.par)) )#
	}#
#
C.pert.mean <- array(NA,c(n.years,nBigMatrix,nBigMatrix))#
	for(i in 1:n.years){#
		year.C <-h * (outer(meshpts, meshpts, Ct_z1z_mean, m.par = params.to.use[,i]))#
		C.pert.mean[i,,] <- year.C#
	}#
#
pert.K <- stoc_pert_analysis(params.to.use, n.est, n.runin, C.pert, C.pert.mean)#
meshpts <- pert.K$meshpts#
elas.s <- apply(pert.K$elas.s,2,sum)#
elas.s.mean <- apply(pert.K$elas.s.mean,2,sum)#
elas.s.sd <- elas.s - elas.s.mean#
sens.mean <- elas.s.mean * pert.K$Ls / p_bz.mean#
set_graph_pars("panel4")#
plot(meshpts,elas.s,type="l",xlab="Size (t), z",ylab=expression(e[S] ^p[b]))#
add_panel_label("a")#
plot(meshpts,elas.s.mean,type="l",xlab="Size (t), z",ylab=expression(e[S] ^{p[b]*","*mu}))#
add_panel_label("b")#
plot(meshpts,elas.s.sd,type="l",xlab="Size (t), z",ylab=expression(e[S] ^{p[b]*","*sigma}))#
add_panel_label("c")#
plot(meshpts,sens.mean,type="l",xlab="Size (t), z",ylab=expression(s[S] ^{p[b]*","*mu}))#
add_panel_label("d")#
dev.copy2eps(file="~/Repos/ipm_book/c7/figures/CarlinapbElasSens.eps")
#########################################################################
#Let's do the survival function#
#########################################################################
#
#First calculate the mean function and perturbation kernels#
#
s.zmean <- 0#
#
for(i in 1:n.years){#
	s.zmean <- s.zmean + s_z(meshpts, params.to.use[,i])#
}#
#
s.zmean <- s.zmean/n.years#
#
Ct_z1z <- function(z1,z,m.par){#
		return( s_z(z, m.par) * (1- p_bz(z, m.par) )*G_z1z(z1, z, m.par) +#
		              s_z(z, m.par) * p_bz(z, m.par) * m.par["p.r"] * b_z(z, m.par) * c_0z1(z1, m.par) ) #
	}#
C.pert <- array(NA,c(n.years,nBigMatrix,nBigMatrix))#
	for(i in 1:n.years){#
		year.C <-h * (outer(meshpts, meshpts, Ct_z1z, m.par = params.to.use[,i]))#
		C.pert[i,,] <- year.C#
	}#
#
Ct_z1z_mean <- function(z1,z,m.par){#
		return( s.zmean * (1- p_bz(z, m.par) )*G_z1z(z1, z, m.par) +#
		              s.zmean * p_bz(z, m.par) * m.par["p.r"] * b_z(z, m.par) * c_0z1(z1, m.par) ) #
	}#
C.pert.mean <- array(NA,c(n.years,nBigMatrix,nBigMatrix))#
	for(i in 1:n.years){#
		year.C <-h * (outer(meshpts, meshpts, Ct_z1z_mean, m.par = params.to.use[,i]))#
		C.pert.mean[i,,] <- year.C#
	}#
#
pert.K <- stoc_pert_analysis(params.to.use, n.est, n.runin, C.pert, C.pert.mean)#
#
#max(pert.K$elas.tmp-apply(pert.K$elas.s,2,sum))#
#
meshpts <- pert.K$meshpts#
elas.s <- apply(pert.K$elas.s,2,sum)#
elas.s.mean <- apply(pert.K$elas.s.mean,2,sum)#
elas.s.sd <- elas.s - elas.s.mean#
sens.mean <- elas.s.mean * pert.K$Ls / s.zmean#
#
#Check sum elasticities is 1#
#
cat(sum(pert.K$elas.s)," should be 1","\n")#
#
set_graph_pars("panel4")#
plot(meshpts,elas.s,type="l",xlab="Size (t), z",ylab=expression(e[S] ^s(z)))#
add_panel_label("a")#
plot(meshpts,elas.s.mean,type="l",xlab="Size (t), z",ylab=expression(e[S] ^{s(z)*","*mu}))#
add_panel_label("b")#
plot(meshpts,elas.s.sd,type="l",xlab="Size (t), z",ylab=expression(e[S] ^{s(z)*","*sigma}))#
add_panel_label("c")#
plot(meshpts,sens.mean,type="l",xlab="Size (t), z",ylab=expression(s[S] ^{s(z)*","*mu}))#
add_panel_label("d")#
#
dev.copy2eps(file="~/Repos/ipm_book/c7/figures/CarlinasElasSens.eps")
#########################################################################
#Let's do the growth function#
#########################################################################
#
#First calculate the mean function and perturbation kernels#
#
G.mean <- 0#
#
for(i in 1:n.years){#
	G.mean <- G.mean + outer(meshpts, meshpts, G_z1z, m.par = params.to.use[,i])#
}#
#
G.mean <- G.mean/n.years#
#
Ct_z1z <- function(z1,z,m.par){#
		return( s_z(z, m.par) * (1- p_bz(z, m.par) )*G_z1z(z1, z, m.par)  ) #
	}#
C.pert <- array(NA,c(n.years,nBigMatrix,nBigMatrix))#
	for(i in 1:n.years){#
		year.C <-h * (outer(meshpts, meshpts, Ct_z1z, m.par = params.to.use[,i]))#
		C.pert[i,,] <- year.C#
	}#
#
Ct_z1z_mean <- function(z1,z,m.par){#
		return( s_z(z, m.par)  * (1- p_bz(z, m.par) ) * G.mean) #
	}#
C.pert.mean <- array(NA,c(n.years,nBigMatrix,nBigMatrix))#
	for(i in 1:n.years){#
		year.C <-h * (outer(meshpts, meshpts, Ct_z1z_mean, m.par = params.to.use[,i]))#
		C.pert.mean[i,,] <- year.C#
	}#
pert.K <- stoc_pert_analysis(params.to.use, n.est, n.runin, C.pert, C.pert.mean)#
#
meshpts <- pert.K$meshpts#
elas.s.sd <- pert.K$elas.s - pert.K$elas.s.mean#
sens.mean <- pert.K$elas.s.mean * pert.K$Ls / G.mean#
#
## set up the plots#
ikeep <- ikeep <- which(meshpts>1.5 & meshpts<5) # use to extract a region to plot#
set_graph_pars("panel4")#
## plot the growth sensitivity and elasticity surfaces#
image(meshpts[ikeep], meshpts[ikeep], t(pert.K$elas.s[ikeep,ikeep]),#
      col=grey(seq(0.6, 1, length=100)),#
      xlab="Size (t), z", ylab="Size (t+1), z\'")#
contour(meshpts[ikeep], meshpts[ikeep], t(pert.K$elas.s[ikeep,ikeep]), #
        add=TRUE)#
add_panel_label("a")#
## plot the offspring size kernel sensitivity and elasticity surfaces#
image(meshpts[ikeep], meshpts[ikeep], t(pert.K$elas.s.mean[ikeep,ikeep]),#
      col=grey(seq(0.6, 1, length=100)),#
      xlab="Size (t), z", ylab="Size (t+1), z\'")#
contour(meshpts[ikeep], meshpts[ikeep], t(pert.K$elas.s.mean[ikeep,ikeep]), add=TRUE)#
add_panel_label("b")#
image(meshpts[ikeep], meshpts[ikeep], t(elas.s.sd [ikeep,ikeep]),#
      col=grey(seq(0.6, 1, length=100)),#
      xlab="Size (t), z", ylab="Size (t+1), z\'")#
contour(meshpts[ikeep], meshpts[ikeep], t(elas.s.sd [ikeep,ikeep]), add=TRUE)#
add_panel_label("c")#
image(meshpts[ikeep], meshpts[ikeep], t(sens.mean[ikeep,ikeep]),#
      col=grey(seq(0.6, 1, length=100)),#
      xlab="Size (t), z", ylab="Size (t+1), z\'")#
contour(meshpts[ikeep], meshpts[ikeep], t(sens.mean[ikeep,ikeep]), add=TRUE)#
add_panel_label("d")#
#
dev.copy2eps(file="~/Repos/ipm_book/c7/figures/CarlinaGElasSens.eps")
## ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~#
## Fit and run fixed and mixed effects effects Carlina stochastic IPM#
## ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~#
#
rm(list=ls(all=TRUE))#
#
library(doBy)#
library(lme4)#
library(MCMCglmm)#
set.seed(53241986)#
#
## Working directory must be set here, so the source()'s below run#
root=ifelse(.Platform$OS.type=="windows","c:/repos","~/Repos"); #
setwd(paste(root,"/ipm_book/Rcode/c2",sep="")); #
#
source("../utilities/Standard Graphical Pars.R");#
#
root=ifelse(.Platform$OS.type=="windows","c:/repos","~/Repos"); #
setwd(paste(root,"/ipm_book/Rcode/c7/Carlina",sep="")); #
#
load("Yearly parameters.Rdata")#
#
source("Carlina Demog Funs DI.R") #
######################################################################
#Stochastic perturbation analysis#
######################################################################
#
nBigMatrix <- 100#
n.est <- 20000#
n.runin <- 500#
minsize <- 1.5#
maxsize <- 5#
n.years <-20#
#
stoc_pert_analysis<-function(params,n.est,n.runin,C.t,C.t.mean){#
	year.i <- sample(1:n.years,n.est+1,replace=TRUE)#
#
	K.year.i <- array(NA,c(n.years,nBigMatrix,nBigMatrix))#
	for(i in 1:n.years){#
		year.K<-mk_K(nBigMatrix,params[,i],minsize,maxsize)#
		K.year.i[i,,] <- year.K$K#
	}#
#
	h <- year.K$h; #
	meshpts <- year.K$meshpts#
#Calculate mean kernel, v and w#
#
	mean.kernel <- apply(K.year.i,2:3,mean)#
#
	w <- Re(eigen(mean.kernel)$vectors[,1]); #
	v <- Re(eigen(t(mean.kernel))$vectors[,1]);#
#
	# scale eigenvectors <v,w>=1 #
	w <- abs(w)/sum(h*abs(w))#
	v <- abs(v)#
	v <- v/(h*sum(v*w))#
    cat(h*sum(v*w)," should = 1","\n")#
#
#Esimate Lambda s#
#initialize variables	#
#
	nt<-rep(1/nBigMatrix,nBigMatrix)#
	rt.V <- rt.N <- rep(NA,n.est)#
#Iterate model#
#
	for (year.t in 1:n.est){#
		if(year.t%%10000==0) cat("iterate: ", year.t,"\n");#
		#iterate model with year-specific kernel#
		nt1<-K.year.i[year.i[year.t],,] %*% nt#
		sum.nt1<-sum(nt1)#
		#Calculate log growth rates  #
		rt.V[year.t] <- log(sum(nt1*v)/sum(nt*v))#
		rt.N[year.t] <- log(sum(nt1)/sum(nt))#
		nt <- nt1 / sum.nt1  #
	}#
#
Ls <- exp(mean(rt.V))#
### Get wt and Rt time series ####
	wt<-matrix(1/nBigMatrix, nrow=n.est+1, ncol=nBigMatrix);#
	for (i in 1:n.est) {#
		K             <- K.year.i[year.i[i],,]#
		wt[i+1,]  <-K %*% wt[i,]#
		wt[i+1,]  <-wt[i+1,]/sum(wt[i+1,]);#
		if(i%%10000==0) cat("wt ",i,"\n")#
#
	}#
### Get vt time series ####
	vt<-matrix(1/nBigMatrix, nrow=n.est+1, ncol=nBigMatrix);#
	for (i in (n.est+1):2) {#
		K           <- K.year.i[year.i[i],,]#
		vt[i-1,]  <- vt[i,] %*% K#
		vt[i-1,]  <- vt[i-1,]/sum(vt[i-1,]);#
		if(i%%10000==0) cat("vt  ",i,"\n")#
#
	}#
#
elas.s <- matrix(0,nBigMatrix,nBigMatrix)#
elas.s.mean <- matrix(0,nBigMatrix,nBigMatrix)#
#
for (year.t in n.runin:(n.est-n.runin)) {#
#
		#standard calculations needed for the various formulae#
#
	vt1.wt                       <- outer(vt[year.t+1,],wt[year.t,],FUN="*")#
	vt1.C.wt                    <- vt1.wt * C.t[year.i[year.t],,]  #
	 vt1.C.wt.mean        <- vt1.wt * C.t.mean[year.i[year.t],,] #
	K           <- K.year.i[year.i[year.t],,]#
	vt1.K.wt    <- sum(vt[year.t+1,] * (K %*% wt[year.t,]))#
#
		#calculation of the standard elasticities#
		elas.s            <-elas.s + (vt1.C.wt) / vt1.K.wt;#
		elas.s.mean <-elas.s.mean + (vt1.C.wt.mean) / vt1.K.wt;#
}#
#
 elas.s             <- elas.s/(n.est-2*n.runin+1)#
 elas.s.mean  <- elas.s.mean/(n.est-2*n.runin+1)#
 return(list(meshpts=year.K$meshpts, h=h, elas.s=elas.s, elas.s.mean=elas.s.mean, mean.kernel=mean.kernel, Ls=Ls))#
#
}#
#
#########################################################################
#Let's do the intercept of the probability of flowering function#
#########################################################################
#
#Select the parameters to use#
params.to.use <- m.par.est#
#
#Calculate meshpts and h for evaluating the perturbation kernels#
year.K      <-  mk_K(nBigMatrix,params.to.use[,1],minsize,maxsize)#
meshpts <-  year.K$meshpts#
h              <-  year.K$h#
#
#First calculate the mean and sd of beta0 and perturbation kernels#
#
beta.0.mean <- mean(params.to.use["flow.int",])#
beta.0.sd       <- sd(params.to.use["flow.int",])#
#
Ct_z1z <- function(z1,z,m.par){#
		return(s_z(z, m.par) * ( m.par["p.r"] * b_z(z, m.par) * c_0z1(z1, m.par) - G_z1z(z1, z, m.par)) * #
		(1/(1+exp(m.par["flow.int"]+m.par["flow.z"]*z))) * p_bz(z, m.par) *#
		 m.par["flow.int"])#
	 }#
C.pert <- array(NA,c(n.years,nBigMatrix,nBigMatrix))#
	for(i in 1:n.years){#
		year.C <- h * (outer(meshpts, meshpts, Ct_z1z, m.par = params.to.use[,i]))#
		C.pert[i,,] <- year.C#
	}#
#
Ct_z1z_mean <- function(z1,z,m.par){#
		return(s_z(z, m.par) * ( m.par["p.r"] * b_z(z, m.par) * c_0z1(z1, m.par) - G_z1z(z1, z, m.par)) *#
		(1/(1+exp(m.par["flow.int"]+m.par["flow.z"]*z)))  * p_bz(z, m.par) *#
		 beta.0.mean)#
	}#
#
C.pert.mean <- array(NA,c(n.years,nBigMatrix,nBigMatrix))#
	for(i in 1:n.years){#
		year.C <- h * (outer(meshpts, meshpts, Ct_z1z_mean, m.par = params.to.use[,i]))#
		C.pert.mean[i,,] <- year.C#
	}#
#
pert.K <- stoc_pert_analysis(params.to.use, n.est, n.runin, C.pert, C.pert.mean)#
#
elas.s            <- sum(pert.K$elas.s)#
elas.s.mean <- sum(pert.K$elas.s.mean)#
elas.s.sd       <- elas.s-elas.s.mean#
cat("Stochastic elasticity ",elas.s,"\n")#
cat("Stochastic elasticity mean ",elas.s.mean,"\n")#
cat("Stochastic elasticity sd ",elas.s.sd,"\n")#
cat("Stochastic sensitivity  mean ",pert.K$Ls*elas.s.mean/beta.0.mean,"\n")#
cat("Stochastic sensitivity  sd ",pert.K$Ls*elas.s.sd/beta.0.sd,"\n")
###################################################################
#Code for time invariant parameters#
#
stoc_pert_analysis_invariant<-function(params,n.est,n.runin,K.t.deriv,K.t.deriv2){#
	year.i <- sample(1:n.years,n.est+1,replace=TRUE)#
#
	K.year.i <- array(NA,c(n.years,nBigMatrix,nBigMatrix))#
	for(i in 1:n.years){#
		year.K<-mk_K(nBigMatrix,params[,i],minsize,maxsize)#
		K.year.i[i,,] <- year.K$K#
	}#
#
	h <- year.K$h; #
	meshpts <- year.K$meshpts#
#Calculate mean kernel, v and w#
#
	mean.kernel <- apply(K.year.i,2:3,mean)#
#
	w <- Re(eigen(mean.kernel)$vectors[,1]); #
	v <- Re(eigen(t(mean.kernel))$vectors[,1]);#
#
	# scale eigenvectors <v,w>=1 #
	w <- abs(w)/sum(h*abs(w))#
	v <- abs(v)#
	v <- v/(h*sum(v*w))#
    cat(h*sum(v*w)," should = 1","\n")#
#
#Esimate Lambda s#
#initialize variables	#
#
	nt<-rep(1/nBigMatrix,nBigMatrix)#
	rt.V <- rt.N <- rep(NA,n.est)#
#Iterate model#
#
	for (year.t in 1:n.est){#
		if(year.t%%10000==0) cat("iterate: ", year.t,"\n");#
		#iterate model with year-specific kernel#
		nt1<-K.year.i[year.i[year.t],,] %*% nt#
		sum.nt1<-sum(nt1)#
		#Calculate log growth rates  #
		rt.V[year.t] <- log(sum(nt1*v)/sum(nt*v))#
		rt.N[year.t] <- log(sum(nt1)/sum(nt))#
		nt <- nt1 / sum.nt1  #
	}#
#
Ls <- exp(mean(rt.V))#
### Get wt and Rt time series ####
	wt<-matrix(1/nBigMatrix, nrow=n.est+1, ncol=nBigMatrix);#
	for (i in 1:n.est) {#
		K             <- K.year.i[year.i[i],,]#
		wt[i+1,]  <-K %*% wt[i,]#
		wt[i+1,]  <-wt[i+1,]/sum(wt[i+1,]);#
		if(i%%10000==0) cat("wt ",i,"\n")#
#
	}#
### Get vt time series ####
	vt<-matrix(1/nBigMatrix, nrow=n.est+1, ncol=nBigMatrix);#
	for (i in (n.est+1):2) {#
		K           <- K.year.i[year.i[i],,]#
		vt[i-1,]  <- vt[i,] %*% K#
		vt[i-1,]  <- vt[i-1,]/sum(vt[i-1,]);#
		if(i%%10000==0) cat("vt  ",i,"\n")#
#
	}#
#
Exp.1 <- 0#
Exp.2 <- 0#
#
for (year.t in n.runin:(n.est-n.runin)) {#
#
		#standard calculations needed for the various formulae#
#
	vt1.K.t.deriv.wt       <- sum(vt[year.t+1,] * (K.t.deriv[year.i[year.t],,] %*% wt[year.t,]))#
	vt1.K.t.deriv2.wt    <- sum(vt[year.t+1,] * (K.t.deriv2[year.i[year.t],,] %*% wt[year.t,]))#
	K                                <- K.year.i[year.i[year.t],,]#
	vt1.K.wt                   <- sum(vt[year.t+1,] * (K %*% wt[year.t,]))#
#
		#calculation the expectations #
		Exp.1            <- Exp.1 + vt1.K.t.deriv2.wt / vt1.K.wt;#
		Exp.2            <- Exp.2 + (vt1.K.t.deriv.wt / vt1.K.wt)^2;		#
}#
#
 Exp.1              <- Exp.1 /(n.est-2*n.runin+1)#
 Exp.2              <- Exp.2 /(n.est-2*n.runin+1)#
 sens.s             <- (Ls/2)*(Exp.1-Exp.2)#
 return(list(meshpts=year.K$meshpts, h=h, sens.s=sens.s, Ls=Ls))#
#
}#
#
##################################################
#Calculate K derivative kernels#
##################################################
Kt_partial_A <- function(z1,z,m.par){#
		return(s_z(z, m.par) * p_bz(z, m.par) * m.par["p.r"] * c_0z1(z1, m.par) * b_z(z, m.par))	#
		 }#
Kt.partial.A <- array(NA,c(n.years,nBigMatrix,nBigMatrix))#
	for(i in 1:n.years){#
		year.C <- h * (outer(meshpts, meshpts, Kt_partial_A, m.par = params.to.use[,i]))#
		Kt.partial.A[i,,] <- year.C#
	}#
#
#Note the derivative kernels are the same here but we're written the code as if they#
#were different so it's easier to alter for other parameters.#
#
Kt_partial_A2 <- function(z1,z,m.par){#
		return(s_z(z, m.par) * p_bz(z, m.par) * m.par["p.r"] * c_0z1(z1, m.par) * b_z(z, m.par))#
	}#
#
Kt.partial.A2 <- array(NA,c(n.years,nBigMatrix,nBigMatrix))#
	for(i in 1:n.years){#
		year.C <- h * (outer(meshpts, meshpts, Kt_partial_A2, m.par = params.to.use[,i]))#
		Kt.partial.A2[i,,] <- year.C#
	}#
#
pert.K <- stoc_pert_analysis_invariant(params.to.use, n.est, n.runin, Kt.partial.A, Kt.partial.A2)#
#
pert.K$sens.s
rm(list=ls(all=TRUE))#
library(nlme)#
library(MASS)#
library(rjags)#
library(coda)#
library(MCMCglmm)#
root=ifelse(.Platform$OS.type=="windows","c:/repos","~/Repos"); #
setwd(paste(root,"/ipm_book/Rcode/c7/Carlina",sep="")); #
dataf<-read.csv("Carlina survive or flower.fre",header=T)#
attach(dataf)#
s.dataf<-dataf[flow==1,]#
s.dataf$flow<-0#
names(s.dataf)[5]<-"die"#
detach(dataf)#
dataf<-read.csv("Carlina survive or died.txt",header=T)#
attach(dataf)#
dataf<-rbind(dataf,s.dataf)#
attach(dataf)#
rec.size<-z[age==1]#
rec.yeart<-yeart[age==1]#
fit.rec.lm<-lm(rec.size~factor(rec.yeart))#
anova(fit.rec.lm)#
summary(fit.rec.lm)#
fit.rec.lm<-lm(rec.size~factor(rec.yeart)-1)#
par(mfrow=c(1,1),bty="l",pch=19)#
qqnorm(fit.rec.lm$coef)#
qqline(fit.rec.lm$coef)#
fit.rec.r<-lme(rec.size~1,data=dataf,random=~1|rec.yeart)#
detach(dataf)#
####################################################################################################################################
##growth analysis#
dataf<-data.frame(read.csv("crlnagrw.txt",header=T));#
attach(dataf)#
fit.grow.i<-lm(lst1~factor(dataf$yeart)*lst-1)#
summary(lm(fit.grow.i$coef[1:16]~fit.grow.i$coef[17:32]))#
plot(fit.grow.i$coef[1:16],fit.grow.i$coef[17:32])#
cor.test(fit.grow.i$coef[1:16],fit.grow.i$coef[17:32])#
lst.c<-lst-mean(lst)#
fit.grow<-lm(lst1~lst.c+factor(dataf$yeart))#
fit.grow.i<-lm(lst1~lst.c*factor(dataf$yeart))#
anova(fit.grow,fit.grow.i)#
fit.grow<-lm(lst1~factor(dataf$yeart)*lst.c-1)#
summary(lm(fit.grow$coef[1:16]~fit.grow$coef[17:32]))#
plot(fit.grow$coef[1:16],fit.grow$coef[17:32])#
cor.test(fit.grow$coef[1:16],fit.grow$coef[17:32])#
par(mfrow=c(1,2),bty="l",pty="s",pch=19)#
qqnorm(fit.grow$coef[1:16])#
qqline(fit.grow$coef[1:16])#
qqnorm(fit.grow$coef[17:32])#
qqline(fit.grow$coef[17:32])#
g.dataf<-groupedData(lst1~lst.c|yeart,data=data.frame(dataf,lst.c))#
fit.grow.r<-lme(lst1~lst.c,data=g.dataf,random=~1|yeart)#
fit.grow.r.s<-lme(lst1~lst.c,data=g.dataf,random=~lst.c|yeart)#
fit.grow.r.s.diag<-lme(lst1~lst.c,data=g.dataf,random=pdDiag(~lst.c))#
fit.grow.r.s.diag.v<-lme(lst1~lst.c,data=g.dataf,random=pdDiag(~lst.c),weight=varExp(form=~fitted(.)))#
anova(fit.grow.r, fit.grow.r.s, fit.grow.r.s.diag, fit.grow.r.s.diag.v)#
summary(fit.grow.r.s.diag)#
intervals(fit.grow.r.s.diag)#
par(mfrow=c(1,2),bty="l",pty="s",pch=19)#
qqnorm(ranef(fit.grow.r.s.diag)[,1])#
qqline(ranef(fit.grow.r.s.diag)[,1])#
qqnorm(ranef(fit.grow.r.s.diag)[,2])#
qqline(ranef(fit.grow.r.s.diag)[,2])#
######################################################################################################################################
#fit bivariate model to growth and recruitment#
rec.yeart=rec.yeart-1#
rec.size=rec.size[rec.yeart>0]#
rec.yeart=rec.yeart[rec.yeart>0]#
lst=lst[yeart<16]#
lst1=lst1[yeart<16]#
yeart=yeart[yeart<16]#
mean.lst <- mean(lst)#
lst<-lst-mean.lst#
reclm<-lm(rec.size~factor(rec.yeart)-1)#
grlm<-lm(lst1~factor(yeart)*lst-1)#
plot(reclm$coef[1:15],grlm$coef[1:15])#
cor.test(reclm$coef[1:15],grlm$coef[1:15])#
N1=length(lst)#
N2=N1+length(rec.size)#
######################################################################################################################################
#fit it with BUGS Cam formulation#
n.chains=3;#
rho=rnorm(1,0,0.1)#
a=rnorm(1,1,0.1)#
b=rnorm(1,1,0.1)#
X=c(lst,rep(NA,length(rec.size))); Y=c(lst1,rec.size); ngroup=15; group=c(yeart,rec.yeart);#
#data=list("N1","N2","X","Y","ngroup","group"); #
#
data = list(N1=N1,N2=N2,X=X,Y=Y,ngroup=ngroup,group=group)#
inits=function() {#
	list(mu.A=rnorm(1,0,1),mu.Ar=rnorm(1,0,1),B=rnorm(1,0,1),#
	     prec.B=rnorm(1,1,0.1),prec.e=rnorm(1,1,0.1),prec.er=rnorm(1,1,0.1),a=a,b=b,rho=rho,#
	     x=rnorm(ngroup,0.0,1),y=rnorm(ngroup,0.0,1),B.g=rnorm(ngroup)#
	    )#
}#
parameters=c("mu.A","mu.Ar","B","sd.B","rho","sd.e","sd.er","sd.A","sd.Ar");#
glmm.sim=jags.model(data=data,inits=inits,#
file="model grow rec Car Cam.txt",n.chains=n.chains)#
#
samps <- coda.samples(glmm.sim,parameters,n.iter=20000,thin=100)#
plot(samps[,1:4])#
plot(samps[,5:8])#
plot(samps[,9])#
gelman.plot(samps)#
autocorr.plot(samps)#
#
burn.in <- 5000#
summary(window(samps, start=burn.in))#
cov.matrix <- matrix(c(0.1950^2,0.7664*0.1950*0.2749,0.7664*0.1950*0.2749,0.2749^2),nrow=2)#
cov2cor(cov.matrix)
burn.in <- 5000#
summary(window(samps, start=burn.in))
## ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~#
## Run LTRE analysis using large parameter set from fitted mixed models#
## ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~#
#
rm(list=ls(all=TRUE))#
require(mgcv)#
require(randomForest)#
#
set.seed(53241986)#
#
## Working directory must be set here, so the source()'s below run#
root=ifelse(.Platform$OS.type=="windows","c:/repos","~/Repos"); #
setwd(paste(root,"/ipm_book/Rcode/c2",sep="")); #
#
source("../utilities/Standard Graphical Pars.R");#
#
root=ifelse(.Platform$OS.type=="windows","c:/repos","~/Repos"); #
setwd(paste(root,"/ipm_book/Rcode/c7/Carlina",sep="")); #
source("Carlina Demog Funs DI.R") #
#
######################################################################
#LTRE analysis#
######################################################################
#
nBigMatrix <- 100#
n.est <- 20000#
n.runin <- 500#
minsize <- 1.5#
maxsize <- 5#
n.years <-20#
#
#function adds yearly lambda to param matrix, also removes parameters that#
#don't vary with time, and calculates d lambda / d parameters numerically#
#returns data.frame with parameters and lambda as columns for analysis#
#plus other stuff#
LTRE.calcs<-function(params,delta=.Machine$double.eps^(1/3)){#
#
	K.year.i     <- array(NA,c(n.years,nBigMatrix,nBigMatrix))#
	lambda.i  <- rep(NA,n.years)#
	n.params <- dim(params)[1]#
#Calculate lambda for each yearly kernel#
	for(i in 1:n.years){#
		year.K         <- mk_K(nBigMatrix,params[,i],minsize,maxsize)#
		lambda.i[i] <- Re(eigen(year.K$K,only.values=TRUE)$values[1])#
		K.year.i[i,,] <- year.K$K#
		#cat(lambda.i[i],"\n")#
	}#
#
	h <- year.K$h; #
	meshpts <- year.K$meshpts#
#Calculate mean kernel, and mean of each params#
#
	mean.kernel   <- apply(K.year.i,2:3,mean)#
	mean.params   <- apply(params,1,mean)#
#Calculate sensitivities to mean kernel for each parameter usinf finite differences#
#
parm.to.pert         <- 1:n.params#
d.lambda.d.theta <- rep(NA,n.params)#
#
for(i in 1:n.params){#
	p.pert                 <-  (i==parm.to.pert)*delta#
	K.up                   <- mk_K(nBigMatrix,mean.params+p.pert,minsize,maxsize)$K#
	lambda.up              <- Re(eigen(K.up,only.values=TRUE)$values[1])#
	K.down                 <-  mk_K(nBigMatrix,mean.params-p.pert,minsize,maxsize)$K#
	lambda.down            <-  Re(eigen(K.down,only.values=TRUE)$values[1])#
	d.lambda.d.theta[i] = 	(lambda.up - lambda.down) / (2*delta)#
	}#
	names(d.lambda.d.theta) <- rownames(params)#
#remove parameters that don't vary#
#
	which.vary<- which(apply(params,1,sd)>0);	#
	LTRE.data <- rbind(params[which.vary,],lambda.i)#
	LTRE.data.df<- data.frame(t(LTRE.data)); #
	d.lambda.d.theta <- d.lambda.d.theta[which.vary]#
return(list(meshpts=meshpts, h=h,mean.kernel=mean.kernel,LTRE.data=LTRE.data.df,#
	 d.lambda.d.theta=d.lambda.d.theta))#
#
}#
#
load(file="Big param matrix.Rdata")#
#
n.years <-5000#
LTRE.nums <- LTRE.calcs(store.params)#
#
#extract parameters/lambda and sensitivities to formulae look neater#
#
ps.and.l <- LTRE.nums$LTRE.data#
d.lambda.d.theta <- LTRE.nums$d.lambda.d.theta#
#
#calculate sensitivity matrix#
sens.mat <- outer(d.lambda.d.theta,d.lambda.d.theta,FUN="*")#
#
#calculate sensitivity matrix - cols 1 to 6 contain parameters#
var.cov <- cov(ps.and.l[,1:6])#
#
#Calculate 1st order approx#
var.terms.1.order <-sens.mat*var.cov#
#
#sum terms#
var(ps.and.l[,"lambda.i"])#
sum(var.terms.1.order)#
#
#error#
(var(ps.and.l[,"lambda.i"])-sum(var.terms.1.order))/var(ps.and.l[,"lambda.i"])*100#
#
#calculate Cont terms and sort#
order.terms.1.order <- sort(apply(var.terms.1.order,1,sum)/sum(var.terms.1.order))#
order.terms.1.order #
#
#fit lm model for variation in lambda#
#
fit <- lm(lambda.i~surv.int+surv.z+flow.int+grow.int+grow.z+rcsz.int, data=ps.and.l)#
#
#extract slopes#
slopes <- as.numeric(coef(fit)[2:7])#
#
#check slopes ~ sensitivites#
plot(d.lambda.d.theta,slopes)#
abline(0,1)#
#
#cacluate sensitivity matrix using slopes#
sens.mat <- outer(slopes,slopes,FUN="*")#
#
#calculate weighted var-covar matrix#
var.terms.lm <- sens.mat*var.cov#
#
#check total variance equal#
sum(var.terms.lm) + (summary(fit)$sigma)^2#
var(ps.and.l[,"lambda.i"])#
#
#calculate Cont terms and sort#
order.terms.lm <- sort(apply(var.terms.lm,1,sum)/sum(var.terms.lm))#
order.terms.lm#
#
#do calculation with predict#
terms <- predict(fit,type="terms")#
#
cov(terms)#
#
sum(cov(terms)) + (summary(fit)$sigma)^2#
#
#fit gam model#
fit <- gam(ps.and.l[,"lambda.i"] ~ s(ps.and.l[,"surv.int"]) + #
                                                            s(ps.and.l[,"surv.z"])    +#
                                                            s(ps.and.l[,"flow.int"]) +#
                                                            s(ps.and.l[,"grow.int"])+#
                                                            s(ps.and.l[,"grow.z"])   +#
                                                            s(ps.and.l[,"rcsz.int"]) , data=ps.and.l)#
#
#extract terms using predict#
terms <- predict(fit,type="terms")#
#
#weighted var-cov matrix#
var.terms.gam <- cov(terms)#
#
rownames(var.terms.gam) <- rownames(var.terms.1.order)#
colnames(var.terms.gam)  <- colnames(var.terms.1.order)#
#
var.terms.gam#
sum(var.terms.gam)+summary(fit)$scale#
#
var(ps.and.l[,"lambda.i"])#
#
#calculate Cont terms and sort#
order.terms.gam <- sort(apply(var.terms.gam,1,sum)/sum(var.terms.gam))#
order.terms.gam#
#
################################################
#Random Forests
# 'Tune' the random forest parameter mtry#
out<- tuneRF(x=subset(ps.and.l,select=-lambda.i), y=ps.and.l$lambda.i,ntreeTry=500,stepfactor=1,improve=0.02); #
plot(out); ## tells us to use mtry=4
x=subset(ps.and.l,select=-lambda.i)#
out<- tuneRF(x=x, y=ps.and.l$lambda.i,#
ntreeTry=500,stepfactor=1,improve=0.02); #
plot(out); ## tells us to use mtry=4
lambdaRF <- randomForest(x=x,y=ps.and.l$lambda.i,ntree=500,importance=TRUE,mtry=4)
graphics.off(); #
dev.new(width=8,height=5); set_graph_pars("panel2");#
# verify that the number of trees is enough#
# the plot of R^2 vs. number of trees used should saturate #
plot(lambdaRF$rsq,type="l",xlab="Number of trees",ylab="Model r^2"); add_panel_label("a")#
#
# Plot the importance results #
varImpPlot(lambdaRF,type=1,scale=FALSE,main="")#
add_panel_label("b")#
#
#sorted Cont terms#
order.terms.1.order #
order.terms.lm#
order.terms.gam
# 'Tune' the random forest parameter mtry#
year.p=subset(ps.and.l,select=-lambda.i)#
lambda.t <- ps.and.l$lambda.i#
out<- tuneRF(x=year.p, y=lambda.t,#
ntreeTry=500,stepfactor=1,improve=0.02); #
plot(out); ## tells us to use mtry=4
lambdaRF <- randomForest(x=year.p,y=lambda.t,ntree=500,importance=TRUE,mtry=4)#
#
graphics.off(); #
dev.new(width=8,height=5); set_graph_pars("panel2");#
# verify that the number of trees is enough#
# the plot of R^2 vs. number of trees used should saturate #
plot(lambdaRF$rsq,type="l",xlab="Number of trees",ylab="Model r^2"); add_panel_label("a")
# Plot the importance results #
varImpPlot(lambdaRF,type=1,scale=FALSE,main="")#
add_panel_label("b")
#sorted Cont terms#
order.terms.1.order #
order.terms.lm#
order.terms.gam
